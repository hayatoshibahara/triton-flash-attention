{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b79f140f",
   "metadata": {},
   "source": [
    "# Flash Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170756b",
   "metadata": {},
   "source": [
    "[論文](https://arxiv.org/abs/2205.14135)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceae840",
   "metadata": {},
   "source": [
    "## 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aab3fe",
   "metadata": {},
   "source": [
    "Transformerは、入力のシーケンスが長いと処理が重くなってしまう\n",
    "\n",
    "原因は、アテンションの計算時間とメモリ使用量が入力シーケンスの長さ（系列長）の2乗に比例するため\n",
    "\n",
    "本論文では、メモリの読み書きを考慮してアテンションを再設計した**Flash Attention**を提案する\n",
    "\n",
    "Flash Attentionは、データをブロックに分けSRAM内でアテンションを計算し、メモリ間の読み書き回数を削減する手法\n",
    "\n",
    "この手法により、系列長1KのGPT-2の学習速度を3倍高速化し、系列長を広げることで性能も改善した"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f005a42",
   "metadata": {},
   "source": [
    "## 導入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7562c63b",
   "metadata": {},
   "source": [
    "![](image/fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36fbbc4",
   "metadata": {},
   "source": [
    "Transformerの処理が重い理由は、計算速度ではなくメモリの転送速度にある（メモリ律速）\n",
    "\n",
    "GPUは、カーネルを実行するためにHBM（High Bandwidth Memory）からSRAMにデータを読み込み、実行後に書き込む\n",
    "\n",
    "HBMとSRAM間の読み込みと書き込みが、計算速度と比べ遅い:\n",
    "\n",
    "- HBM\n",
    "    - 転送速度 1.5TB/s\n",
    "    - サイズ 40GB\n",
    "- SRAM\n",
    "    - 転送速度 19TB/s\n",
    "    - サイズ 20MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b296be8d",
   "metadata": {},
   "source": [
    "標準的なアテンションは次のように計算できる:\n",
    "\n",
    "$$\n",
    "S = QK^{\\top} \\in \\mathbb{R}^{N \\times N}\n",
    "$$\n",
    "\n",
    "- $Q$: $N\\times d$ 行列のクエリ\n",
    "- $K^\\top$: $N\\times d$ 行列のキーの転置\n",
    "- $S$: $N\\times N$ 行列の生のアテンションスコア\n",
    "\n",
    "$$\n",
    "P = \\text{softmax}(S) \\in \\mathbb{R}^{N \\times N}\n",
    "$$\n",
    "\n",
    "- $P$: $N\\times N$ 行列のアテンションスコア（要素の総和が1になるテンソル）\n",
    "\n",
    "$$\n",
    "O = PV \\in \\mathbb{R}^{N \\times d}\n",
    "$$\n",
    "\n",
    "- $V$: $N\\times d$ 行列のバリュー\n",
    "- $O$: $N\\times d$ 行列の最終的な出力（バリューの重み付き和）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de23e1",
   "metadata": {},
   "source": [
    "標準的なアテンションの実装は、ソフトマックスの計算に全ての列の情報が必要\n",
    "\n",
    "その結果HBMとSRAM間の読み込みと書き込みが多く発生してしまう:\n",
    "\n",
    "1. HBMから $Q$ と $K$ をブロックごとにSRAMに **読み込み、** 生のアテンションスコア $S$ を計算し、HBMに **書き戻す**\n",
    "1. HBMから $S$ をSRAMに **読み込み、** アテンションスコア $P$ を計算しHBMに **書き戻す**\n",
    "1. HBMから $P$ と $V$ をブロックごとにSRAMに **読み込み、** $O$ を計算しHBMに **書き戻す**\n",
    "1. Oを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce19843",
   "metadata": {},
   "source": [
    "![](image/algorithm0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f75ee2",
   "metadata": {},
   "source": [
    "\n",
    "Flash Attentionは、低速なHBM（High Bandwidth Memory）と高速なSRAM間の読み書きが少なくなるように設計:\n",
    "\n",
    "1. 順伝播では、入力をブロックに分割し（tiling）、SRAM内でブロックごとのアテンションを計算する（図参照）\n",
    "2. 逆伝搬では、順伝播の中間計算結果（$S$と$P$）をHBMに保存せず、再計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219fb0f8",
   "metadata": {},
   "source": [
    "標準的なテンションのHBMへの読み書き回数（IO複雑性）は、系列長の2乗に比例:\n",
    "\n",
    "$$\n",
    "\\Omega(Nd + N^2)\n",
    "$$\n",
    "\n",
    "- $N$: 系列長\n",
    "- $d$: アテンションヘッドの次元数\n",
    "\n",
    "Flash AttentionのIO複雑性は、SRAMのサイズの逆数が乗算され、系列長に線形に比例し、最大9倍低くなる:\n",
    "\n",
    "$$\n",
    "O(N^2 d^2 M^{-1})\n",
    "$$\n",
    "\n",
    "- $M$: SRAMのサイズ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f0bb61",
   "metadata": {},
   "source": [
    "ベンチマークの結果:\n",
    "\n",
    "- モデルの学習を高速化できた\n",
    "    - 系列長512のBERT-largeの学習を15%高速化\n",
    "    - 系列長1KのGPT-2の学習を3倍高速化\n",
    "    - 系列長1Kから4Kのlong-range arenaの学習を2.4倍高速化\n",
    "- モデルの品質を改善できた\n",
    "    - GPT-2のパープレキシティを0.7改善\n",
    "    - 長文分類の性能を6.4ポイント改善\n",
    "- アテンション計算を高速化できた\n",
    "    - 一般的な系列長で最大3倍高速"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e86563",
   "metadata": {},
   "source": [
    "## Flash Attention\n",
    "\n",
    "Flash Attentionアルゴリズムのポイントは、**タイリング** と **逆伝搬時の統計量の再計算** にある"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62063dd",
   "metadata": {},
   "source": [
    "### タイリング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59010716",
   "metadata": {},
   "source": [
    "Flash Attentionは、データをブロックに分け（タイリング）SRAM内でアテンションを計算する\n",
    "\n",
    "SRAM内でアテンションを計算するため、入力全体ではなく部分的なソフトマックス計算が必要になる（オンラインソフトマックス）\n",
    "\n",
    "部分的なソフトマックス計算のために、標準的なソフトマックス関数を **分解** する:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef153b",
   "metadata": {},
   "source": [
    "分解するために、ベクトル $x$ のソフトマックス計算に対して、3つの特徴量 $m(x)$・$f(x)$・$l(x)$ を定義\n",
    "\n",
    "$m(x)$ は、$x$ の要素の **最大値** :\n",
    "\n",
    "$$\n",
    "m(x) := \\max_{i} x_i\n",
    "$$\n",
    "\n",
    "$f(x)$ は、**ソフトマックスの分子** （オーバーフローを防ぐため各要素から最大値を引き、指数関数を適用したベクトル）:\n",
    "\n",
    "$$\n",
    "f(x) := [e^{x_1 - m(x)}, ..., e^{x_B - m(x)}\n",
    "]\n",
    "$$\n",
    "\n",
    "$l(x)$ は、 **ソフトマックスの分母** （$f(x)$ の全要素の合計値）:\n",
    "\n",
    "$$\n",
    "l(x) := \\sum_{i} f(x)_i\n",
    "$$\n",
    "\n",
    "$x$ の最終的なソフトマックスの出力:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x) := \\frac{f(x)}{l(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7882a757",
   "metadata": {},
   "source": [
    "例えば、2つのベクトル $x^{(1)}, x^{(2)} \\in \\mathbb{R}^B$ を結合した $x =[x^{(1)} x^{(2)}] \\in \\mathbb{R}^{2B}$のソフトマックス計算は次のように分解できる:\n",
    "\n",
    "$m(x)$は、結合したベクトルの要素の最大値:\n",
    "\n",
    "$$\n",
    "m(x) = m([x^{(1)} x^{(2)}]) = \\max(m(x^{(1)}), m(x^{(2)}))\n",
    "$$\n",
    "\n",
    "$l(x)$は、結合したベクトルの$f(x)$の全要素の合計値（$x^{(1)}$と$x^{(2)}$のソフトマックスの分母をそれぞれの最大値で重み付けした和）:\n",
    "\n",
    "$$\n",
    "l(x) = l([x^{(1)} x^{(2)}]) = e^{m(x^{(1)}) - m(x)}l(x^{(1)}) + e^{m(x^{(2)}) - m(x)}l(x^{(2)})\n",
    "$$\n",
    "\n",
    "結合したベクトルの最終的なソフトマックスの出力:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x) = \\frac{f(x)}{l(x)}\n",
    "$$\n",
    "\n",
    "新しいベクトル（例えば $x^{(3)}$）が追加された場合は、計算済みの $m(x)$ と $l(x)$ を更新し、 $x^{(3)}$ を含めたソフトマックスを高速に計算できる\n",
    "\n",
    "この仕組みを応用することで、ブロックごとにSRAM内でアテンションを計算できる（Algorithm 1）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac621e3",
   "metadata": {},
   "source": [
    "### 再計算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef3fe5",
   "metadata": {},
   "source": [
    "標準的なアテンションの場合、逆伝搬時（$dQ$・$dK$・$dV$・$dO$ の計算時）に中間計算結果 $S$ （生のアテンションスコア）と $P$ （アテンションスコア）が必要\n",
    "\n",
    "Flash Attentionでは、中間計算結果 $S$ と $P$ をHBMに保存せず、再計算する\n",
    "\n",
    "生のアテンションスコア $S$ は、$Q$ と $K$ のブロックをSRAMに読み込み再計算する:\n",
    "\n",
    "$$\n",
    "S_{ij} = \\tau Q_i K_j^T\n",
    "$$\n",
    "\n",
    "- $Q_i$: $i$ 番目のクエリ\n",
    "- $K_j$: $j$ 番目のキー\n",
    "- $\\tau$ は、ソフトマックスのスケール定数\n",
    "\n",
    "アテンションスコア $P$ は、該当ブロックの $m$ と $l$ から再計算できる:\n",
    "\n",
    "$$\n",
    "P_{ij} = \\text{diag}(l_i)^{-1} \\exp{(S_{ij} - m_i)}\n",
    "$$\n",
    "\n",
    "- $\\text{diag}(l_i)^{-1}$: 対角行列の逆行列でソフトマックスの分母の逆数\n",
    "- $\\exp{(S_{ij} - m_i)}$: ソフトマックスの分子\n",
    "\n",
    "再計算により $S$ と $P$ をHBMに保存する必要がなくなり、メモリ使用量とメモリアクセスを削減できる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6e739",
   "metadata": {},
   "source": [
    "### アルゴリズム"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de93009",
   "metadata": {},
   "source": [
    "Flash Attentionは、小さなブロック単位でアテンションを計算し、メモリ使用量とメモリアクセスを削減する\n",
    "\n",
    "大まかな流れは、 $K$ ブロックと $V$ ブロックをロードして固定し、 $Q$ ブロックを順に処理し、対応する $O$ ブロックを計算する\n",
    "\n",
    "HBMに $Q$ ・ $K$ ・ $V$ があり、SRAMのサイズが $M$ とする\n",
    "\n",
    "1. SRAMサイズ $M$ に基づいて、 $K$ ・ $V$ から読み込むブロックの列数 $B_c$ と、$Q$ から読み込むブロックの行数 $B_r$ を設定\n",
    "2. HBMに、アテンションの計算結果 $O$・ソフトマックスの分母 $l$ ・生のアテンションスコアの最大値 $m$ を格納するメモリを作成\n",
    "3. $Q$ ・ $K$ ・ $V$ のブロックサイズを決定\n",
    "4. $O$ ・ $l$ ・ $m$ のブロックサイズを決定\n",
    "5. $K_j$ ブロックと $V_j$ ブロックを順番に処理する外側のループを開始\n",
    "6. HBMから$ K_j$ ブロック・ $V_j$ ブロックをSRAMに読み込む\n",
    "7. $Q_i$ ブロックを順番に処理する内側のループを開始\n",
    "8. $Q_i$ ブロック・ $O_i$ ブロック・ $l_i$ ブロック・ $m_i$ ブロックをSRAMに読み込む\n",
    "9. 生のアテンションスコア $S_{ij} = Q_iK_j^T$ を計算\n",
    "10. 生のアテンションスコアの要素の最大値 $\\tilde{m}_{ij} = \\text{rowmax}(S_{ij})$ を求め、アテンションスコア $\\tilde{P}_{ij} = \\exp{(S_{ij} - \\tilde{m}_{ij})}$ を計算し、ソフトマックスの分母 $\\tilde{l}_{ij} = \\text{rowsum}(\\tilde{P}_{ij})$ を計算\n",
    "11. 過去の $m_i$ と $l_i$ と、$\\tilde{m}_ij$ と $\\tilde{l}_{ij}$ を使って、新しい$m_i^{\\text{new}}$と$l_i^{\\text{new}}$を計算\n",
    "12. $m_i^{\\text{new}}$ と $l_i^{\\text{new}}$ で過去のアテンション $O_i$ をスケールして 、現在のブロックのアテンションを使って $O_i$ を更新し、HBMに書き戻す\n",
    "13. 次のループで使用する $m_i^{\\text{new}}$ と $l_i^{\\text{new}}$ を $m_i$ と $l_i$ としてHBMに書き戻す\n",
    "14. 内側のループ終了\n",
    "15. 外側のループ終了\n",
    "16. $O$を返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f287cf64",
   "metadata": {},
   "source": [
    "![](image/algorithm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066f446",
   "metadata": {},
   "source": [
    "## Flash Attentionの順伝播の詳細"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1548296",
   "metadata": {},
   "source": [
    "順伝播では、クエリ $Q$ ・キー $K$ ・バリュー $V$ からアテンション $O$ を求める:\n",
    "\n",
    "$$\n",
    "S = QK^T \\in \\mathbb{R}^{N\\times N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P = \\text{softmax}(S) \\in \\mathbb{R}^{N\\times N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "O = PV \\in \\mathbb{R}^{N\\times d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc7ffe",
   "metadata": {},
   "source": [
    "ブロックごとの生のアテンションスコア $S_{ij}$ は次式で求められる:\n",
    "\n",
    "$$\n",
    "S_{ij} = q_i^T k_j\n",
    "$$\n",
    "\n",
    "- $q_i$: $i$番目のクエリブロック\n",
    "- $k_j$: $j$番目のキーブロック"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f108c",
   "metadata": {},
   "source": [
    "ソフトマックスの分母は次式で求められる:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d659e30d",
   "metadata": {},
   "source": [
    "$$\n",
    "L_i = \\sum_{j} e^{q_i^T k_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329128e7",
   "metadata": {},
   "source": [
    "$v_j$ を $j$ 番目のバリューブロックとすると、出力の $i$ 番目のブロックのアテンションは次式で求められる:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00b4d4",
   "metadata": {},
   "source": [
    "$$\n",
    "o_i = P_{i:}V = \\sum_{j} P_{ij}v_j = \\sum_{j} \\frac{e^{q_i^T k_j}}{L_i}v_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649ee5f",
   "metadata": {},
   "source": [
    "- $P_{ij}$: $i$ 番目のクエリブロックと $j$ 番目のキーブロックのアテンションスコア"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa1452b",
   "metadata": {},
   "source": [
    "以上の計算式で、$L_i$ を計算し、 $\\frac{e^{q_i^T k_j}}{L_i}v_j$ を繰り返し足し合わせることで、$O(N)$の線形メモリで全てのアテンション $O$ を計算できる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5d837e",
   "metadata": {},
   "source": [
    "因果マスクとドロップアウトを考慮した完全な順伝播アルゴリズム:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5164c47e",
   "metadata": {},
   "source": [
    "1. 乱数生成器の状態 $\\mathcal{R}$ を初期化し、HBMに保存\n",
    "2. SRAMサイズ $M$ に基づいて、$K$ ・$V$ から読み込むブロックごとの列数 $B_c$ と、Qから読み込むブロックごとの行数 $B_r$ を決定\n",
    "3. $O$ ・ $l$ ・ $m$ をHBMに初期化\n",
    "4. $Q$ ・ $K$ ・ $V$ のブロックサイズを決定\n",
    "5. $O$ ・ $T$ ・ $m$ のブロックサイズを決定\n",
    "6. $K_j$ ブロック・ $V_j$ ブロックを順番に処理する外側のループを開始\n",
    "7. $K_j$ ブロック・ $V_j$ ブロックをSRAMに読み込む\n",
    "8. $Q_i$ ブロックを順番に処理する内側のループ\n",
    "9. HBMから $K_j$ ブロックと $V_j$ ブロックをSRAMに読み込む\n",
    "10. 生のアテンションスコアを計算 $S_{ij} = \\tau Q_j K_j$\n",
    "11. 生のアテンションスコアに因果マスクを適用 $S_{ij}^{\\text{masked}}$\n",
    "12. 生のアテンションスコアの最大値 $\\tilde{m_{ij}}$ を求め、アテンションスコア $\\tilde{P_{ij}}$ とソフトマックスの分母 $\\tilde{l}_{ij}$ を計算\n",
    "13. 最大値を更新し $m_i^{\\text{new}}$、ソフトマックスの分子を更新 $l_i^{new}$\n",
    "14. アテンションスコアにドロップアウトを適用 $P_{ij}^{\\text{\\text{dropped}}}$\n",
    "15. $l_i^{\\text{new}}$ と $m_i^{\\text{new}}$ でアテンション $O_i$ を更新し、HBMに書き戻す\n",
    "16. $m_i^{\\text{new}}$ と $l_i^{\\text{new}}$ を $m_i$ と $l_i$ としてHBMに書き戻す\n",
    "17. 内側のループ終了\n",
    "18. 外側のループ終了\n",
    "19. $O$ を返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d91788",
   "metadata": {},
   "source": [
    "![](image/algorithm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438092cd",
   "metadata": {},
   "source": [
    "## Flash Attentionの逆伝播詳細"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b25257",
   "metadata": {},
   "source": [
    "逆伝搬では、出力の勾配からクエリ・キー・バリューの勾配を求める:\n",
    "\n",
    "- $\\phi$: スカラーの損失関数\n",
    "- $dO \\in \\mathbb{R}^{n\\times d}$: 出力の勾配 $\\frac{\\partial{\\phi}}{\\partial{O}}$\n",
    "- $dQ \\in \\mathbb{R}^{n\\times d}$: クエリの勾配 $\\frac{\\partial{\\phi}}{\\partial{Q}}$\n",
    "- $dK \\in \\mathbb{R}^{n\\times d}$: キーの勾配 $\\frac{\\partial{\\phi}}{\\partial{K}}$\n",
    "- $dV \\in \\mathbb{R}^{n\\times d}$: バリューの勾配 $\\frac{\\partial{\\phi}}{\\partial{V}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c1d96f",
   "metadata": {},
   "source": [
    "$dV$ は連鎖律より、$dV = P^T dO$ で求められる:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed61b0d",
   "metadata": {},
   "source": [
    "$$\n",
    "dv_j = \\sum_{i} P_{ij} do_i = \\sum_{i} \\frac{e^{q_i^T k_j}}{L_i} do_i\n",
    "$$\n",
    "\n",
    "- $L_i$ は、順伝播で計算済みのソフトマックスの分母\n",
    "- $dv_j$ は、繰り返し足し合わせることで追加のメモリ無しで計算が可能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af273aa",
   "metadata": {},
   "source": [
    "$dQ$ と $dK$ を求めるために、$dP$ と $dS$ が必要\n",
    "\n",
    "$dP$ は $dP = dOV^T$ で求められる:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3cb22",
   "metadata": {},
   "source": [
    "$$\n",
    "dP_{ij} = do_i^T v_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aec5e3",
   "metadata": {},
   "source": [
    "$dS$は、$y=\\text{softmax}(x)$のヤコビアンが$diag(y) - yy^T$という事実より求められる:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d9541",
   "metadata": {},
   "source": [
    "$$\n",
    "dS_{i:} = (diag(P_{i:}) - P_{i:}^T P_{i:}) dP_{i:} = P_{i:} \\circ dP_{i:} - (P_{i:}^T dP_{i:}) P_{i:}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7713ba",
   "metadata": {},
   "source": [
    "$$\n",
    "D_i = P_{i:}^T dP_{i:} = \\sum_{j} \\frac{e^{q_i^T k_j}}{L_i} do_i^T v_j = do_i^T \\sum_{j} \\frac{e^{q_i^T k_j}}{L_i} v_j = do_i^T o_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b558cd7",
   "metadata": {},
   "source": [
    "$$\n",
    "dS_{i:} = P_{i:} \\circ dP_{i:} - D_i P_{i:}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e1ca5",
   "metadata": {},
   "source": [
    "$dQ$ は、$S_{ij} = q_i^T k_j$ より計算できる:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae5274",
   "metadata": {},
   "source": [
    "$$\n",
    "dq_i = \\sum_{j} dS_{ij} k_j = \\sum_{j} P_{ij}(dP_{ij} - D_i)k_j = \\sum_{j} \\frac{e^{q_i^T k_j}}{L_i}(do_i^T v_j - D_i)k_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d08d91",
   "metadata": {},
   "source": [
    "$dK$も同様に求められる:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a1a03",
   "metadata": {},
   "source": [
    "$$\n",
    "dk_j = \\sum_{i} dS_{ij} q_i = \\sum_{i} P_{ij}(dP_{ij} - D_i)q_i = \\sum_{i} \\frac{e^{q_i^T k_j}}{L_i}(do_i^T v_j - D_i)q_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dce774",
   "metadata": {},
   "source": [
    "以上の計算式で、逆伝搬も $O(N)$ の線形メモリで計算できる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588a822",
   "metadata": {},
   "source": [
    "標準的な逆伝播アルゴリズム:\n",
    "\n",
    "![](image/algorithm3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca12ffb",
   "metadata": {},
   "source": [
    "Flash Attentionでの逆伝播アルゴリズム:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa93187",
   "metadata": {},
   "source": [
    "$K$と$V$のブロックをロードし、$Q$ブロックを順番に処理して、KとVの勾配を足し続ける"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7faaac2",
   "metadata": {},
   "source": [
    "1. 順伝播で使用した乱数生成器の状態 $\\mathcal{R}$ を復元\n",
    "2. $K$・$V$から読み込むブロックの列数$B_c$と$Q$から読み込むブロックの行数$B_r$を設定\n",
    "3. $Q$・$K$・$V$・のブロックサイズを決定\n",
    "4. $O$・$l$・$m$のブロックサイズを決定\n",
    "5. $dQ$・$dK$・$dV$を初期化し、ブロックサイズを決定\n",
    "6. $K_j$ブロックと$V_j$ブロックを順番に処理する外側のループを開始\n",
    "7. HBMから$K_j$・$V_j$をSRAMに読み込む\n",
    "8. SRAMで$\\tilde{dK_j}$と$\\tilde{dV_j}$を0で初期化\n",
    "9. $Q_i$ブロックを順番に処理する内側のループを開始\n",
    "10. HBMから$Q_i$・$O_i$・$dO_i$・$dQ_i$・$l_i$・$m_i$を読み込む\n",
    "11. 生のアテンションスコアを計算 $S_ij = \\tau Q_i K_j^T$（$\\tau$ は、ソフトマックスのスケール定数）\n",
    "12. 因果マスクを適用 $S_{ij}^{\\text{masked}} = \\text{MASK}(S_{ij})$\n",
    "13. アテンションスコアを計算 $P_{ij} = \\text{diag}(l_i)^{-1} \\exp{(S_{ij}^{\\text{masked}} - m_i)}$\n",
    "14. 乱数生成器からドロップアウト用のマスクを復元\n",
    "15. アテンションスコアにドロップアウトを適用 $P_{ij}^{\\text{dropped}} = P_{ij} Z_{ij}$\n",
    "16. $\\tilde{dV}$に$(P_{ij}^{\\text{dropped}})^T dO_i$を加算\n",
    "17. $dP_{ij}^{\\text{dropped}} = dO_iV_j^T$を計算\n",
    "18. $dP_{ij} = dP_{ij}^{\\text{dropped}} Z_{ij}$を計算\n",
    "19. $D_i = \\text{rowsum}(dO_i O_i)$を計算\n",
    "20. $dS_{ij} = P_{ij} (dP_{ij} - D_i)$を計算\n",
    "21. $dQ_i$に$\\tau dS_{ij}K_j$を加算し、HBMに書き込む\n",
    "22. $\\tilde{dK_j}$に$\\tau dS_{ij}^T Q_i$を加算\n",
    "23. 内側のループ終了\n",
    "24. $\\tilde{dK_j}$を$dK$として、$\\tilde{dV_j}$を$dV_j$としてHBMに書き戻す\n",
    "25. 外側のループ終了\n",
    "26. $dQ$・$dK$・$dV$を返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c32330",
   "metadata": {},
   "source": [
    "![](image/algorithm4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f6473",
   "metadata": {},
   "source": [
    "## 実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd644d",
   "metadata": {},
   "source": [
    "### 環境構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab906351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: triton in /opt/miniconda/envs/py312/lib/python3.12/site-packages (3.4.0)\n",
      "Requirement already satisfied: tabulate in /opt/miniconda/envs/py312/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: pytest in /opt/miniconda/envs/py312/lib/python3.12/site-packages (8.4.2)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from triton) (78.1.1)\n",
      "Requirement already satisfied: iniconfig>=1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pytest) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pytest) (25.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pytest) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pytest) (2.19.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🟩 Tritonバージョン: 3.4.0\n",
      "🟩 使用デバイス: cuda:0\n"
     ]
    }
   ],
   "source": [
    "%pip install triton tabulate pytest\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import tabulate\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.tools.tensor_descriptor import TensorDescriptor\n",
    "import pytest\n",
    "\n",
    "# ログ設定\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = '🟦'\n",
    "        case logging.INFO:\n",
    "            level = '🟩'\n",
    "        case logging.WARNING:\n",
    "            level = '🟨'\n",
    "        case logging.ERROR:\n",
    "            level = '🟥'\n",
    "        case logging.CRITICAL:\n",
    "            level = '🛑'\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.NOTSET)\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "if os.path.exists('triton.log'):\n",
    "    os.remove('triton.log')\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "file_handler = logging.FileHandler('triton.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# 3.4.0\n",
    "logger.info(f'Tritonバージョン: {triton.__version__}')\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "logger.info(f'使用デバイス: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dabb29",
   "metadata": {},
   "source": [
    "### ユーティリティ関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768451d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "is_hip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519f3352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_cuda():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n",
    "\n",
    "is_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6110252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def supports_host_descriptor():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] >= 9\n",
    "\n",
    "supports_host_descriptor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa483e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_blackwell():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] == 10\n",
    "\n",
    "is_blackwell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907d3057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_hopper():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] == 9\n",
    "\n",
    "is_hopper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディスクリプタの設定\n",
    "# ディスクリプタは、HBM上の巨大なテンソルからデータブロックを読み込む際の形状\n",
    "\n",
    "def _host_descriptor_pre_hook(nargs):\n",
    "    BLOCK_M = nargs[\"BLOCK_M\"]\n",
    "    BLOCK_N = nargs[\"BLOCK_N\"]\n",
    "    HEAD_DIM = nargs[\"HEAD_DIM\"]\n",
    "\n",
    "    if not isinstance(nargs[\"desc_q\"], TensorDescriptor):\n",
    "        return\n",
    "\n",
    "    # QのブロックをSRAMにロードする際の形状を (BLOCK_M, HEAD_DIM) に設定\n",
    "    # BLOCK_M個のクエリベクトル全体（HEAD_DIM）を読み込む\n",
    "    nargs[\"desc_q\"].block_shape = [BLOCK_M, HEAD_DIM]\n",
    "\n",
    "    # FP8を使用する場合は、メモリレイアウトが異なる\n",
    "    if nargs[\"FP8_OUTPUT\"]:\n",
    "        nargs[\"desc_v\"].block_shape = [HEAD_DIM, BLOCK_N]\n",
    "    else:\n",
    "        # VのブロックをSRAMにロードする際の形状を (BLOCK_N, HEAD_DIM) に設定\n",
    "        # BLOCK_N個のバリューベクトル全体（HEAD_DIM）を読み込む\n",
    "        nargs[\"desc_v\"].block_shape = [BLOCK_N, HEAD_DIM]\n",
    "\n",
    "    # KのブロックをSRAMにロードする際の形状を (BLOCK_N, HEAD_DIM) に設定\n",
    "    # BLOCK_N個のキーベクトル全体（HEAD_DIM）を読み込む\n",
    "    nargs[\"desc_k\"].block_shape = [BLOCK_N, HEAD_DIM]\n",
    "\n",
    "    # OのブロックをSRAMにロードする際の形状を (BLOCK_M, HEAD_DIM) に設定\n",
    "    # BLOCK_M個の出力ベクトル全体（HEAD_DIM）を読み込む\n",
    "    nargs[\"desc_o\"].block_shape = [BLOCK_M, HEAD_DIM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87665e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep(conf):\n",
    "    BLOCK_M = conf.kwargs[\"BLOCK_M\"]\n",
    "    BLOCK_N = conf.kwargs[\"BLOCK_N\"]\n",
    "    return not (\n",
    "        is_cuda() and \\\n",
    "        torch.cuda.get_device_capability()[0] == 9 and \\\n",
    "        BLOCK_M * BLOCK_N < 128 * 128 and \\\n",
    "        conf.num_warps == 8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa859a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_invalid_configs(configs, named_args, **kwargs):\n",
    "    N_CTX = kwargs[\"N_CTX\"]\n",
    "\n",
    "    # Filter out configs where BLOCK_M > N_CTX\n",
    "    return [conf for conf in configs if conf.kwargs.get(\"BLOCK_M\", 0) <= N_CTX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "742594b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _maybe_make_tensor_desc(desc_or_ptr, shape, strides, block_shape):\n",
    "    if isinstance(desc_or_ptr, tl.tensor_descriptor):\n",
    "        return desc_or_ptr\n",
    "    else:\n",
    "        return tl.make_tensor_descriptor(desc_or_ptr, shape, strides, block_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c63c9cc",
   "metadata": {},
   "source": [
    "### 自動チューナーの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218a530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ソフトウェアパイプラインのステージ数\n",
    "# 多いと並列実行のレイテンシが改善するが、メモリ使用量も増加する\n",
    "\n",
    "if is_hip():\n",
    "    NUM_STAGES_OPTIONS = [1]\n",
    "elif supports_host_descriptor():\n",
    "    NUM_STAGES_OPTIONS = [2, 3, 4]\n",
    "else:\n",
    "    NUM_STAGES_OPTIONS = [2, 3, 4]\n",
    "\n",
    "NUM_STAGES_OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a10f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<triton.runtime.autotuner.Config at 0x759d8663c4d0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c2f0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663cc50>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c080>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c5f0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c410>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c530>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c560>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c5c0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c620>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c650>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c680>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c6b0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c6e0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c710>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c740>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c770>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c7a0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c7d0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c800>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c830>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c860>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c890>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c8c0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c8f0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c920>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c950>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c980>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c9b0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c9e0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663ca10>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663ca40>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663ca70>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663caa0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663cad0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663cb00>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ブロックごとのクエリの行数BMは64または128\n",
    "# ブロックごとのキー/バリューの行数BNは32、64、128\n",
    "# 一つのブロックを処理するために使用するワープ数は4または8\n",
    "configs = [\n",
    "    triton.Config({'BLOCK_M': BM, 'BLOCK_N': BN}, num_stages=s, num_warps=w, pre_hook=_host_descriptor_pre_hook) \\\n",
    "    for BM in [64, 128]\\\n",
    "    for BN in [32, 64, 128]\\\n",
    "    for s in NUM_STAGES_OPTIONS \\\n",
    "    for w in [4, 8]\\\n",
    "]\n",
    "\n",
    "# テスト用の設定\n",
    "if \"PYTEST_VERSION\" in os.environ:\n",
    "    configs = [\n",
    "        triton.Config(dict(BLOCK_M=128, BLOCK_N=64), num_stages=2, num_warps=4, pre_hook=_host_descriptor_pre_hook),\n",
    "    ]\n",
    "\n",
    "configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a97b2",
   "metadata": {},
   "source": [
    "### 順伝搬カーネル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd94371",
   "metadata": {},
   "source": [
    "#### _attn_fwd_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb42f74",
   "metadata": {},
   "source": [
    "Algorithm 2の実装\n",
    "\n",
    "**1つのクエリのブロック** に対して、キー・バリューの全ブロックを順に処理し、アキュームレータを更新するワーカーカーネル\n",
    "\n",
    "アキュームレータ`acc`は、アテンションスコア $P$ とバリュー$V$ の行列積\n",
    "\n",
    "ラッパーカーネル（`_attn_fwd`）で、アキュームレータをソフトマックスの分母$l$で割ることでアテンションを計算する\n",
    "\n",
    "因果マスクを適用するため、アテンションスコアの対角成分上のブロックとそれ以外で異なる計算:\n",
    "\n",
    "- STAGE == 1: アテンション行列の対角成分以外のブロック全てに対する処理（因果マスクを考慮しない）\n",
    "- STAGE == 2: アテンション行列の対角成分のブロックの処理（因果マスクを考慮する）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4cbed6",
   "metadata": {},
   "source": [
    "アテンション行列と因果マスク（[ref](https://livebook.manning.com/wiki/categories/llm/causal+attention)）:\n",
    "\n",
    "![](image/causal_attention.png)\n",
    "\n",
    "※ Tritonではクエリ単位ではなく、ブロック単位（複数のクエリ）で処理を行うため図の解釈に注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_fwd_inner(\n",
    "    acc, # 出力を集約するアキュームレータ\n",
    "    l_i, # 前ステップまでのソフトマックスの分母\n",
    "    m_i, # 前ステップまでの生のアテンションスコアの要素の最大値\n",
    "    q, # SRAMにロード済みのクエリのブロック (BLOCK_M, HEAD_DIM)\n",
    "    desc_k, # HBM上のキーのディスクリプタ\n",
    "    desc_v,  # HBM上のバリューのディスクリプタ\n",
    "    offset_y, # ベースオフセット（特定のバッチの特定のヘッドのデータの開始位置）\n",
    "    dtype: tl.constexpr, # 計算に使用するデータ型\n",
    "    start_m, # Qブロックの開始行インデックス\n",
    "    qk_scale, # QKに掛けるスケーリング係数\n",
    "    BLOCK_M: tl.constexpr, # Qブロックの行数\n",
    "    HEAD_DIM: tl.constexpr, # アテンションヘッドの次元数\n",
    "    BLOCK_N: tl.constexpr, # KブロックもしくはVブロックの行数\n",
    "    STAGE: tl.constexpr, # 因果マスキングのステージ指定\n",
    "    offs_m: tl.constexpr, # Qブロックの行インデックスの配列 (BLOCK_M,)\n",
    "    offs_n: tl.constexpr, # Kブロックの列インデックスの配列 (BLOCK_N,)\n",
    "    N_CTX: tl.constexpr, # シーケンスの長さ（コンテキスト長） N\n",
    "    warp_specialize: tl.constexpr, # ワープ単位での最適化のフラグ\n",
    "    IS_HOPPER: tl.constexpr # NVIDIA Hopperアーキテクチャのフラグ\n",
    "):\n",
    "\n",
    "    #################\n",
    "    # ポインタの初期化 #\n",
    "    #################\n",
    "\n",
    "    # アテンション行列の対角成分より前の部分を処理する場合（off-diagonal）\n",
    "    if STAGE == 1:\n",
    "        # Qのブロックよりも前の位置にあるKとVのブロックが計算範囲\n",
    "        lo, hi = 0, start_m * BLOCK_M\n",
    "\n",
    "    # アテンション行列の対角成分に対する処理する場合（on-diagonal）\n",
    "    elif STAGE == 2:\n",
    "        # 現在のQブロックと同じ位置にあるKとVのブロックが計算範囲\n",
    "        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n",
    "        lo = tl.multiple_of(lo, BLOCK_M)\n",
    "\n",
    "    # 因果マスキングが不要な場合\n",
    "    else:\n",
    "        # シーケンス全体にわたるKとVのブロックが計算範囲\n",
    "        lo, hi = 0, N_CTX\n",
    "\n",
    "    # Kブロックのオフセットを計算\n",
    "    offsetk_y = offset_y + lo\n",
    "\n",
    "    # Vブロックのオフセットを計算\n",
    "    # 注意: FP8データ型の場合、Vが転置されているためオフセット計算が異なる\n",
    "    if dtype == tl.float8e5:\n",
    "        offsetv_y = offset_y * HEAD_DIM + lo\n",
    "    else:\n",
    "        offsetv_y = offset_y + lo\n",
    "\n",
    "    #######################################\n",
    "    # KブロックとVブロックを順番に処理するループ #\n",
    "    #######################################\n",
    "\n",
    "    # ポインタの開始位置をBLOCK_Nずつ勧めながら、KとVのブロックを順番に処理\n",
    "    for start_n in tl.range(lo, hi, BLOCK_N, warp_specialize=warp_specialize):\n",
    "\n",
    "        # コンパイラへのヒント（KとVのブロックの開始行のインデックスはBLOCK_Nの倍数）\n",
    "        start_n = tl.multiple_of(start_n, BLOCK_N)\n",
    "\n",
    "        # KのブロックをSRAMにロードし、転置\n",
    "        # (BLOCK_N, HEAD_DIM) -> (HEAD_DIM, BLOCK_N)\n",
    "        k = desc_k.load([offsetk_y, 0]).T\n",
    "\n",
    "        # 生のアテンションスコアQKを計算\n",
    "        # S_ij = Qi_K_j^T\n",
    "        # (BLOCK_M, HEAD_DIM) @ (HEAD_DIM, BLOCK_N) -> (BLOCK_M, BLOCK_N)\n",
    "        qk = tl.dot(q, k)\n",
    "\n",
    "        # アテンション行列の対角の場合（on-diagonal）\n",
    "        if STAGE == 2:\n",
    "\n",
    "            # 「Qの行インデックス >= Kの列インデックス」を満たす2次元因果マスクを作成\n",
    "            mask = offs_m[:, None] >= (start_n + offs_n[None, :])\n",
    "\n",
    "            # 生のアテンションスコアQKのスケールを調整し、マスクを適用\n",
    "            # S_ij^{masked} = MASK(\\tau S_{ij})\n",
    "            # (BLOCK_M, BLOCK_N)\n",
    "            qk = qk * qk_scale + tl.where(mask, 0, -1.0e6)\n",
    "\n",
    "            # 調整したアテンションスコアQKの要素の最大値を更新\n",
    "            # m_i^{new} = max(m_i, rowmax(S_ij^{masked}))\n",
    "            # (BLOCK_M,)\n",
    "            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n",
    "\n",
    "            # 調整したアテンションスコアから最大値を引く\n",
    "            # S_ij = S_ij^{masked} - m_i^{new}\n",
    "            # (BLOCK_M, BLOCK_N) - (BLOCK_M, 1) -> (BLOCK_M, BLOCK_N)\n",
    "            qk -= m_ij[:, None]\n",
    "\n",
    "        # アテンション行列の対角成分より前の部分の場合（off-diagonal）\n",
    "        else:\n",
    "            # 生のアテンションスコアQKのスケールを調整し、最大値を更新\n",
    "            # m_i^{new} = max(m_i, rowmax(τ S_ij) * \\tau)\n",
    "            # (BLOCK_M,)\n",
    "            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n",
    "\n",
    "            # 調整したアテンションスコアから最大値を引く\n",
    "            # S_ij = S_ij - m_i^{new}\n",
    "            # (BLOCK_M, BLOCK_N) - (BLOCK_M, 1) -> (BLOCK_M, BLOCK_N)\n",
    "            qk = qk * qk_scale - m_ij[:, None]\n",
    "\n",
    "        # アテンションスコアPを計算（指数ではなく2のべき乗を使用することで高速化）\n",
    "        # \\tilde{P}_{ij} = \\exp(S_{ij})\n",
    "        # (BLOCK_M, BLOCK_N)\n",
    "        p = tl.math.exp2(qk)\n",
    "\n",
    "        # 最大値が更新されたため、再スケーリング用のアルファを計算（e^{m_i-m_i^{new}}\n",
    "        # alpha = exp(m_i - m_i^{new})\n",
    "        # (BLOCK_M,)\n",
    "        alpha = tl.math.exp2(m_i - m_ij)\n",
    "\n",
    "        # ソフトマックスの分母を計算（アテンションスコアを列方向に潰して合計）\n",
    "        # \\tilde{l}_{ij} = rowsum(\\tilde{P}_{ij})\n",
    "        # (BLOCK_M,)\n",
    "        l_ij = tl.sum(p, 1)\n",
    "\n",
    "        # 出力のアキュームレータを更新\n",
    "        if not IS_HOPPER and warp_specialize and BLOCK_M == 128 and HEAD_DIM == 128:\n",
    "            BM: tl.constexpr = acc.shape[0]\n",
    "            BN: tl.constexpr = acc.shape[1]\n",
    "            acc0, acc1 = acc.reshape([BM, 2, BN // 2]).permute(0, 2, 1).split()\n",
    "            acc0 = acc0 * alpha[:, None]\n",
    "            acc1 = acc1 * alpha[:, None]\n",
    "            acc = tl.join(acc0, acc1).permute(0, 2, 1).reshape([BM, BN])\n",
    "        else:\n",
    "            # 過去に計算したアキュームレータをアルファでスケーリング（新しい最大値の基準に合わせる）\n",
    "            # O_i = O_i * alpha\n",
    "            acc = acc * alpha[:, None]\n",
    "\n",
    "        # VブロックをSRAMにロード\n",
    "        # FP8データ型の場合はメモリレイアウトが異なる（Blockwell世代以前）\n",
    "        if dtype == tl.float8e5:\n",
    "            # (HEAD_DIM, BLOCK_N) -> (BLOCK_N, HEAD_DIM)\n",
    "            v = desc_v.load([0, offsetv_y]).T\n",
    "        else:\n",
    "            # (BLOCK_N, HEAD_DIM)\n",
    "            v = desc_v.load([offsetv_y, 0])\n",
    "\n",
    "        # アテンションスコアPをキャスト\n",
    "        p = p.to(dtype)\n",
    "\n",
    "        # PとVの内積を計算し、アキュームレータに加算\n",
    "        # O_i += \\tilde{P}_{ij} V_j\n",
    "        # (BLOCK_M, BLOCK_N) @ (BLOCK_N, HEAD_DIM) -> (BLOCK_M, HEAD_DIM)\n",
    "        acc = tl.dot(p, v, acc)\n",
    "\n",
    "        ###################\n",
    "        # 次のループへの準備 #\n",
    "        ###################\n",
    "\n",
    "        # ソフトマックスの分母を更新\n",
    "        # l_i^{new} = l_i * alpha + \\tilde{l}_{ij}\n",
    "        l_i = l_i * alpha + l_ij\n",
    "\n",
    "        # 最大値を更新\n",
    "        m_i = m_ij\n",
    "\n",
    "        # Kのブロックのオフセットを更新\n",
    "        offsetk_y += BLOCK_N\n",
    "\n",
    "        # Vのブロックのオフセットを更新\n",
    "        offsetv_y += BLOCK_N\n",
    "\n",
    "    return acc, l_i, m_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec8aa06",
   "metadata": {},
   "source": [
    "#### _attn_fwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac96426d",
   "metadata": {},
   "source": [
    "**1つのクエリブロックに対する** 全てのアテンション計算を実行するマネージャー\n",
    "\n",
    "担当するブロックのクエリテンソルを読み込み、STAGEを決定し、_attn_fwd_innerを呼び出し、アテンションの計算を完成させる\n",
    "\n",
    "因果マスクを適応する場合、対角成分以外のブロックを処理し、対角成分のブロックを続けて処理する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4284fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自動チューナーの設定\n",
    "@triton.autotune(\n",
    "    configs=list(filter(keep, configs)),\n",
    "    key=[\"N_CTX\", \"HEAD_DIM\", \"FP8_OUTPUT\", \"warp_specialize\"],\n",
    "    prune_configs_by={'early_config_prune': prune_invalid_configs}\n",
    ")\n",
    "@triton.jit\n",
    "def _attn_fwd(\n",
    "    sm_scale, # QKに掛けるスケーリング係数\n",
    "    M, # 生のアテンションスコアの要素の最大値mへのポインタ\n",
    "    Z, # バッチサイズ\n",
    "    H, # ヘッド数\n",
    "    desc_q, # HBM上のQのディスクリプタ\n",
    "    desc_k, # HBM上のKのディスクリプタ\n",
    "    desc_v, # HBM上のVのディスクリプタ\n",
    "    desc_o, # HBM上のOのディスクリプタ\n",
    "    N_CTX, # シーケンスの長さ（コンテキスト長）\n",
    "    HEAD_DIM: tl.constexpr, # アテンションヘッドの次元数 d\n",
    "    BLOCK_M: tl.constexpr, # Qブロックの行数 B_r\n",
    "    BLOCK_N: tl.constexpr, # KブロックもしくはVブロックの列数 B_c\n",
    "    FP8_OUTPUT: tl.constexpr, # FP8データ型を使用するフラグ\n",
    "    STAGE: tl.constexpr, # 因果マスクを使う場合は3、使わない場合は1\n",
    "    warp_specialize: tl.constexpr, # ワープの処理を最適化するフラグ\n",
    "    IS_HOPPER: tl.constexpr, # NVIDIA Hopperアーキテクチャのフラグ\n",
    "):\n",
    "\n",
    "    #########\n",
    "    # 初期化 #\n",
    "    #########\n",
    "\n",
    "    dtype = tl.float8e5 if FP8_OUTPUT else tl.float16\n",
    "\n",
    "    tl.static_assert(BLOCK_N <= HEAD_DIM)\n",
    "\n",
    "    # 現在のプログラムIDの0次元目を取得し、Qのブロックのインデックスとする\n",
    "    start_m = tl.program_id(0)\n",
    "\n",
    "    # 現在のプログラムIDの1次元目を取得し、ヘッドとバッチの複合インデックスとする\n",
    "    off_hz = tl.program_id(1)\n",
    "\n",
    "    # 現在のプログラムインスタンスが担当するバッチインデックスを計算\n",
    "    off_z = off_hz // H\n",
    "\n",
    "    # 現在のプログラムインスタンスが担当するヘッドインデックスを計算\n",
    "    off_h = off_hz % H\n",
    "\n",
    "    # バッチとヘッド全体でのトークンの総数（平坦化に必要）\n",
    "    y_dim = Z * H * N_CTX\n",
    "\n",
    "    ####################\n",
    "    # ディスクリプタを作成 #\n",
    "    ####################\n",
    "\n",
    "    # HBM上のQにアクセスするディスクリプタを作成\n",
    "    desc_q = _maybe_make_tensor_desc(\n",
    "        desc_q,\n",
    "        shape=[y_dim, HEAD_DIM], # テンソルの形状\n",
    "        strides=[HEAD_DIM, 1], # メモリ上でのデータの並び順\n",
    "        block_shape=[BLOCK_M, HEAD_DIM] # SRAMにロードするブロックの形状（BLOCK_M個のクエリベクトル全体を読み込む）\n",
    "    )\n",
    "\n",
    "    # HBM上のVにアクセスするディスクリプタを作成\n",
    "    # FP8を使用する場合は、Vが転置されている\n",
    "    if FP8_OUTPUT:\n",
    "        desc_v = _maybe_make_tensor_desc(\n",
    "            desc_v,\n",
    "            shape=[HEAD_DIM, y_dim], # テンソルの形状\n",
    "            strides=[N_CTX, 1], # メモリ上でのデータの並び順\n",
    "            block_shape=[HEAD_DIM, BLOCK_N] # SRAMにロードするブロックの形状\n",
    "        )\n",
    "    else:\n",
    "        desc_v = _maybe_make_tensor_desc(\n",
    "            desc_v, # HBM上のVにアクセスするディスクリプタ\n",
    "            shape=[y_dim, HEAD_DIM], # テンソルの形状\n",
    "            strides=[HEAD_DIM, 1], # メモリ上でのデータの並び順\n",
    "            block_shape=[BLOCK_N, HEAD_DIM] # SRAMにロードするブロックの形状（BLOCK_N個のバリューベクトル全体を読み込む）\n",
    "        )\n",
    "\n",
    "    # HBM上のKにアクセスするディスクリプタを作成\n",
    "    desc_k = _maybe_make_tensor_desc(\n",
    "        desc_k,\n",
    "        shape=[y_dim, HEAD_DIM], # テンソルの形状\n",
    "        strides=[HEAD_DIM, 1], # メモリ上でのデータの並び順\n",
    "        block_shape=[BLOCK_N, HEAD_DIM] # SRAMにロードするブロックの形状（BLOCK_N個のキーベクトル全体を読み込む）\n",
    "    )\n",
    "\n",
    "    # HBM上のOにアクセスするディスクリプタを作成\n",
    "    desc_o = _maybe_make_tensor_desc(\n",
    "        desc_o,\n",
    "        shape=[y_dim, HEAD_DIM], # テンソルの形状\n",
    "        strides=[HEAD_DIM, 1], # メモリ上でのデータの並び順\n",
    "        block_shape=[BLOCK_M, HEAD_DIM] # SRAMにロードするブロックの形状（BLOCK_M個の出力ベクトル全体を読み込む）\n",
    "    )\n",
    "\n",
    "    ######################\n",
    "    # 変数とポインタの初期化 #\n",
    "    ######################\n",
    "\n",
    "    # 担当するベースオフセットを計算\n",
    "    # バッチオフセット * シーケンス長 * ヘッド数 + ヘッドオフセット * シーケンス長\n",
    "    offset_y = off_z * (N_CTX * H) + off_h * N_CTX\n",
    "\n",
    "    # 担当するQブロックのオフセットを計算\n",
    "    # ベースオフセット + Qブロックのインデックス * Qブロックの行数\n",
    "    qo_offset_y = offset_y + start_m * BLOCK_M\n",
    "\n",
    "    # Qブロックの行インデックスの配列を作成\n",
    "    # (BLOCK_M,)\n",
    "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "\n",
    "    # KブロックもしくはVブロックの列インデックスの配列を作成\n",
    "    # (BLOCK_N,)\n",
    "    offs_n = tl.arange(0, BLOCK_N)\n",
    "\n",
    "    # 生のアテンションスコアの最大値をマイナス無限大で初期化\n",
    "    # (BLOCK_M,)\n",
    "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n",
    "\n",
    "    # ソフトマックスの分母を1.0で初期化\n",
    "    # (BLOCK_M,)\n",
    "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n",
    "\n",
    "    # 出力のアキュームレータを0で初期化\n",
    "    # (BLOCK_M, HEAD_DIM)\n",
    "    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    ################################\n",
    "    # Qのロードとメインループの呼び出し #\n",
    "    ################################\n",
    "\n",
    "    # ソフトマックスのスケーリング係数を計算（指数ではなく2のべき乗を使用して高速化）\n",
    "    qk_scale = sm_scale # 0.5\n",
    "    qk_scale *= 1.44269504 # 1/log(2)\n",
    "\n",
    "    # 担当するQブロックをHBMからSRAMに読み込み\n",
    "    # (BLOCK_M, HEAD_DIM)\n",
    "    q = desc_q.load([qo_offset_y, 0])\n",
    "\n",
    "    # STAGEが1もしくは3の場合（0b01 & 0b01 || 0b11 & 0b01）\n",
    "    if STAGE & 1:\n",
    "        # 1の場合、因果マスクなしで全て計算\n",
    "        # 3の場合、アテンション行列の対角成分より以外の部分に対する計算（off-diagonal）\n",
    "        acc, l_i, m_i = _attn_fwd_inner(\n",
    "            acc,\n",
    "            l_i,\n",
    "            m_i,\n",
    "            q,\n",
    "            desc_k,\n",
    "            desc_v,\n",
    "            offset_y,\n",
    "            dtype,\n",
    "            start_m,\n",
    "            qk_scale,\n",
    "            BLOCK_M,\n",
    "            HEAD_DIM,\n",
    "            BLOCK_N,\n",
    "            4 - STAGE, # STAGEが1なら3、STAGEが3なら1を渡す\n",
    "            offs_m,\n",
    "            offs_n,\n",
    "            N_CTX,\n",
    "            warp_specialize,\n",
    "            IS_HOPPER\n",
    "        )\n",
    "\n",
    "    # STAGEが3の場合（0b11 & 0b10）\n",
    "    if STAGE & 2:\n",
    "        # アテンション行列の対角成分に対する計算（on-diagonal）\n",
    "        acc, l_i, m_i = _attn_fwd_inner(\n",
    "            acc,\n",
    "            l_i,\n",
    "            m_i,\n",
    "            q,\n",
    "            desc_k,\n",
    "            desc_v,\n",
    "            offset_y,\n",
    "            dtype,\n",
    "            start_m,\n",
    "            qk_scale,\n",
    "            BLOCK_M,\n",
    "            HEAD_DIM,\n",
    "            BLOCK_N,\n",
    "            2, # STAGEに2を渡す\n",
    "            offs_m,\n",
    "            offs_n,\n",
    "            N_CTX,\n",
    "            warp_specialize,\n",
    "            IS_HOPPER\n",
    "        )\n",
    "\n",
    "    ###################\n",
    "    # 最終処理と書き出し #\n",
    "    ###################\n",
    "\n",
    "    # 生のアテンションスコアの要素の最大値m_iを、数値的な安定性のためにlog-sum-expに変換\n",
    "    # log-sum-exp = m_i + log(l_i)\n",
    "    # (BLOCK_M,)\n",
    "    m_i += tl.math.log2(l_i)\n",
    "\n",
    "    # アテンションを計算\n",
    "    # O_i = ソフトマックスの分子 / ソフトマックスの分母\n",
    "    acc = acc / l_i[:, None]\n",
    "\n",
    "    # m_iをHBMに書き出すポインタを計算\n",
    "    # Mのポインタ + ヘッドとバッチのオフセット * シーケンス長 + Qブロックのオフセット\n",
    "    # (BLOCK_M,)\n",
    "    m_ptrs = M + off_hz * N_CTX + offs_m\n",
    "\n",
    "    # m_iをHBMに書き出し\n",
    "    tl.store(m_ptrs, m_i)\n",
    "\n",
    "    # アテンションをHBMに書き出し\n",
    "    # (BLOCK_M, HEAD_DIM)\n",
    "    desc_o.store([qo_offset_y, 0], acc.to(dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9be3d2",
   "metadata": {},
   "source": [
    "### 逆伝播カーネル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b0465",
   "metadata": {},
   "source": [
    "Algorithm 4を実装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9cebd8",
   "metadata": {},
   "source": [
    "#### _attn_bwd_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e890b5",
   "metadata": {},
   "source": [
    "ソフトマックスの勾配計算に必要なDelta（$D_i$）を求めるワーカーカーネル\n",
    "\n",
    "$$\n",
    "D_i = \\text{rowsum}(dO_i O_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd53cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_bwd_preprocess(\n",
    "    O, # 順伝播の出力テンソル\n",
    "    DO, # Oの勾配テンソル\n",
    "    Delta, # ソフトマックスの勾配計算に必要な中間テンソルへのポインタ\n",
    "    Z, # バッチサイズ\n",
    "    H, # ヘッド数\n",
    "    N_CTX, # シーケンス長（コンテキスト長）\n",
    "    BLOCK_M: tl.constexpr, # このカーネルが処理する行数\n",
    "    HEAD_DIM: tl.constexpr # アテンションヘッドの次元数\n",
    "):\n",
    "    #################\n",
    "    # ポインタの初期化 #\n",
    "    #################\n",
    "\n",
    "    # 現在のプログラムIDの0次元目を取得し、行インデックスのオフセットを計算\n",
    "    # (BLOCK_M,)\n",
    "    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "\n",
    "    # 現在のプログラムIDの1次元目を取得し、ヘッドとバッチの複合インデックスとする\n",
    "    off_hz = tl.program_id(1)\n",
    "\n",
    "    # 列方向のインデックスを作成\n",
    "    # (HEAD_DIM,)\n",
    "    off_n = tl.arange(0, HEAD_DIM)\n",
    "\n",
    "    ################ \n",
    "    # データのロード #\n",
    "    ################ \n",
    "\n",
    "    # 順伝播の結果の一部をロード\n",
    "    # ベースオフセット + バッチとヘッドのオフセット + 行オフセット + 列オフセット\n",
    "    # (BLOCK_M, HEAD_DIM)\n",
    "    o = tl.load(\n",
    "        O + \\\n",
    "        off_hz * HEAD_DIM * N_CTX + \\\n",
    "        off_m[:, None] * HEAD_DIM + \\\n",
    "        off_n[None, :]\n",
    "    )\n",
    "\n",
    "    # 順伝播の結果の勾配の一部をロード\n",
    "    # ベースオフセット + バッチとヘッドのオフセット + 行オフセット + 列オフセット\n",
    "    # (BLOCK_M, HEAD_DIM)\n",
    "    do = tl.load(\n",
    "        DO + \\\n",
    "        off_hz * HEAD_DIM * N_CTX + \\\n",
    "        off_m[:, None] * HEAD_DIM + \\\n",
    "        off_n[None, :] \\\n",
    "    ).to(tl.float32)\n",
    "\n",
    "    ##############\n",
    "    # Deltaの計算 #\n",
    "    ##############\n",
    "\n",
    "    # oとdoを要素ごとに乗算し、列方向を潰して合計を計算\n",
    "    # (BLOCK_M, HEAD_DIM) * (BLOCK_M, HEAD_DIM) -> (BLOCK_M,)\n",
    "    delta = tl.sum(o * do, axis=1)\n",
    "\n",
    "    # HBM上のDeltaに書き出し\n",
    "    # ベースオフセット + バッチとヘッドのオフセット + 行オフセット\n",
    "    tl.store(Delta + off_hz * N_CTX + off_m, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71580a45",
   "metadata": {},
   "source": [
    "#### _attn_bwd_dkdv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d836a",
   "metadata": {},
   "source": [
    "Qブロックに対するキーブロックの勾配 $dK$ ・バリューブロックの勾配 $dV$ を計算するワーカー関数\n",
    "\n",
    "**SRAMにロード済みの$K$ブロックと$V$ブロックを固定し**、$Q$ブロックと$dO$ブロックを順に処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788effb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_bwd_dkdv(\n",
    "    dk, # Kの勾配を累積する変数\n",
    "    dv, # Vの勾配を累積する変数\n",
    "    Q, # 順伝播で使用したクエリテンソルへのポインタ\n",
    "    k, # SRAMにロード済みのKブロック\n",
    "    v, # SRAMにロード済みのVブロック\n",
    "    sm_scale, # 順伝播で使用したスケーリング係数（1/log(2)）\n",
    "    DO, # 出力Oの勾配テンソルへのポインタ\n",
    "    M, # 順伝播の最後に保存したMテンソルへのポインタ\n",
    "    D, # 事前計算されたDeltaテンソルへのポインタ\n",
    "    stride_tok, # 次のトークン（次の行）に進むためのストライド（Q・K・V・DO共通）\n",
    "    stride_d, # 次の次元（次の列）に進むためのストライド（Q・K・V・DO共通）\n",
    "    H, # ヘッド数\n",
    "    N_CTX, # シーケンス長（コンテキスト長）\n",
    "    BLOCK_M1: tl.constexpr, # このカーネルで使用するQブロックの行数\n",
    "    BLOCK_N1: tl.constexpr, # このカーネルで使用するKブロックもしくはVブロックの列数\n",
    "    HEAD_DIM: tl.constexpr, # ヘッドの次元数\n",
    "    start_n, # 担当するKブロックの開始列インデックス\n",
    "    start_m, # 担当するQブロックの開始行インデックス\n",
    "    num_steps, # forループの反復回数\n",
    "    MASK: tl.constexpr # 因果マスキングを適用するかどうかのフラグ\n",
    "):\n",
    "    #################\n",
    "    # ポインタを初期化 #\n",
    "    #################\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するQブロックの行インデックス範囲を計算\n",
    "    # (BLOCK_M1,)\n",
    "    offs_m = start_m + tl.arange(0, BLOCK_M1)\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するKブロックの列インデックス範囲を計算\n",
    "    # (BLOCK_N1,)\n",
    "    offs_n = start_n + tl.arange(0, BLOCK_N1)\n",
    "\n",
    "    # ヘッダの次元インデックスの配列を作成\n",
    "    # (HEAD_DIM,)\n",
    "    offs_k = tl.arange(0, HEAD_DIM)\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するQブロックのポインタを計算\n",
    "    # ベースポインタ + 行オフセット * トークンストライド + 列オフセット * 次元ストライド\n",
    "    # (BLOCK_M1, HEAD_DIM)\n",
    "    qT_ptrs = Q + \\\n",
    "        offs_m[None, :] * stride_tok + \\\n",
    "        offs_k[:, None] * stride_d\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するDOブロックのポインタを計算\n",
    "    # ベースポインタ + 行オフセット * トークンストライド + 列オフセット * 次元ストライド\n",
    "    # (BLOCK_M1, HEAD_DIM)\n",
    "    do_ptrs = DO + \\\n",
    "        offs_m[:, None] * stride_tok + \\\n",
    "        offs_k[None, :] * stride_d\n",
    "\n",
    "    # BLOCK_N1はBLOCK_M1の倍数であることを検証\n",
    "    tl.static_assert(BLOCK_N1 % BLOCK_M1 == 0)\n",
    "\n",
    "    #########################\n",
    "    # QとDOのブロックを順に処理 #\n",
    "    #########################\n",
    "\n",
    "    curr_m = start_m\n",
    "    step_m = BLOCK_M1\n",
    "    for blk_idx in range(num_steps):\n",
    "\n",
    "        ##########################\n",
    "        # アテンションスコアの再計算 #\n",
    "        ##########################\n",
    "\n",
    "        # QブロックをSRAMにロード\n",
    "        # (BLOCK_M1, HEAD_DIM)\n",
    "        qT = tl.load(qT_ptrs)\n",
    "\n",
    "        # 生のアテンンションスコアの要素の最大値m_iへのポインタを計算\n",
    "        # (BLOCK_M1,)\n",
    "        offs_m = curr_m + tl.arange(0, BLOCK_M1)\n",
    "\n",
    "        # m_iをSRAMにロード\n",
    "        # (BLOCK_M1,)\n",
    "        m = tl.load(M + offs_m)\n",
    "\n",
    "        # 生のアテンションスコアを再計算 S_ij = Q_i K_j^T\n",
    "        # (BLOCK_M1, BLOCK_N1) = (BLOCK_M1, HEAD_DIM) @ (HEAD_DIM, BLOCK_N1)\n",
    "        qkT = tl.dot(k, qT)\n",
    "\n",
    "        # アテンションスコアPを再計算 P_ij = exp(S_ij - m_i)\n",
    "        # (BLOCK_M1, BLOCK_N1)\n",
    "        pT = tl.math.exp2(qkT - m[None, :])\n",
    "\n",
    "        ############\n",
    "        # 勾配の計算 #\n",
    "        ############\n",
    "\n",
    "        # アテンションスコアにマスクを適用\n",
    "        if MASK:\n",
    "            mask = (offs_m[None, :] >= offs_n[:, None])\n",
    "            pT = tl.where(mask, pT, 0.0)\n",
    "\n",
    "        # DOブロックをSRAMにロード\n",
    "        # (BLOCK_M1, HEAD_DIM)\n",
    "        do = tl.load(do_ptrs)\n",
    "\n",
    "        # pTをコピーしてfp16にキャスト\n",
    "        # (BLOCK_M1, BLOCK_N1)\n",
    "        ppT = pT\n",
    "        ppT = ppT.to(tl.float16)\n",
    "\n",
    "        # dVを累積\n",
    "        # \\tilde{dV_j} = \\tilde{dV_j} + P_{ij}^T dO_i\n",
    "        # (BLOCK_N1, BLOCK_M1) @ (BLOCK_M1, HEAD_DIM) = (BLOCK_N1, HEAD_DIM)\n",
    "        dv += tl.dot(ppT, do)\n",
    "\n",
    "        # デルタテンソル D_i をSRAMにロード\n",
    "        # (BLOCK_N1,)\n",
    "        Di = tl.load(D + offs_m)\n",
    "\n",
    "        # dPを計算\n",
    "        # dP_{ij} = dO @ V^T\n",
    "        dpT = tl.dot(v, tl.trans(do)).to(tl.float32)\n",
    "\n",
    "        # dSを計算\n",
    "        # dS_{ij} = P_{ij} (dP_{ij} - D_i)\n",
    "        dsT = pT * (dpT - Di[None, :])\n",
    "        dsT = dsT.to(tl.float16)\n",
    "\n",
    "        # dKを計算し、アキュームレータに加算\n",
    "        # \\tilde{dK_j} = \\tilde{dK_j} + dS_{ij}^T Q_i\n",
    "        # (HEAD_DIM, BLOCK_N1) @ (BLOCK_N1, BLOCK_M1) = (HEAD_DIM, BLOCK_M1)\n",
    "        dk += tl.dot(dsT, tl.trans(qT))\n",
    "\n",
    "        ################\n",
    "        # ポインタの更新 #\n",
    "        ################\n",
    "\n",
    "        curr_m += step_m\n",
    "        qT_ptrs += step_m * stride_tok\n",
    "        do_ptrs += step_m * stride_tok\n",
    "\n",
    "    return dk, dv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7715f690",
   "metadata": {},
   "source": [
    "#### _attn_bwd_dq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da5aef",
   "metadata": {},
   "source": [
    "`_attn_bwd_dq`は、$dQ$（クエリの勾配）を計算するワーカー関数\n",
    "\n",
    "**SRAMにロード済みの一つのQブロックを固定し**、KブロックとVブロックをループして計算した勾配をアキュームレータに加算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e646f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_bwd_dq(\n",
    "    dq, # Qの勾配のアキュームレータ\n",
    "    q, # SRAMにロード済みのQブロック\n",
    "    K, # Kテンソル全体へのポインタ\n",
    "    V, # Vテンソル全体へのポインタ\n",
    "    do, # qに対応する出力Oの勾配ブロック\n",
    "    m, # qに対応する生のアテンションスコアの要素の最大値mのブロック\n",
    "    D, # 事前計算済みのDeltaブロックへのポインタ\n",
    "    stride_tok, # 次のトークン（次の行）に進むためのストライド（Q・K・V・DO共通）\n",
    "    stride_d, # 次の次元（次の列）に進むためのストライド（Q・K・V・DO共通）\n",
    "    H, # ヘッド数\n",
    "    N_CTX, # シーケンス長（コンテキスト長）\n",
    "    BLOCK_M2: tl.constexpr, # このカーネルで使用するブロックサイズ\n",
    "    BLOCK_N2: tl.constexpr, # このカーネルで使用するブロックサイズ\n",
    "    HEAD_DIM: tl.constexpr, # ヘッドの次元数\n",
    "    start_m, # 担当するQブロックの開始行インデックス\n",
    "    start_n, # 担当するKブロックの開始列インデックス\n",
    "    num_steps, # forループの反復回数\n",
    "    MASK: tl.constexpr # 因果マスキングを適用するかどうかのフラグ\n",
    "):\n",
    "    #################\n",
    "    # ポインタの初期化\n",
    "    #################\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するQブロックの行インデックス範囲を計算\n",
    "    # (BLOCK_M2,)\n",
    "    offs_m = start_m + tl.arange(0, BLOCK_M2)\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するKブロックの列インデックス範囲を計算\n",
    "    # (BLOCK_N2,)\n",
    "    offs_n = start_n + tl.arange(0, BLOCK_N2)\n",
    "\n",
    "    # ヘッダの次元インデックスの配列を作成\n",
    "    # (HEAD_DIM,)\n",
    "    offs_k = tl.arange(0, HEAD_DIM)\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するKブロックのポインタを計算\n",
    "    # ベースポインタ + 行オフセット * トークンストライド + 列オフセット * 次元ストライド\n",
    "    # (HEAD_DIM, BLOCK_N2)\n",
    "    kT_ptrs = K + \\\n",
    "        offs_n[None, :] * stride_tok + \\\n",
    "        offs_k[:, None] * stride_d\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するVブロックのポインタを計算\n",
    "    # ベースポインタ + 行オフセット * トークンストライド + 列オフセット * 次元ストライド\n",
    "    # (HEAD_DIM, BLOCK_N2)\n",
    "    vT_ptrs = V + \\\n",
    "        offs_n[None, :] * stride_tok + \\\n",
    "        offs_k[:, None] * stride_d\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するDブロックをSRAMにロード\n",
    "    # 注意: D（Delta）は事前にds_scaleで割られている\n",
    "    # (BLOCK_M2,)\n",
    "    Di = tl.load(D + offs_m)\n",
    "\n",
    "    # BLOCK_M2はBLOCK_N2の倍数であることを検証\n",
    "    tl.static_assert(BLOCK_M2 % BLOCK_N2 == 0)\n",
    "\n",
    "    ########################\n",
    "    # KとVのブロックを順に処理 #\n",
    "    ########################\n",
    "\n",
    "    curr_n = start_n\n",
    "    step_n = BLOCK_N2\n",
    "    for blk_idx in range(num_steps):\n",
    "\n",
    "        # KブロックをSRAMにロード\n",
    "        # (HEAD_DIM, BLOCK_N2)\n",
    "        kT = tl.load(kT_ptrs)\n",
    "\n",
    "        # VブロックをSRAMにロード\n",
    "        # (HEAD_DIM, BLOCK_N2)\n",
    "        vT = tl.load(vT_ptrs)\n",
    "\n",
    "        # 生のアテンションスコアを再計算 S_ij = Q_i K_j^T\n",
    "        # (BLOCK_M2, HEAD_DIM) @ (HEAD_DIM, BLOCK_N2) -> (BLOCK_M2, BLOCK_N2)\n",
    "        qk = tl.dot(q, kT)\n",
    "\n",
    "        # アテンションスコアPを再計算 P_ij = exp(S_ij - m_i)\n",
    "        # (BLOCK_M2, BLOCK_N2)\n",
    "        p = tl.math.exp2(qk - m)\n",
    "\n",
    "        # 因果マスクを使用する場合\n",
    "        if MASK:\n",
    "            # マスクを作成\n",
    "            offs_n = curr_n + tl.arange(0, BLOCK_N2)\n",
    "            mask = (offs_m[:, None] >= offs_n[None, :])\n",
    "\n",
    "            # マスクを適用\n",
    "            p = tl.where(mask, p, 0.0)\n",
    "\n",
    "        # アテンションスコアの勾配dPを計算\n",
    "        # dP_{ij} = dO_{i} V_j^T\n",
    "        dp = tl.dot(do, vT).to(tl.float32)\n",
    "\n",
    "        # アテンションスコアの勾配dSを計算\n",
    "        # dS = P * (dP - D_i)\n",
    "        ds = p * (dp - Di[:, None])\n",
    "        ds = ds.to(tl.float16)\n",
    "\n",
    "        # Qの勾配dQを計算し、アキュームレータに加算\n",
    "        # dQ_i = dQ_i + dS_{ij} K_j\n",
    "        # 注意: kTは事前にqk_scaleでスケーリングされているため戻す必要がある\n",
    "        dq += tl.dot(ds, tl.trans(kT))\n",
    "\n",
    "        ################\n",
    "        # ポインタの更新 #\n",
    "        ################\n",
    "\n",
    "        # Kブロックの列オフセットを更新\n",
    "        curr_n += step_n\n",
    "\n",
    "        # Kブロックのポインタを更新\n",
    "        kT_ptrs += step_n * stride_tok\n",
    "\n",
    "        # Vブロックのポインタを更新\n",
    "        vT_ptrs += step_n * stride_tok\n",
    "\n",
    "    return dq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417236c5",
   "metadata": {},
   "source": [
    "#### _attn_bwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ea1ce",
   "metadata": {},
   "source": [
    "逆伝搬全体を制御するマネージャー\n",
    "\n",
    "担当範囲を決定し、必要なデータを準備し、ワーカー関数を呼び出し、$dQ$ ・ $dK$ ・ $dV$ をHBMに書き戻す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dec1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_bwd(\n",
    "    Q, # 順伝播で使用したクエリテンソルへのポインタ\n",
    "    K, # 順伝播で使用したキーテンソルへのポインタ\n",
    "    V, # 順伝播で使用したバリューテンソルへのポインタ\n",
    "    sm_scale, # 順伝播で使用したスケーリング係数（1/log(2)）\n",
    "    DO, # 出力Oの勾配テンソル全体へのポインタ\n",
    "    DQ, # 計算結果を書き込むQの勾配テンソル全体へのポインタ\n",
    "    DK, # 計算結果を書き込むKの勾配テンソル全体へのポインタ\n",
    "    DV, # 計算結果を書き込むVの勾配テンソル全体へのポインタ\n",
    "    M, # 順伝播で保存した生のアテンションスコアの要素の最大値mへのポインタ\n",
    "    D, # 事前計算されたDeltaテンソル\n",
    "    stride_z, # バッチ方向に進むためのストライド（Q・K・V・DO共通）\n",
    "    stride_h, # ヘッド方向に進むためのストライド（Q・K・V・DO共通）\n",
    "    stride_tok, # 次のトークン（次の行）に進むためのストライド（Q・K・V・DO共通）\n",
    "    stride_d, # 次の次元（次の列）に進むためのストライド（Q・K・V・DO共通）\n",
    "    H, # ヘッド数\n",
    "    N_CTX, # シーケンス長（コンテキスト長）\n",
    "    BLOCK_M1: tl.constexpr, # dKとdVの計算に使用するブロックサイズ\n",
    "    BLOCK_N1: tl.constexpr, # dKとdVの計算に使用するブロックサイズ\n",
    "    BLOCK_M2: tl.constexpr,  # dQの計算に使用するブロックサイズ\n",
    "    BLOCK_N2: tl.constexpr,  # dQの計算に使用するブロックサイズ\n",
    "    BLK_SLICE_FACTOR: tl.constexpr, # ブロックを更に細かくスライスする際の分割数\n",
    "    HEAD_DIM: tl.constexpr # アテンションヘッドの次元数\n",
    "):\n",
    "\n",
    "    #########\n",
    "    # 初期化 #\n",
    "    #########\n",
    "\n",
    "    # ln(2)を定数として定義\n",
    "    LN2: tl.constexpr = 0.6931471824645996\n",
    "\n",
    "    # 現在のプログラムインスタンスIDの2次元目を取得し、バッチとヘッドの複合インデックスとする\n",
    "    bhid = tl.program_id(2)\n",
    "\n",
    "    # バッチとヘッドのオフセットを計算\n",
    "    off_chz = (bhid * N_CTX).to(tl.int64)\n",
    "\n",
    "    # オフセットを計算し、INT64にキャスト\n",
    "    # ヘッダのオフセット + バッチのオフセット\n",
    "    adj = (stride_h * (bhid % H) + stride_z * (bhid // H)).to(tl.int64)\n",
    "\n",
    "    # 現在のプログラムインスタンスIDの0次元目を取得し、行もしくは列のブロックインデックスとする\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    # 現在のプログラムインスタンスが担当するバッチとヘッドのオフセットをポインタに反映\n",
    "    Q += adj\n",
    "    K += adj\n",
    "    V += adj\n",
    "    DO += adj\n",
    "    DQ += adj\n",
    "    DK += adj\n",
    "    DV += adj\n",
    "    M += off_chz\n",
    "    D += off_chz\n",
    "\n",
    "    # ヘッドの次元インデックスの配列を作成\n",
    "    offs_k = tl.arange(0, HEAD_DIM)\n",
    "\n",
    "    ##############\n",
    "    # dKとdVを計算 #\n",
    "    ##############\n",
    "\n",
    "    # 現在のプログラムインスタンスが担当するKブロック・Vブロックの開始列インデックスを計算\n",
    "    start_n = pid * BLOCK_N1\n",
    "\n",
    "    # 現在のプログラムインスタンスが担当するQブロックの開始行インデックスを計算\n",
    "    start_m = start_n\n",
    "\n",
    "    # 一度に処理するK・Vブロックサイズを更に細かくする（1/2）\n",
    "    MASK_BLOCK_M1: tl.constexpr = BLOCK_M1 // BLK_SLICE_FACTOR\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するKブロックの列オフセットを計算\n",
    "    # (BLOCK_N1,)\n",
    "    offs_n = start_n + tl.arange(0, BLOCK_N1)\n",
    "\n",
    "    # dVをゼロで初期化\n",
    "    # (BLOCK_N1, HEAD_DIM)\n",
    "    dv = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    # dKをゼロで初期化\n",
    "    # (BLOCK_N1, HEAD_DIM)\n",
    "    dk = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するKブロックをSRAMにロード\n",
    "    # Kポインタ + 行オフセット * トークンストライド + 列オフセット * 次元ストライド\n",
    "    # (BLOCK_N1, HEAD_DIM)\n",
    "    k = tl.load(K + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するVブロックをSRAMにロード\n",
    "    # Vポインタ + 行オフセット * トークンストライド + 列オフセット * 次元ストライド\n",
    "    # (BLOCK_N1, HEAD_DIM)\n",
    "    v = tl.load(V + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "\n",
    "    # 必要な反復回数を計算\n",
    "    num_steps = BLOCK_N1 // MASK_BLOCK_M1\n",
    "\n",
    "    # 因果マスキングが必要な対角ブロックに対してdKとdVを計算（on-diagonal）\n",
    "    dk, dv = _attn_bwd_dkdv(\n",
    "        dk,\n",
    "        dv,\n",
    "        Q,\n",
    "        k,\n",
    "        v,\n",
    "        sm_scale,\n",
    "        DO,\n",
    "        M,\n",
    "        D,\n",
    "        stride_tok,\n",
    "        stride_d,\n",
    "        H,\n",
    "        N_CTX,\n",
    "        MASK_BLOCK_M1,\n",
    "        BLOCK_N1,\n",
    "        HEAD_DIM,\n",
    "        start_n,\n",
    "        start_m,\n",
    "        num_steps,\n",
    "        MASK=True # 因果マスキングを適用\n",
    "    )\n",
    "\n",
    "    # ひとつのブロック分進める（逆伝播のため、オンダイアゴナルからオフダイアゴナルに進める）\n",
    "    start_m += num_steps * MASK_BLOCK_M1\n",
    "    num_steps = (N_CTX - start_m) // BLOCK_M1\n",
    "\n",
    "    # 因果マスキングが不要なブロックに対してdKとdVを計算（off-diagonal）\n",
    "    dk, dv = _attn_bwd_dkdv(\n",
    "        dk,\n",
    "        dv,\n",
    "        Q,\n",
    "        k,\n",
    "        v,\n",
    "        sm_scale,\n",
    "        DO,\n",
    "        M,\n",
    "        D,\n",
    "        stride_tok,\n",
    "        stride_d,\n",
    "        H,\n",
    "        N_CTX,\n",
    "        BLOCK_M1,\n",
    "        BLOCK_N1,\n",
    "        HEAD_DIM,\n",
    "        start_n,\n",
    "        start_m,\n",
    "        num_steps,\n",
    "        MASK=False # 因果マスキングを適用しない\n",
    "    )\n",
    "\n",
    "    # dVの出力先のポインタを計算\n",
    "    # ベースポインタ + 行オフセット * トークンストライド + 列オフセット * 次元ストライド\n",
    "    # (BLOCK_N1, HEAD_DIM)\n",
    "    dv_ptrs = DV + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n",
    "\n",
    "    # dVを書き戻す\n",
    "    tl.store(dv_ptrs, dv)\n",
    "\n",
    "    # dKのスケールを戻す\n",
    "    dk *= sm_scale\n",
    "\n",
    "    # dKの出力先のポインタを計算\n",
    "    # ベースポインタ + 行オフセット * トークンストライド + 列オフセット * 次元ストライド\n",
    "    # (BLOCK_N1, HEAD_DIM)\n",
    "    dk_ptrs = DK + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n",
    "\n",
    "    # dKを書き戻す\n",
    "    tl.store(dk_ptrs, dk)\n",
    "\n",
    "    ###########\n",
    "    # dQを計算 #\n",
    "    ###########\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するQブロックの開始行インデックスを計算\n",
    "    start_m = pid * BLOCK_M2\n",
    "\n",
    "    # 現在のプログラムインスタンスが処理するQブロックの終了行インデックスを計算 \n",
    "    end_n = start_m + BLOCK_M2\n",
    "\n",
    "    # 一度に処理するK・Vブロックサイズを更に細かくする（1/2）\n",
    "    MASK_BLOCK_N2: tl.constexpr = BLOCK_N2 // BLK_SLICE_FACTOR\n",
    "\n",
    "    # Qブロックの行オフセットを計算\n",
    "    # (BLOCK_M2,)\n",
    "    offs_m = start_m + tl.arange(0, BLOCK_M2)\n",
    "\n",
    "    # QブロックをSRAMにロード\n",
    "    # Qポインタ + 行オフセット * トークンストライド + 列オフセット * 次元ストライド\n",
    "    # (BLOCK_M2, HEAD_DIM)\n",
    "    q = tl.load(Q + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "\n",
    "    # dQをゼロで初期化\n",
    "    # (BLOCK_M2, HEAD_DIM)\n",
    "    dq = tl.zeros([BLOCK_M2, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    # dOブロックをSRAMにロード\n",
    "    # DOポインタ + 行オフセット * トークンストライド + 列オフセット * 次元ストライド\n",
    "    # (BLOCK_M2, HEAD_DIM)\n",
    "    do = tl.load(DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "\n",
    "    # log-sum-expテンソルをSRAMにロード\n",
    "    # (BLOCK_M2,) -> (BLOCK_M2, 1)\n",
    "    m = tl.load(M + offs_m)\n",
    "    m = m[:, None]\n",
    "\n",
    "    # 必要な反復回数を計算\n",
    "    num_steps = BLOCK_M2 // MASK_BLOCK_N2\n",
    "\n",
    "    # 因果マスキングが必要な対角ブロックに対してdQを計算（on-diagonal）\n",
    "    dq = _attn_bwd_dq(\n",
    "        dq,\n",
    "        q,\n",
    "        K,\n",
    "        V,\n",
    "        do,\n",
    "        m,\n",
    "        D,\n",
    "        stride_tok,\n",
    "        stride_d,\n",
    "        H,\n",
    "        N_CTX,\n",
    "        BLOCK_M2,\n",
    "        MASK_BLOCK_N2,\n",
    "        HEAD_DIM,\n",
    "        start_m,\n",
    "        end_n - num_steps * MASK_BLOCK_N2,\n",
    "        num_steps,\n",
    "        MASK=True # 因果マスキングを適用\n",
    "    )\n",
    "\n",
    "    # ひとつのブロック分進める（逆方向に進めるのはコードの再利用性とシンプルさのため）\n",
    "    end_n -= num_steps * MASK_BLOCK_N2\n",
    "    num_steps = end_n // BLOCK_N2\n",
    "\n",
    "    # 因果マスキングが不要なブロックに対してdQを計算（off-diagonal）\n",
    "    dq = _attn_bwd_dq(\n",
    "        dq,\n",
    "        q,\n",
    "        K,\n",
    "        V,\n",
    "        do,\n",
    "        m,\n",
    "        D,\n",
    "        stride_tok,\n",
    "        stride_d,\n",
    "        H,\n",
    "        N_CTX,\n",
    "        BLOCK_M2,\n",
    "        BLOCK_N2,\n",
    "        HEAD_DIM,\n",
    "        start_m,\n",
    "        end_n - num_steps * BLOCK_N2,\n",
    "        num_steps,\n",
    "        MASK=False # 因果マスキングを適用しない\n",
    "    )\n",
    "\n",
    "    ###############\n",
    "    # dQの書き戻し #\n",
    "    ###############\n",
    "\n",
    "    # dQの出力先のポインタを計算\n",
    "    # ベースポインタ + 行オフセット * トークンストライド + 列オフセット * 次元ストライド\n",
    "    # (BLOCK_M2, HEAD_DIM)\n",
    "    dq_ptrs = DQ + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n",
    "\n",
    "    # dQをスケーリング（順伝播でexp2を使用したため）\n",
    "    dq *= LN2\n",
    "\n",
    "    # dQを書き戻す\n",
    "    tl.store(dq_ptrs, dq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be74a9",
   "metadata": {},
   "source": [
    "### Attention層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763243ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _attention(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx, # PyTorchで自動的に渡されるコンテキストオブジェクト\n",
    "        q, # クエリテンソル (Z, H, N_CTX, HEAD_DIM)\n",
    "        k, # キーテンソル (Z, H, N_CTX, HEAD_DIM)\n",
    "        v, # バリューテンソル (Z, H, N_CTX, HEAD_DIM)\n",
    "        causal, # 因果マスキングを適用するかどうかのフラグ\n",
    "        sm_scale, # スケーリング係数\n",
    "        warp_specialize=True # Tritonの最適化フラグ\n",
    "    ):\n",
    "        logger.info(f\"順伝搬開始 {q.shape=}{q.dtype=}, {k.shape=}, {k.dtype=}, {v.shape=}, {v.dtype=}, {causal=}, {sm_scale=}, {warp_specialize=}\")\n",
    "\n",
    "        #########\n",
    "        # 初期化 #\n",
    "        #########\n",
    "\n",
    "        # Qのヘッド次元を取得 64\n",
    "        HEAD_DIM_Q = q.shape[-1]\n",
    "        logger.debug(f\"{HEAD_DIM_Q=}\")\n",
    "\n",
    "        # Kのヘッド次元を取得 64\n",
    "        HEAD_DIM_K = k.shape[-1]\n",
    "        logger.debug(f\"{HEAD_DIM_K=}\")\n",
    "\n",
    "        # Vのヘッド次元を取得 64\n",
    "        HEAD_DIM_V = v.shape[-1]\n",
    "        logger.debug(f\"{HEAD_DIM_V=}\")\n",
    "\n",
    "        # Q・K・Vのヘッド次元が同じであることを検証\n",
    "        assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n",
    "\n",
    "        # Kのヘッド次元がサポートされている値であることを検証\n",
    "        assert HEAD_DIM_K in {16, 32, 64, 128, 256}\n",
    "\n",
    "        # 出力テンソルOを初期化 (1, 2, 128, 64)\n",
    "        # (Z, H, N_CTX, HEAD_DIM)\n",
    "        o = torch.empty_like(q)\n",
    "        logger.debug(f\"{o.shape=}, {o.dtype=}\")\n",
    "\n",
    "        # ステージの設定 3\n",
    "        # 因果マスクが有効な場合はステージ3、そうでなければステージ1\n",
    "        stage = 3 if causal else 1\n",
    "        logger.debug(f\"{stage=}\")\n",
    "\n",
    "        extra_kern_args = {}\n",
    "\n",
    "        # AMDの場合 False\n",
    "        if is_hip():\n",
    "            waves_per_eu = 3 if HEAD_DIM_K <= 64 else 2\n",
    "            extra_kern_args = {\"waves_per_eu\": waves_per_eu, \"allow_flush_denorm\": True}\n",
    "        logger.debug(f\"{extra_kern_args=}\")\n",
    "\n",
    "        # 生のアテンションスコアの要素の最大値mを格納するテンソルMを初期化 (1, 2, 128)\n",
    "        # (Z, H, N_CTX)\n",
    "        M = torch.empty(\n",
    "            (q.shape[0], q.shape[1], q.shape[2]),\n",
    "            device=q.device,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        logger.debug(f\"{M.shape=}\")\n",
    "\n",
    "        ####################\n",
    "        # ディスクリプタの準備 #\n",
    "        ####################\n",
    "\n",
    "        # 新しいGPUの場合 True\n",
    "        if supports_host_descriptor() and not (is_hopper() and warp_specialize):\n",
    "            # バッチサイズ、ヘッド数、コンテキスト長を掛け合わせてフラット化\n",
    "            # 1 * 2 * 128 = 256\n",
    "            y_dim = q.shape[0] * q.shape[1] * q.shape[2]\n",
    "            logger.debug(f\"{y_dim=}\")\n",
    "\n",
    "            dummy_block = [1, 1]\n",
    "\n",
    "            # Qのテンソルディスクリプタを作成\n",
    "            desc_q = TensorDescriptor(\n",
    "                q,\n",
    "                shape=[y_dim, HEAD_DIM_K], # (Z * H * N_CTX, HEAD_DIM)\n",
    "                strides=[HEAD_DIM_K, 1], # (HEAD_DIM, 1)\n",
    "                block_shape=dummy_block\n",
    "            )\n",
    "            logger.debug(f\"Qのディスクリプタを作成\")\n",
    "\n",
    "            # Vのテンソルディスクリプタを作成\n",
    "            if q.dtype == torch.float8_e5m2:\n",
    "                # Qのデータ型がFP8の場合は、メモリレイアウトが異なるので注意\n",
    "                desc_v = TensorDescriptor(\n",
    "                    v,\n",
    "                    shape=[HEAD_DIM_K, y_dim], # (HEAD_DIM, Z * H * N_CTX)\n",
    "                    strides=[q.shape[2], 1], # (N_CTX, 1)\n",
    "                    block_shape=dummy_block\n",
    "                )\n",
    "                logger.debug(f\"FP8用のVのディスクリプタを作成\")\n",
    "            else:\n",
    "                desc_v = TensorDescriptor(\n",
    "                    v,\n",
    "                    shape=[y_dim, HEAD_DIM_K], # (Z * H * N_CTX, HEAD_DIM)\n",
    "                    strides=[HEAD_DIM_K, 1], # (HEAD_DIM, 1)\n",
    "                    block_shape=dummy_block\n",
    "                )\n",
    "                logger.debug(f\"Vのディスクリプタを作成\")\n",
    "\n",
    "            # Kのテンソルディスクリプタを作成\n",
    "            desc_k = TensorDescriptor(\n",
    "                k,\n",
    "                shape=[y_dim, HEAD_DIM_K], # (Z * H * N_CTX, HEAD_DIM)\n",
    "                strides=[HEAD_DIM_K, 1], # (HEAD_DIM, 1)\n",
    "                block_shape=dummy_block\n",
    "            )\n",
    "            logger.debug(f\"Kのディスクリプタを作成\")\n",
    "\n",
    "            # Oのテンソルディスクリプタを作成\n",
    "            desc_o = TensorDescriptor(\n",
    "                o,\n",
    "                shape=[y_dim, HEAD_DIM_K], # (Z * H * N_CTX, HEAD_DIM)\n",
    "                strides=[HEAD_DIM_K, 1], # (HEAD_DIM, 1)\n",
    "                block_shape=dummy_block\n",
    "            )\n",
    "            logger.debug(f\"Oのディスクリプタを作成\")\n",
    "        else:\n",
    "            desc_q = q\n",
    "            desc_v = v\n",
    "            desc_k = k\n",
    "            desc_o = o\n",
    "            logger.debug(f\"ポインタを準備 {desc_q.shape=}, {desc_k.shape=}, {desc_v.shape=}, {desc_o.shape=}\")\n",
    "\n",
    "        ################\n",
    "        # カーネルの起動 #\n",
    "        ################\n",
    "\n",
    "        def alloc_fn(size: int, align: int, _):\n",
    "            return torch.empty(size, dtype=torch.int8, device=\"cuda\")\n",
    "\n",
    "        triton.set_allocator(alloc_fn)\n",
    "\n",
    "        # 起動グリッドを定義\n",
    "        # Qブロックのヘッドごとにプログラムインスタンスを割り当てる\n",
    "        def grid(META):\n",
    "            return (\n",
    "                triton.cdiv(q.shape[2], META[\"BLOCK_M\"]), # N_CTX / BLOCK_M = Qブロックの総数\n",
    "                q.shape[0] * q.shape[1], # バッチサイズ * ヘッド数 = ヘッドの総数\n",
    "                1 \n",
    "            )\n",
    "        logger.debug(f\"起動グリッドを準備\")\n",
    "\n",
    "        ctx.grid = grid\n",
    "\n",
    "        if is_blackwell() and warp_specialize:\n",
    "            if HEAD_DIM_K == 128 and q.dtype == torch.float16:\n",
    "                extra_kern_args[\"maxnreg\"] = 168\n",
    "            else:\n",
    "                extra_kern_args[\"maxnreg\"] = 80\n",
    "\n",
    "        logger.debug(f\"_attn_fwdカーネルの実行\")\n",
    "        _attn_fwd[grid](\n",
    "            sm_scale,\n",
    "            M,\n",
    "            q.shape[0],\n",
    "            q.shape[1],\n",
    "            desc_q,\n",
    "            desc_k,\n",
    "            desc_v,\n",
    "            desc_o,\n",
    "            N_CTX=q.shape[2],\n",
    "            HEAD_DIM=HEAD_DIM_K,\n",
    "            FP8_OUTPUT=q.dtype==torch.float8_e5m2,\n",
    "            STAGE=stage,\n",
    "            warp_specialize=warp_specialize,\n",
    "            IS_HOPPER=is_hopper(),\n",
    "            **extra_kern_args\n",
    "        )\n",
    "\n",
    "        #########\n",
    "        # 後処理 #\n",
    "        #########\n",
    "\n",
    "        # 逆伝搬で使用する値を保存\n",
    "        ctx.save_for_backward(q, k, v, o, M)\n",
    "        ctx.sm_scale = sm_scale\n",
    "        ctx.HEAD_DIM = HEAD_DIM_K\n",
    "        ctx.causal = causal\n",
    "\n",
    "        logger.info(f\"順伝搬終了 {o.shape=}, {o.dtype=}\")\n",
    "        return o \n",
    "\n",
    "    @staticmethod\n",
    "    def backward(\n",
    "        ctx, # forwardで保存した値を保持するコンテキストオブジェクト\n",
    "        do # 出力Oの勾配テンソル\n",
    "    ):\n",
    "        logger.info(f\"逆伝搬開始 {do.shape=}, {do.dtype=}\")\n",
    "\n",
    "        #########\n",
    "        # 初期化 #\n",
    "        #########\n",
    "\n",
    "        # 保存したテンソルを取得\n",
    "        q, k, v, o, M = ctx.saved_tensors\n",
    "\n",
    "        logger.debug(f\"{q.shape=}, {k.shape=}, {v.shape=}, {o.shape=}, {M.shape=}\")\n",
    "\n",
    "        # dOのメモリが連続していることを検証\n",
    "        assert do.is_contiguous()\n",
    "\n",
    "        # 全てのテンソルのストライドが同じであることを検証\n",
    "        assert q.stride() == k.stride() == v.stride() == o.stride() == do.stride()\n",
    "\n",
    "        # 出力先の勾配テンソルを初期化\n",
    "        dq = torch.empty_like(q)\n",
    "        dk = torch.empty_like(k)\n",
    "        dv = torch.empty_like(v)\n",
    "\n",
    "        # 1, 2, 128\n",
    "        BATCH, N_HEAD, N_CTX = q.shape[:3]\n",
    "        logger.debug(f\"{BATCH=}, {N_HEAD=}, {N_CTX=}\")\n",
    "\n",
    "        PRE_BLOCK = 128\n",
    "        logger.debug(f\"{PRE_BLOCK=}\")\n",
    "\n",
    "        NUM_WARPS, NUM_STAGES = 4, 5\n",
    "        logger.debug(f\"{NUM_WARPS=}, {NUM_STAGES=}\")\n",
    "\n",
    "        BLOCK_M1, BLOCK_N1, BLOCK_M2, BLOCK_N2 = 32, 128, 128, 32\n",
    "        logger.debug(f\"{BLOCK_M1=}, {BLOCK_N1=}, {BLOCK_M2=}, {BLOCK_N2=}\")\n",
    "\n",
    "        BLK_SLICE_FACTOR = 2\n",
    "        logger.debug(f\"{BLK_SLICE_FACTOR=}\")\n",
    "\n",
    "        RCP_LN2 = 1.4426950408889634  # = 1.0 / ln(2)\n",
    "        logger.debug(f\"{RCP_LN2=}\")\n",
    "\n",
    "        # Kを事前スケーリング\n",
    "        # アテンションウェイトの再計算でexp2を使用するため\n",
    "        arg_k = k\n",
    "        arg_k = arg_k * (ctx.sm_scale * RCP_LN2)\n",
    "        logger.debug(f\"{arg_k.shape=}\")\n",
    "\n",
    "        ###################################\n",
    "        # _attn_bwd_preprocessカーネルの実行 #\n",
    "        ###################################\n",
    "\n",
    "        PRE_BLOCK = 128\n",
    "        logger.debug(f\"{PRE_BLOCK=}\")\n",
    "        \n",
    "        # N_CTXがPRE_BLOCKの倍数であることを検証\n",
    "        assert N_CTX % PRE_BLOCK == 0\n",
    "\n",
    "        # 2次元の起動グリッドを定義 (1, 2)\n",
    "        pre_grid = (\n",
    "            N_CTX // PRE_BLOCK, # N_CTX / PRE_BLOCK = Qブロックの総数\n",
    "            BATCH * N_HEAD # バッチサイズ * ヘッド数 = ヘッドの総数\n",
    "        )\n",
    "        logger.debug(f\"{pre_grid=}\")\n",
    "\n",
    "        # preprocessカーネルの計算結果を格納するテンソルを初期化 (1, 2, 128)\n",
    "        # (Z, H, N_CTX)\n",
    "        delta = torch.empty_like(M)\n",
    "        logger.debug(f\"{delta.shape=}\")\n",
    "\n",
    "        logger.debug(f\"_attn_bwd_preprocessカーネルの実行（Deltaを計算）\")\n",
    "        _attn_bwd_preprocess[pre_grid](\n",
    "            o,\n",
    "            do,\n",
    "            delta,\n",
    "            BATCH,\n",
    "            N_HEAD,\n",
    "            N_CTX,\n",
    "            BLOCK_M=PRE_BLOCK,\n",
    "            HEAD_DIM=ctx.HEAD_DIM\n",
    "        )\n",
    "\n",
    "        #########################\n",
    "        # _attn_bwdカーネルの実行 #\n",
    "        #########################\n",
    "\n",
    "        # 3次元の起動グリッドを定義 (1, 1, 2)\n",
    "        grid = (\n",
    "            N_CTX // BLOCK_N1, # N_CTX / BLOCK_N1 = Kブロックの総数\n",
    "            1,\n",
    "            BATCH * N_HEAD # バッチサイズ * ヘッド数 = ヘッドの総数\n",
    "        )\n",
    "        logger.debug(f\"{grid=}\")\n",
    "\n",
    "        logger.debug(f\"_attn_bwdカーネルの実行（dQ・dK・dVを計算）\")\n",
    "        _attn_bwd[grid](\n",
    "            q,\n",
    "            arg_k,\n",
    "            v,\n",
    "            ctx.sm_scale,\n",
    "            do,\n",
    "            dq,\n",
    "            dk,\n",
    "            dv,\n",
    "            M,\n",
    "            delta,\n",
    "            q.stride(0),\n",
    "            q.stride(1),\n",
    "            q.stride(2),\n",
    "            q.stride(3),\n",
    "            N_HEAD,\n",
    "            N_CTX,\n",
    "            BLOCK_M1=BLOCK_M1,\n",
    "            BLOCK_N1=BLOCK_N1,\n",
    "            BLOCK_M2=BLOCK_M2,\n",
    "            BLOCK_N2=BLOCK_N2,\n",
    "            BLK_SLICE_FACTOR=BLK_SLICE_FACTOR,\n",
    "            HEAD_DIM=ctx.HEAD_DIM,\n",
    "            num_warps=NUM_WARPS,\n",
    "            num_stages=NUM_STAGES\n",
    "        )\n",
    "\n",
    "        logger.info(f\"逆伝搬終了 {dq.shape=}, {dq.dtype=}, {dk.shape=}, {dk.dtype=}, {dv.shape=}, {dv.dtype=}\")\n",
    "        return dq, dk, dv, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "742e83f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Function.apply of <class '__main__._attention'>>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = _attention.apply\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a5559",
   "metadata": {},
   "source": [
    "### 検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8c40a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TORCH_HAS_FP8 = hasattr(torch, 'float8_e5m2')\n",
    "TORCH_HAS_FP8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f25e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"Z\", [1, 4])\n",
    "@pytest.mark.parametrize(\"H\", [2, 48])\n",
    "@pytest.mark.parametrize(\"N_CTX\", [128, 1024, (2 if is_hip() else 4) * 1024])\n",
    "@pytest.mark.parametrize(\"HEAD_DIM\", [64, 128])\n",
    "@pytest.mark.parametrize(\"causal\", [True])  # FIXME: Non-causal tests do not pass at the moment.\n",
    "@pytest.mark.parametrize(\"warp_specialize\", [False, True] if is_blackwell() else [False])\n",
    "@pytest.mark.parametrize(\"mode\", [\"fwd\", \"bwd\"])\n",
    "@pytest.mark.parametrize(\"provider\", [\"triton-fp16\"] + ([\"triton-fp8\"] if TORCH_HAS_FP8 else []))\n",
    "def test_op(Z, H, N_CTX, HEAD_DIM, causal, warp_specialize, mode, provider, dtype=torch.float16):\n",
    "    logger.info(f\"テスト開始 Z={Z}, H={H}, N_CTX={N_CTX}, HEAD_DIM={HEAD_DIM}, causal={causal}, warp_specialize={warp_specialize}, mode={mode}, provider={provider}, dtype={dtype}\")\n",
    "\n",
    "    if mode == \"fwd\" and \"fp16\" in provider:\n",
    "        pytest.skip(\"Avoid running the forward computation twice.\")\n",
    "\n",
    "    if mode == \"bwd\" and \"fp8\" in provider:\n",
    "        pytest.skip(\"Backward pass with FP8 is not supported.\")\n",
    "\n",
    "    torch.manual_seed(20)\n",
    "\n",
    "    q = (torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=DEVICE).normal_(mean=0.0, std=0.5).requires_grad_())\n",
    "    logger.debug(f\"{q.shape=}\")\n",
    "\n",
    "    k = (torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=DEVICE).normal_(mean=0.0, std=0.5).requires_grad_())\n",
    "    logger.debug(f\"{k.shape=}\")\n",
    "\n",
    "    v = (torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=DEVICE).normal_(mean=0.0, std=0.5).requires_grad_())\n",
    "    logger.debug(f\"{v.shape=}\")\n",
    "\n",
    "    sm_scale = 0.5\n",
    "\n",
    "    ###########################################\n",
    "    # PyTorchの標準関数を使用してアテンションを計算 #\n",
    "    ###########################################\n",
    "\n",
    "    ref_dtype = dtype\n",
    "\n",
    "    if mode == \"fwd\" and \"fp8\" in provider:\n",
    "        ref_dtype = torch.float32\n",
    "\n",
    "    q = q.to(ref_dtype)\n",
    "    k = k.to(ref_dtype)\n",
    "    v = v.to(ref_dtype)\n",
    "\n",
    "    # 因果マスクの作成\n",
    "    M = torch.tril(torch.ones((N_CTX, N_CTX), device=DEVICE))\n",
    "    logger.debug(f\"{M.shape=}\")\n",
    "\n",
    "    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n",
    "    logger.debug(f\"{p.shape=}\")\n",
    "\n",
    "    if causal:\n",
    "        p[:, :, M == 0] = float(\"-inf\")\n",
    "        logger.debug(\"因果マスクを適用\")\n",
    "\n",
    "    p = torch.softmax(p.float(), dim=-1)\n",
    "    logger.debug(\"ソフトマックスを適用\")\n",
    "\n",
    "    p = p.to(ref_dtype)\n",
    "    # p = torch.exp(p)\n",
    "\n",
    "    ref_out = torch.matmul(p, v).half()\n",
    "    logger.debug(f\"バリューを集約 {ref_out.shape=}\")\n",
    "\n",
    "    if mode == \"bwd\":\n",
    "        dout = torch.randn_like(q)\n",
    "        ref_out.backward(dout)\n",
    "        ref_dv, v.grad = v.grad.clone(), None\n",
    "        ref_dk, k.grad = k.grad.clone(), None\n",
    "        ref_dq, q.grad = q.grad.clone(), None\n",
    "\n",
    "    #############################\n",
    "    # Tritonによるアテンション計算 #\n",
    "    #############################\n",
    "\n",
    "    if mode == \"fwd\" and \"fp8\" in provider:\n",
    "        q = q.to(torch.float8_e5m2)\n",
    "        k = k.to(torch.float8_e5m2)\n",
    "        v = v.permute(0, 1, 3, 2).contiguous()\n",
    "        v = v.permute(0, 1, 3, 2)\n",
    "        v = v.to(torch.float8_e5m2)\n",
    "\n",
    "    tri_out = attention(q, k, v, causal, sm_scale, warp_specialize).half()\n",
    "    logger.debug(f\"Tritonによるアテンション計算 {tri_out.shape=}\")\n",
    "\n",
    "    if mode == \"fwd\":\n",
    "        atol = 3 if \"fp8\" in provider else 1e-2\n",
    "        torch.testing.assert_close(tri_out, ref_out, atol=atol, rtol=0)\n",
    "        return\n",
    "\n",
    "    tri_out.backward(dout)\n",
    "    logger.debug(f\"Tritionによる逆伝播計算 {tri_out.shape=}\")\n",
    "\n",
    "    tri_dv, v.grad = v.grad.clone(), None\n",
    "    tri_dk, k.grad = k.grad.clone(), None\n",
    "    tri_dq, q.grad = q.grad.clone(), None\n",
    "\n",
    "    ################\n",
    "    # 計算結果を比較 #\n",
    "    ################\n",
    "\n",
    "    torch.testing.assert_close(tri_out, ref_out, atol=1e-2, rtol=0)\n",
    "    rtol = 0.0\n",
    "\n",
    "    # Relative tolerance workaround for known hardware limitation of CDNA2 GPU.\n",
    "    # For details see https://pytorch.org/docs/stable/notes/numerical_accuracy.html#reduced-precision-fp16-and-bf16-gemms-and-convolutions-on-amd-instinct-mi200-devices\n",
    "    if torch.version.hip is not None and \\\n",
    "        triton.runtime.driver.active.get_current_target().arch == \"gfx90a\":\n",
    "        rtol = 1e-2\n",
    "\n",
    "    torch.testing.assert_close(tri_dv, ref_dv, atol=1e-2, rtol=rtol)\n",
    "    torch.testing.assert_close(tri_dk, ref_dk, atol=1e-2, rtol=rtol)\n",
    "    torch.testing.assert_close(tri_dq, ref_dq, atol=1e-2, rtol=rtol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d0d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🟩 テスト開始 Z=1, H=2, N_CTX=128, HEAD_DIM=64, causal=True, warp_specialize=False, mode=fwd, provider=triton-fp8, dtype=torch.float16\n",
      "🟦 q.shape=torch.Size([1, 2, 128, 64])\n",
      "🟦 k.shape=torch.Size([1, 2, 128, 64])\n",
      "🟦 v.shape=torch.Size([1, 2, 128, 64])\n",
      "🟦 M.shape=torch.Size([128, 128])\n",
      "🟦 p.shape=torch.Size([1, 2, 128, 128])\n",
      "🟦 因果マスクを適用\n",
      "🟦 ソフトマックスを適用\n",
      "🟦 バリューを集約 ref_out.shape=torch.Size([1, 2, 128, 64])\n",
      "🟩 順伝搬開始 q.shape=torch.Size([1, 2, 128, 64])q.dtype=torch.float8_e5m2, k.shape=torch.Size([1, 2, 128, 64]), k.dtype=torch.float8_e5m2, v.shape=torch.Size([1, 2, 128, 64]), v.dtype=torch.float8_e5m2, causal=True, sm_scale=0.5, warp_specialize=False\n",
      "🟦 HEAD_DIM_Q=64\n",
      "🟦 HEAD_DIM_K=64\n",
      "🟦 HEAD_DIM_V=64\n",
      "🟦 o.shape=torch.Size([1, 2, 128, 64]), o.dtype=torch.float8_e5m2\n",
      "🟦 stage=3\n",
      "🟦 extra_kern_args={}\n",
      "🟦 M.shape=torch.Size([1, 2, 128])\n",
      "🟦 y_dim=256\n",
      "🟦 Qのディスクリプタを作成\n",
      "🟦 FP8用のVのディスクリプタを作成\n",
      "🟦 Kのディスクリプタを作成\n",
      "🟦 Oのディスクリプタを作成\n",
      "🟦 起動グリッドを準備\n",
      "🟦 _attn_fwdカーネルの実行\n",
      "🟩 順伝搬終了 o.shape=torch.Size([1, 2, 128, 64]), o.dtype=torch.float8_e5m2\n",
      "🟦 Tritonによるアテンション計算 tri_out.shape=torch.Size([1, 2, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "test_op(\n",
    "    Z=1,\n",
    "    H=2,\n",
    "    N_CTX=128,\n",
    "    HEAD_DIM=32,\n",
    "    causal=True,\n",
    "    warp_specialize=False,\n",
    "    mode=\"fwd\",\n",
    "    provider=\"triton-fp8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🟩 テスト開始 Z=1, H=2, N_CTX=128, HEAD_DIM=64, causal=True, warp_specialize=False, mode=bwd, provider=triton-fp16, dtype=torch.float16\n",
      "🟦 q.shape=torch.Size([1, 2, 128, 64])\n",
      "🟦 k.shape=torch.Size([1, 2, 128, 64])\n",
      "🟦 v.shape=torch.Size([1, 2, 128, 64])\n",
      "🟦 M.shape=torch.Size([128, 128])\n",
      "🟦 p.shape=torch.Size([1, 2, 128, 128])\n",
      "🟦 因果マスクを適用\n",
      "🟦 ソフトマックスを適用\n",
      "🟦 バリューを集約 ref_out.shape=torch.Size([1, 2, 128, 64])\n",
      "/opt/miniconda/envs/py312/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:179.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "🟩 順伝搬開始 q.shape=torch.Size([1, 2, 128, 64])q.dtype=torch.float16, k.shape=torch.Size([1, 2, 128, 64]), k.dtype=torch.float16, v.shape=torch.Size([1, 2, 128, 64]), v.dtype=torch.float16, causal=True, sm_scale=0.5, warp_specialize=False\n",
      "🟦 HEAD_DIM_Q=64\n",
      "🟦 HEAD_DIM_K=64\n",
      "🟦 HEAD_DIM_V=64\n",
      "🟦 o.shape=torch.Size([1, 2, 128, 64]), o.dtype=torch.float16\n",
      "🟦 stage=3\n",
      "🟦 extra_kern_args={}\n",
      "🟦 M.shape=torch.Size([1, 2, 128])\n",
      "🟦 y_dim=256\n",
      "🟦 Qのディスクリプタを作成\n",
      "🟦 Vのディスクリプタを作成\n",
      "🟦 Kのディスクリプタを作成\n",
      "🟦 Oのディスクリプタを作成\n",
      "🟦 起動グリッドを準備\n",
      "🟦 _attn_fwdカーネルの実行\n",
      "🟩 順伝搬終了 o.shape=torch.Size([1, 2, 128, 64]), o.dtype=torch.float16\n",
      "🟦 Tritonによるアテンション計算 tri_out.shape=torch.Size([1, 2, 128, 64])\n",
      "🟩 逆伝搬開始 do.shape=torch.Size([1, 2, 128, 64]), do.dtype=torch.float16\n",
      "🟦 q.shape=torch.Size([1, 2, 128, 64]), k.shape=torch.Size([1, 2, 128, 64]), v.shape=torch.Size([1, 2, 128, 64]), o.shape=torch.Size([1, 2, 128, 64]), M.shape=torch.Size([1, 2, 128])\n",
      "🟦 BATCH=1, N_HEAD=2, N_CTX=128\n",
      "🟦 PRE_BLOCK=128\n",
      "🟦 NUM_WARPS=4, NUM_STAGES=5\n",
      "🟦 BLOCK_M1=32, BLOCK_N1=128, BLOCK_M2=128, BLOCK_N2=32\n",
      "🟦 BLK_SLICE_FACTOR=2\n",
      "🟦 RCP_LN2=1.4426950408889634\n",
      "🟦 arg_k.shape=torch.Size([1, 2, 128, 64])\n",
      "🟦 PRE_BLOCK=128\n",
      "🟦 pre_grid=(1, 2)\n",
      "🟦 delta.shape=torch.Size([1, 2, 128])\n",
      "🟦 _attn_bwd_preprocessカーネルの実行（Deltaを計算）\n",
      "🟦 grid=(1, 1, 2)\n",
      "🟦 _attn_bwdカーネルの実行（dQ・dK・dVを計算）\n",
      "🟩 逆伝搬終了 dq.shape=torch.Size([1, 2, 128, 64]), dq.dtype=torch.float16, dk.shape=torch.Size([1, 2, 128, 64]), dk.dtype=torch.float16, dv.shape=torch.Size([1, 2, 128, 64]), dv.dtype=torch.float16\n",
      "🟦 Tritionによる逆伝播計算 tri_out.shape=torch.Size([1, 2, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "test_op(\n",
    "    Z=1,\n",
    "    H=2,\n",
    "    N_CTX=128,\n",
    "    HEAD_DIM=32,\n",
    "    causal=True,\n",
    "    warp_specialize=False,\n",
    "    mode=\"bwd\",\n",
    "    provider=\"triton-fp16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b2ef5",
   "metadata": {},
   "source": [
    "### ベンチマーク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0115d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from flash_attn.flash_attn_interface import \\\n",
    "        flash_attn_qkvpacked_func as flash_attn_func\n",
    "    HAS_FLASH = True\n",
    "except BaseException:\n",
    "    HAS_FLASH = False\n",
    "\n",
    "HAS_FLASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_HAS_FP8 = hasattr(torch, 'float8_e5m2')\n",
    "TORCH_HAS_FP8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7727bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH, N_HEADS = 4, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vary seq length for fixed head and batch=4\n",
    "configs = []\n",
    "for HEAD_DIM in [64, 128]:\n",
    "    for mode in [\"fwd\", \"bwd\"]:\n",
    "        for causal in [True, False]:\n",
    "            # Enable warpspec for causal fwd on Hopper\n",
    "            enable_ws = mode == \"fwd\" and (is_blackwell() or (is_hopper() and not causal))\n",
    "            for warp_specialize in [False, True] if enable_ws else [False]:\n",
    "                configs.append(\n",
    "                    triton.testing.Benchmark(\n",
    "                        x_names=[\"N_CTX\"],\n",
    "                        x_vals=[2**i for i in range(10, 15)],\n",
    "                        line_arg=\"provider\",\n",
    "                        line_vals=[\"triton-fp16\"] + ([\"triton-fp8\"] if TORCH_HAS_FP8 else []) +\n",
    "                        ([\"flash\"] if HAS_FLASH else []),\n",
    "                        line_names=[\"Triton [FP16]\"] + ([\"Triton [FP8]\"] if TORCH_HAS_FP8 else []) +\n",
    "                        ([\"Flash-2\"] if HAS_FLASH else []),\n",
    "                        styles=[(\"red\", \"-\"), (\"blue\", \"-\"), (\"green\", \"-\")],\n",
    "                        ylabel=\"TFLOPS\",\n",
    "                        plot_name=\n",
    "                        f\"fused-attention-batch{BATCH}-head{N_HEADS}-d{HEAD_DIM}-{mode}-causal={causal}-warp_specialize={warp_specialize}\",\n",
    "                        args={\n",
    "                            \"H\": N_HEADS,\n",
    "                            \"BATCH\": BATCH,\n",
    "                            \"HEAD_DIM\": HEAD_DIM,\n",
    "                            \"mode\": mode,\n",
    "                            \"causal\": causal,\n",
    "                            \"warp_specialize\": warp_specialize,\n",
    "                        },\n",
    "                    ))\n",
    "\n",
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(configs)\n",
    "def bench_flash_attention(BATCH, H, N_CTX, HEAD_DIM, causal, warp_specialize, mode, provider, device=DEVICE):\n",
    "    assert mode in [\"fwd\", \"bwd\"]\n",
    "    dtype = torch.float16\n",
    "    if \"triton\" in provider:\n",
    "        q = torch.randn((BATCH, H, N_CTX, HEAD_DIM), dtype=dtype, device=device, requires_grad=True)\n",
    "        k = torch.randn((BATCH, H, N_CTX, HEAD_DIM), dtype=dtype, device=device, requires_grad=True)\n",
    "        v = torch.randn((BATCH, H, N_CTX, HEAD_DIM), dtype=dtype, device=device, requires_grad=True)\n",
    "        if mode == \"fwd\" and \"fp8\" in provider:\n",
    "            q = q.to(torch.float8_e5m2)\n",
    "            k = k.to(torch.float8_e5m2)\n",
    "            v = v.permute(0, 1, 3, 2).contiguous()\n",
    "            v = v.permute(0, 1, 3, 2)\n",
    "            v = v.to(torch.float8_e5m2)\n",
    "        sm_scale = 1.3\n",
    "        fn = lambda: attention(q, k, v, causal, sm_scale, warp_specialize)\n",
    "        if mode == \"bwd\":\n",
    "            o = fn()\n",
    "            do = torch.randn_like(o)\n",
    "            fn = lambda: o.backward(do, retain_graph=True)\n",
    "        ms = triton.testing.do_bench(fn)\n",
    "\n",
    "    if provider == \"flash\":\n",
    "        qkv = torch.randn((BATCH, N_CTX, 3, H, HEAD_DIM), dtype=dtype, device=device, requires_grad=True)\n",
    "        fn = lambda: flash_attn_func(qkv, causal=causal)\n",
    "        if mode == \"bwd\":\n",
    "            o = fn()\n",
    "            do = torch.randn_like(o)\n",
    "            fn = lambda: o.backward(do, retain_graph=True)\n",
    "        ms = triton.testing.do_bench(fn)\n",
    "    flops_per_matmul = 2.0 * BATCH * H * N_CTX * N_CTX * HEAD_DIM\n",
    "    total_flops = 2 * flops_per_matmul\n",
    "    if causal:\n",
    "        total_flops *= 0.5\n",
    "    if mode == \"bwd\":\n",
    "        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n",
    "    return total_flops * 1e-12 / (ms * 1e-3)\n",
    "\n",
    "# only works on post-Ampere GPUs right now\n",
    "bench_flash_attention.run(save_path=\".\", print_data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
