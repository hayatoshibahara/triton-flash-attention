{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b79f140f",
   "metadata": {},
   "source": [
    "# Flash Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170756b",
   "metadata": {},
   "source": [
    "[è«–æ–‡](https://arxiv.org/abs/2205.14135)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceae840",
   "metadata": {},
   "source": [
    "## æ¦‚è¦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aab3fe",
   "metadata": {},
   "source": [
    "Transformerã¯ã€å…¥åŠ›ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒé•·ã„ã¨å‡¦ç†ãŒé‡ããªã£ã¦ã—ã¾ã†\n",
    "\n",
    "åŸå› ã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è¨ˆç®—æ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•ï¼ˆç³»åˆ—é•·ï¼‰ã®2ä¹—ã«æ¯”ä¾‹ã™ã‚‹ãŸã‚\n",
    "\n",
    "æœ¬è«–æ–‡ã§ã¯ã€ãƒ¡ãƒ¢ãƒªã®èª­ã¿æ›¸ãã‚’è€ƒæ…®ã—ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å†è¨­è¨ˆã—ãŸ**Flash Attention**ã‚’ææ¡ˆã™ã‚‹\n",
    "\n",
    "Flash Attentionã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†ã‘SRAMå†…ã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—ã—ã€ãƒ¡ãƒ¢ãƒªé–“ã®èª­ã¿æ›¸ãå›æ•°ã‚’å‰Šæ¸›ã™ã‚‹æ‰‹æ³•\n",
    "\n",
    "ã“ã®æ‰‹æ³•ã«ã‚ˆã‚Šã€ç³»åˆ—é•·1Kã®GPT-2ã®å­¦ç¿’é€Ÿåº¦ã‚’3å€é«˜é€ŸåŒ–ã—ã€ç³»åˆ—é•·ã‚’åºƒã’ã‚‹ã“ã¨ã§æ€§èƒ½ã‚‚æ”¹å–„ã—ãŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f005a42",
   "metadata": {},
   "source": [
    "## å°å…¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7562c63b",
   "metadata": {},
   "source": [
    "![](image/fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36fbbc4",
   "metadata": {},
   "source": [
    "Transformerã®å‡¦ç†ãŒé‡ã„ç†ç”±ã¯ã€è¨ˆç®—é€Ÿåº¦ã§ã¯ãªããƒ¡ãƒ¢ãƒªã®è»¢é€é€Ÿåº¦ã«ã‚ã‚‹ï¼ˆãƒ¡ãƒ¢ãƒªå¾‹é€Ÿï¼‰\n",
    "\n",
    "GPUã¯ã€ã‚«ãƒ¼ãƒãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«HBMï¼ˆHigh Bandwidth Memoryï¼‰ã‹ã‚‰SRAMã«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€å®Ÿè¡Œå¾Œã«æ›¸ãè¾¼ã‚€\n",
    "\n",
    "HBMã¨SRAMé–“ã®èª­ã¿è¾¼ã¿ã¨æ›¸ãè¾¼ã¿ãŒã€è¨ˆç®—é€Ÿåº¦ã¨æ¯”ã¹é…ã„:\n",
    "\n",
    "- HBM\n",
    "    - è»¢é€é€Ÿåº¦ 1.5TB/s\n",
    "    - ã‚µã‚¤ã‚º 40GB\n",
    "- SRAM\n",
    "    - è»¢é€é€Ÿåº¦ 19TB/s\n",
    "    - ã‚µã‚¤ã‚º 20MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b296be8d",
   "metadata": {},
   "source": [
    "æ¨™æº–çš„ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯æ¬¡ã®ã‚ˆã†ã«è¨ˆç®—ã§ãã‚‹:\n",
    "\n",
    "$$\n",
    "S = QK^{\\top} \\in \\mathbb{R}^{N \\times N}\n",
    "$$\n",
    "\n",
    "- $Q$: $N\\times d$ è¡Œåˆ—ã®ã‚¯ã‚¨ãƒª\n",
    "- $K^\\top$: $N\\times d$ è¡Œåˆ—ã®ã‚­ãƒ¼ã®è»¢ç½®\n",
    "- $S$: $N\\times N$ è¡Œåˆ—ã®ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢\n",
    "\n",
    "$$\n",
    "P = \\text{softmax}(S) \\in \\mathbb{R}^{N \\times N}\n",
    "$$\n",
    "\n",
    "- $P$: $N\\times N$ è¡Œåˆ—ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ï¼ˆè¦ç´ ã®ç·å’ŒãŒ1ã«ãªã‚‹ãƒ†ãƒ³ã‚½ãƒ«ï¼‰\n",
    "\n",
    "$$\n",
    "O = PV \\in \\mathbb{R}^{N \\times d}\n",
    "$$\n",
    "\n",
    "- $V$: $N\\times d$ è¡Œåˆ—ã®ãƒãƒªãƒ¥ãƒ¼\n",
    "- $O$: $N\\times d$ è¡Œåˆ—ã®æœ€çµ‚çš„ãªå‡ºåŠ›ï¼ˆãƒãƒªãƒ¥ãƒ¼ã®é‡ã¿ä»˜ãå’Œï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de23e1",
   "metadata": {},
   "source": [
    "æ¨™æº–çš„ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã¯ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®è¨ˆç®—ã«å…¨ã¦ã®åˆ—ã®æƒ…å ±ãŒå¿…è¦\n",
    "\n",
    "ãã®çµæœHBMã¨SRAMé–“ã®èª­ã¿è¾¼ã¿ã¨æ›¸ãè¾¼ã¿ãŒå¤šãç™ºç”Ÿã—ã¦ã—ã¾ã†:\n",
    "\n",
    "1. HBMã‹ã‚‰ $Q$ ã¨ $K$ ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«SRAMã« **èª­ã¿è¾¼ã¿ã€** ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ $S$ ã‚’è¨ˆç®—ã—ã€HBMã« **æ›¸ãæˆ»ã™**\n",
    "1. HBMã‹ã‚‰ $S$ ã‚’SRAMã« **èª­ã¿è¾¼ã¿ã€** ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ $P$ ã‚’è¨ˆç®—ã—HBMã« **æ›¸ãæˆ»ã™**\n",
    "1. HBMã‹ã‚‰ $P$ ã¨ $V$ ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«SRAMã« **èª­ã¿è¾¼ã¿ã€** $O$ ã‚’è¨ˆç®—ã—HBMã« **æ›¸ãæˆ»ã™**\n",
    "1. Oã‚’è¿”ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce19843",
   "metadata": {},
   "source": [
    "![](image/algorithm0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f75ee2",
   "metadata": {},
   "source": [
    "\n",
    "Flash Attentionã¯ã€ä½é€ŸãªHBMï¼ˆHigh Bandwidth Memoryï¼‰ã¨é«˜é€ŸãªSRAMé–“ã®èª­ã¿æ›¸ããŒå°‘ãªããªã‚‹ã‚ˆã†ã«è¨­è¨ˆ:\n",
    "\n",
    "1. é †ä¼æ’­ã§ã¯ã€å…¥åŠ›ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†å‰²ã—ï¼ˆtilingï¼‰ã€SRAMå†…ã§ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆå›³å‚ç…§ï¼‰\n",
    "2. é€†ä¼æ¬ã§ã¯ã€é †ä¼æ’­ã®ä¸­é–“è¨ˆç®—çµæœï¼ˆ$S$ã¨$P$ï¼‰ã‚’HBMã«ä¿å­˜ã›ãšã€å†è¨ˆç®—ã™ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219fb0f8",
   "metadata": {},
   "source": [
    "æ¨™æº–çš„ãªãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®HBMã¸ã®èª­ã¿æ›¸ãå›æ•°ï¼ˆIOè¤‡é›‘æ€§ï¼‰ã¯ã€ç³»åˆ—é•·ã®2ä¹—ã«æ¯”ä¾‹:\n",
    "\n",
    "$$\n",
    "\\Omega(Nd + N^2)\n",
    "$$\n",
    "\n",
    "- $N$: ç³»åˆ—é•·\n",
    "- $d$: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒæ•°\n",
    "\n",
    "Flash Attentionã®IOè¤‡é›‘æ€§ã¯ã€SRAMã®ã‚µã‚¤ã‚ºã®é€†æ•°ãŒä¹—ç®—ã•ã‚Œã€ç³»åˆ—é•·ã«ç·šå½¢ã«æ¯”ä¾‹ã—ã€æœ€å¤§9å€ä½ããªã‚‹:\n",
    "\n",
    "$$\n",
    "O(N^2 d^2 M^{-1})\n",
    "$$\n",
    "\n",
    "- $M$: SRAMã®ã‚µã‚¤ã‚º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f0bb61",
   "metadata": {},
   "source": [
    "ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®çµæœ:\n",
    "\n",
    "- ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é«˜é€ŸåŒ–ã§ããŸ\n",
    "    - ç³»åˆ—é•·512ã®BERT-largeã®å­¦ç¿’ã‚’15%é«˜é€ŸåŒ–\n",
    "    - ç³»åˆ—é•·1Kã®GPT-2ã®å­¦ç¿’ã‚’3å€é«˜é€ŸåŒ–\n",
    "    - ç³»åˆ—é•·1Kã‹ã‚‰4Kã®long-range arenaã®å­¦ç¿’ã‚’2.4å€é«˜é€ŸåŒ–\n",
    "- ãƒ¢ãƒ‡ãƒ«ã®å“è³ªã‚’æ”¹å–„ã§ããŸ\n",
    "    - GPT-2ã®ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£ã‚’0.7æ”¹å–„\n",
    "    - é•·æ–‡åˆ†é¡ã®æ€§èƒ½ã‚’6.4ãƒã‚¤ãƒ³ãƒˆæ”¹å–„\n",
    "- ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—ã‚’é«˜é€ŸåŒ–ã§ããŸ\n",
    "    - ä¸€èˆ¬çš„ãªç³»åˆ—é•·ã§æœ€å¤§3å€é«˜é€Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e86563",
   "metadata": {},
   "source": [
    "## Flash Attention\n",
    "\n",
    "Flash Attentionã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒã‚¤ãƒ³ãƒˆã¯ã€**ã‚¿ã‚¤ãƒªãƒ³ã‚°** ã¨ **é€†ä¼æ¬æ™‚ã®çµ±è¨ˆé‡ã®å†è¨ˆç®—** ã«ã‚ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62063dd",
   "metadata": {},
   "source": [
    "### ã‚¿ã‚¤ãƒªãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59010716",
   "metadata": {},
   "source": [
    "Flash Attentionã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†ã‘ï¼ˆã‚¿ã‚¤ãƒªãƒ³ã‚°ï¼‰SRAMå†…ã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "SRAMå†…ã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã€å…¥åŠ›å…¨ä½“ã§ã¯ãªãéƒ¨åˆ†çš„ãªã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹è¨ˆç®—ãŒå¿…è¦ã«ãªã‚‹ï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ï¼‰\n",
    "\n",
    "éƒ¨åˆ†çš„ãªã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹è¨ˆç®—ã®ãŸã‚ã«ã€æ¨™æº–çš„ãªã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã‚’ **åˆ†è§£** ã™ã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef153b",
   "metadata": {},
   "source": [
    "åˆ†è§£ã™ã‚‹ãŸã‚ã«ã€ãƒ™ã‚¯ãƒˆãƒ« $x$ ã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹è¨ˆç®—ã«å¯¾ã—ã¦ã€3ã¤ã®ç‰¹å¾´é‡ $m(x)$ãƒ»$f(x)$ãƒ»$l(x)$ ã‚’å®šç¾©\n",
    "\n",
    "$m(x)$ ã¯ã€$x$ ã®è¦ç´ ã® **æœ€å¤§å€¤** :\n",
    "\n",
    "$$\n",
    "m(x) := \\max_{i} x_i\n",
    "$$\n",
    "\n",
    "$f(x)$ ã¯ã€**ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†å­** ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã‚’é˜²ããŸã‚å„è¦ç´ ã‹ã‚‰æœ€å¤§å€¤ã‚’å¼•ãã€æŒ‡æ•°é–¢æ•°ã‚’é©ç”¨ã—ãŸãƒ™ã‚¯ãƒˆãƒ«ï¼‰:\n",
    "\n",
    "$$\n",
    "f(x) := [e^{x_1 - m(x)}, ..., e^{x_B - m(x)}\n",
    "]\n",
    "$$\n",
    "\n",
    "$l(x)$ ã¯ã€ **ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯** ï¼ˆ$f(x)$ ã®å…¨è¦ç´ ã®åˆè¨ˆå€¤ï¼‰:\n",
    "\n",
    "$$\n",
    "l(x) := \\sum_{i} f(x)_i\n",
    "$$\n",
    "\n",
    "$x$ ã®æœ€çµ‚çš„ãªã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®å‡ºåŠ›:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x) := \\frac{f(x)}{l(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7882a757",
   "metadata": {},
   "source": [
    "ä¾‹ãˆã°ã€2ã¤ã®ãƒ™ã‚¯ãƒˆãƒ« $x^{(1)}, x^{(2)} \\in \\mathbb{R}^B$ ã‚’çµåˆã—ãŸ $x =[x^{(1)} x^{(2)}] \\in \\mathbb{R}^{2B}$ã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹è¨ˆç®—ã¯æ¬¡ã®ã‚ˆã†ã«åˆ†è§£ã§ãã‚‹:\n",
    "\n",
    "$m(x)$ã¯ã€çµåˆã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã®è¦ç´ ã®æœ€å¤§å€¤:\n",
    "\n",
    "$$\n",
    "m(x) = m([x^{(1)} x^{(2)}]) = \\max(m(x^{(1)}), m(x^{(2)}))\n",
    "$$\n",
    "\n",
    "$l(x)$ã¯ã€çµåˆã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã®$f(x)$ã®å…¨è¦ç´ ã®åˆè¨ˆå€¤ï¼ˆ$x^{(1)}$ã¨$x^{(2)}$ã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯ã‚’ãã‚Œãã‚Œã®æœ€å¤§å€¤ã§é‡ã¿ä»˜ã‘ã—ãŸå’Œï¼‰:\n",
    "\n",
    "$$\n",
    "l(x) = l([x^{(1)} x^{(2)}]) = e^{m(x^{(1)}) - m(x)}l(x^{(1)}) + e^{m(x^{(2)}) - m(x)}l(x^{(2)})\n",
    "$$\n",
    "\n",
    "çµåˆã—ãŸãƒ™ã‚¯ãƒˆãƒ«ã®æœ€çµ‚çš„ãªã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®å‡ºåŠ›:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x) = \\frac{f(x)}{l(x)}\n",
    "$$\n",
    "\n",
    "æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆä¾‹ãˆã° $x^{(3)}$ï¼‰ãŒè¿½åŠ ã•ã‚ŒãŸå ´åˆã¯ã€è¨ˆç®—æ¸ˆã¿ã® $m(x)$ ã¨ $l(x)$ ã‚’æ›´æ–°ã—ã€ $x^{(3)}$ ã‚’å«ã‚ãŸã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’é«˜é€Ÿã«è¨ˆç®—ã§ãã‚‹\n",
    "\n",
    "ã“ã®ä»•çµ„ã¿ã‚’å¿œç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«SRAMå†…ã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—ã§ãã‚‹ï¼ˆAlgorithm 1ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac621e3",
   "metadata": {},
   "source": [
    "### å†è¨ˆç®—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef3fe5",
   "metadata": {},
   "source": [
    "æ¨™æº–çš„ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å ´åˆã€é€†ä¼æ¬æ™‚ï¼ˆ$dQ$ãƒ»$dK$ãƒ»$dV$ãƒ»$dO$ ã®è¨ˆç®—æ™‚ï¼‰ã«ä¸­é–“è¨ˆç®—çµæœ $S$ ï¼ˆç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ï¼‰ã¨ $P$ ï¼ˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ï¼‰ãŒå¿…è¦\n",
    "\n",
    "Flash Attentionã§ã¯ã€ä¸­é–“è¨ˆç®—çµæœ $S$ ã¨ $P$ ã‚’HBMã«ä¿å­˜ã›ãšã€å†è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ $S$ ã¯ã€$Q$ ã¨ $K$ ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«èª­ã¿è¾¼ã¿å†è¨ˆç®—ã™ã‚‹:\n",
    "\n",
    "$$\n",
    "S_{ij} = \\tau Q_i K_j^T\n",
    "$$\n",
    "\n",
    "- $Q_i$: $i$ ç•ªç›®ã®ã‚¯ã‚¨ãƒª\n",
    "- $K_j$: $j$ ç•ªç›®ã®ã‚­ãƒ¼\n",
    "- $\\tau$ ã¯ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®ã‚¹ã‚±ãƒ¼ãƒ«å®šæ•°\n",
    "\n",
    "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ $P$ ã¯ã€è©²å½“ãƒ–ãƒ­ãƒƒã‚¯ã® $m$ ã¨ $l$ ã‹ã‚‰å†è¨ˆç®—ã§ãã‚‹:\n",
    "\n",
    "$$\n",
    "P_{ij} = \\text{diag}(l_i)^{-1} \\exp{(S_{ij} - m_i)}\n",
    "$$\n",
    "\n",
    "- $\\text{diag}(l_i)^{-1}$: å¯¾è§’è¡Œåˆ—ã®é€†è¡Œåˆ—ã§ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯ã®é€†æ•°\n",
    "- $\\exp{(S_{ij} - m_i)}$: ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†å­\n",
    "\n",
    "å†è¨ˆç®—ã«ã‚ˆã‚Š $S$ ã¨ $P$ ã‚’HBMã«ä¿å­˜ã™ã‚‹å¿…è¦ãŒãªããªã‚Šã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’å‰Šæ¸›ã§ãã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6e739",
   "metadata": {},
   "source": [
    "### ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de93009",
   "metadata": {},
   "source": [
    "Flash Attentionã¯ã€å°ã•ãªãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—ã—ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’å‰Šæ¸›ã™ã‚‹\n",
    "\n",
    "å¤§ã¾ã‹ãªæµã‚Œã¯ã€ $K$ ãƒ–ãƒ­ãƒƒã‚¯ã¨ $V$ ãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦å›ºå®šã—ã€ $Q$ ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ã«å‡¦ç†ã—ã€å¯¾å¿œã™ã‚‹ $O$ ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "HBMã« $Q$ ãƒ» $K$ ãƒ» $V$ ãŒã‚ã‚Šã€SRAMã®ã‚µã‚¤ã‚ºãŒ $M$ ã¨ã™ã‚‹\n",
    "\n",
    "1. SRAMã‚µã‚¤ã‚º $M$ ã«åŸºã¥ã„ã¦ã€ $K$ ãƒ» $V$ ã‹ã‚‰èª­ã¿è¾¼ã‚€ãƒ–ãƒ­ãƒƒã‚¯ã®åˆ—æ•° $B_c$ ã¨ã€$Q$ ã‹ã‚‰èª­ã¿è¾¼ã‚€ãƒ–ãƒ­ãƒƒã‚¯ã®è¡Œæ•° $B_r$ ã‚’è¨­å®š\n",
    "2. HBMã«ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è¨ˆç®—çµæœ $O$ãƒ»ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯ $l$ ãƒ»ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®æœ€å¤§å€¤ $m$ ã‚’æ ¼ç´ã™ã‚‹ãƒ¡ãƒ¢ãƒªã‚’ä½œæˆ\n",
    "3. $Q$ ãƒ» $K$ ãƒ» $V$ ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’æ±ºå®š\n",
    "4. $O$ ãƒ» $l$ ãƒ» $m$ ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’æ±ºå®š\n",
    "5. $K_j$ ãƒ–ãƒ­ãƒƒã‚¯ã¨ $V_j$ ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ç•ªã«å‡¦ç†ã™ã‚‹å¤–å´ã®ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹\n",
    "6. HBMã‹ã‚‰$ K_j$ ãƒ–ãƒ­ãƒƒã‚¯ãƒ» $V_j$ ãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«èª­ã¿è¾¼ã‚€\n",
    "7. $Q_i$ ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ç•ªã«å‡¦ç†ã™ã‚‹å†…å´ã®ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹\n",
    "8. $Q_i$ ãƒ–ãƒ­ãƒƒã‚¯ãƒ» $O_i$ ãƒ–ãƒ­ãƒƒã‚¯ãƒ» $l_i$ ãƒ–ãƒ­ãƒƒã‚¯ãƒ» $m_i$ ãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«èª­ã¿è¾¼ã‚€\n",
    "9. ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ $S_{ij} = Q_iK_j^T$ ã‚’è¨ˆç®—\n",
    "10. ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®è¦ç´ ã®æœ€å¤§å€¤ $\\tilde{m}_{ij} = \\text{rowmax}(S_{ij})$ ã‚’æ±‚ã‚ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ $\\tilde{P}_{ij} = \\exp{(S_{ij} - \\tilde{m}_{ij})}$ ã‚’è¨ˆç®—ã—ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯ $\\tilde{l}_{ij} = \\text{rowsum}(\\tilde{P}_{ij})$ ã‚’è¨ˆç®—\n",
    "11. éå»ã® $m_i$ ã¨ $l_i$ ã¨ã€$\\tilde{m}_ij$ ã¨ $\\tilde{l}_{ij}$ ã‚’ä½¿ã£ã¦ã€æ–°ã—ã„$m_i^{\\text{new}}$ã¨$l_i^{\\text{new}}$ã‚’è¨ˆç®—\n",
    "12. $m_i^{\\text{new}}$ ã¨ $l_i^{\\text{new}}$ ã§éå»ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ $O_i$ ã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦ ã€ç¾åœ¨ã®ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ä½¿ã£ã¦ $O_i$ ã‚’æ›´æ–°ã—ã€HBMã«æ›¸ãæˆ»ã™\n",
    "13. æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã§ä½¿ç”¨ã™ã‚‹ $m_i^{\\text{new}}$ ã¨ $l_i^{\\text{new}}$ ã‚’ $m_i$ ã¨ $l_i$ ã¨ã—ã¦HBMã«æ›¸ãæˆ»ã™\n",
    "14. å†…å´ã®ãƒ«ãƒ¼ãƒ—çµ‚äº†\n",
    "15. å¤–å´ã®ãƒ«ãƒ¼ãƒ—çµ‚äº†\n",
    "16. $O$ã‚’è¿”ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f287cf64",
   "metadata": {},
   "source": [
    "![](image/algorithm1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066f446",
   "metadata": {},
   "source": [
    "## Flash Attentionã®é †ä¼æ’­ã®è©³ç´°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1548296",
   "metadata": {},
   "source": [
    "é †ä¼æ’­ã§ã¯ã€ã‚¯ã‚¨ãƒª $Q$ ãƒ»ã‚­ãƒ¼ $K$ ãƒ»ãƒãƒªãƒ¥ãƒ¼ $V$ ã‹ã‚‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ $O$ ã‚’æ±‚ã‚ã‚‹:\n",
    "\n",
    "$$\n",
    "S = QK^T \\in \\mathbb{R}^{N\\times N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P = \\text{softmax}(S) \\in \\mathbb{R}^{N\\times N}\n",
    "$$\n",
    "\n",
    "$$\n",
    "O = PV \\in \\mathbb{R}^{N\\times d}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc7ffe",
   "metadata": {},
   "source": [
    "ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã®ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ $S_{ij}$ ã¯æ¬¡å¼ã§æ±‚ã‚ã‚‰ã‚Œã‚‹:\n",
    "\n",
    "$$\n",
    "S_{ij} = q_i^T k_j\n",
    "$$\n",
    "\n",
    "- $q_i$: $i$ç•ªç›®ã®ã‚¯ã‚¨ãƒªãƒ–ãƒ­ãƒƒã‚¯\n",
    "- $k_j$: $j$ç•ªç›®ã®ã‚­ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f108c",
   "metadata": {},
   "source": [
    "ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯ã¯æ¬¡å¼ã§æ±‚ã‚ã‚‰ã‚Œã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d659e30d",
   "metadata": {},
   "source": [
    "$$\n",
    "L_i = \\sum_{j} e^{q_i^T k_j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329128e7",
   "metadata": {},
   "source": [
    "$v_j$ ã‚’ $j$ ç•ªç›®ã®ãƒãƒªãƒ¥ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã¨ã™ã‚‹ã¨ã€å‡ºåŠ›ã® $i$ ç•ªç›®ã®ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯æ¬¡å¼ã§æ±‚ã‚ã‚‰ã‚Œã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00b4d4",
   "metadata": {},
   "source": [
    "$$\n",
    "o_i = P_{i:}V = \\sum_{j} P_{ij}v_j = \\sum_{j} \\frac{e^{q_i^T k_j}}{L_i}v_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649ee5f",
   "metadata": {},
   "source": [
    "- $P_{ij}$: $i$ ç•ªç›®ã®ã‚¯ã‚¨ãƒªãƒ–ãƒ­ãƒƒã‚¯ã¨ $j$ ç•ªç›®ã®ã‚­ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa1452b",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šã®è¨ˆç®—å¼ã§ã€$L_i$ ã‚’è¨ˆç®—ã—ã€ $\\frac{e^{q_i^T k_j}}{L_i}v_j$ ã‚’ç¹°ã‚Šè¿”ã—è¶³ã—åˆã‚ã›ã‚‹ã“ã¨ã§ã€$O(N)$ã®ç·šå½¢ãƒ¡ãƒ¢ãƒªã§å…¨ã¦ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ $O$ ã‚’è¨ˆç®—ã§ãã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5d837e",
   "metadata": {},
   "source": [
    "å› æœãƒã‚¹ã‚¯ã¨ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’è€ƒæ…®ã—ãŸå®Œå…¨ãªé †ä¼æ’­ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5164c47e",
   "metadata": {},
   "source": [
    "1. ä¹±æ•°ç”Ÿæˆå™¨ã®çŠ¶æ…‹ $\\mathcal{R}$ ã‚’åˆæœŸåŒ–ã—ã€HBMã«ä¿å­˜\n",
    "2. SRAMã‚µã‚¤ã‚º $M$ ã«åŸºã¥ã„ã¦ã€$K$ ãƒ»$V$ ã‹ã‚‰èª­ã¿è¾¼ã‚€ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã®åˆ—æ•° $B_c$ ã¨ã€Qã‹ã‚‰èª­ã¿è¾¼ã‚€ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã®è¡Œæ•° $B_r$ ã‚’æ±ºå®š\n",
    "3. $O$ ãƒ» $l$ ãƒ» $m$ ã‚’HBMã«åˆæœŸåŒ–\n",
    "4. $Q$ ãƒ» $K$ ãƒ» $V$ ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’æ±ºå®š\n",
    "5. $O$ ãƒ» $T$ ãƒ» $m$ ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’æ±ºå®š\n",
    "6. $K_j$ ãƒ–ãƒ­ãƒƒã‚¯ãƒ» $V_j$ ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ç•ªã«å‡¦ç†ã™ã‚‹å¤–å´ã®ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹\n",
    "7. $K_j$ ãƒ–ãƒ­ãƒƒã‚¯ãƒ» $V_j$ ãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«èª­ã¿è¾¼ã‚€\n",
    "8. $Q_i$ ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ç•ªã«å‡¦ç†ã™ã‚‹å†…å´ã®ãƒ«ãƒ¼ãƒ—\n",
    "9. HBMã‹ã‚‰ $K_j$ ãƒ–ãƒ­ãƒƒã‚¯ã¨ $V_j$ ãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«èª­ã¿è¾¼ã‚€\n",
    "10. ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— $S_{ij} = \\tau Q_j K_j$\n",
    "11. ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã«å› æœãƒã‚¹ã‚¯ã‚’é©ç”¨ $S_{ij}^{\\text{masked}}$\n",
    "12. ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®æœ€å¤§å€¤ $\\tilde{m_{ij}}$ ã‚’æ±‚ã‚ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ $\\tilde{P_{ij}}$ ã¨ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯ $\\tilde{l}_{ij}$ ã‚’è¨ˆç®—\n",
    "13. æœ€å¤§å€¤ã‚’æ›´æ–°ã— $m_i^{\\text{new}}$ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†å­ã‚’æ›´æ–° $l_i^{new}$\n",
    "14. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã«ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’é©ç”¨ $P_{ij}^{\\text{\\text{dropped}}}$\n",
    "15. $l_i^{\\text{new}}$ ã¨ $m_i^{\\text{new}}$ ã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ $O_i$ ã‚’æ›´æ–°ã—ã€HBMã«æ›¸ãæˆ»ã™\n",
    "16. $m_i^{\\text{new}}$ ã¨ $l_i^{\\text{new}}$ ã‚’ $m_i$ ã¨ $l_i$ ã¨ã—ã¦HBMã«æ›¸ãæˆ»ã™\n",
    "17. å†…å´ã®ãƒ«ãƒ¼ãƒ—çµ‚äº†\n",
    "18. å¤–å´ã®ãƒ«ãƒ¼ãƒ—çµ‚äº†\n",
    "19. $O$ ã‚’è¿”ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d91788",
   "metadata": {},
   "source": [
    "![](image/algorithm2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438092cd",
   "metadata": {},
   "source": [
    "## Flash Attentionã®é€†ä¼æ’­è©³ç´°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b25257",
   "metadata": {},
   "source": [
    "é€†ä¼æ¬ã§ã¯ã€å‡ºåŠ›ã®å‹¾é…ã‹ã‚‰ã‚¯ã‚¨ãƒªãƒ»ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã®å‹¾é…ã‚’æ±‚ã‚ã‚‹:\n",
    "\n",
    "- $\\phi$: ã‚¹ã‚«ãƒ©ãƒ¼ã®æå¤±é–¢æ•°\n",
    "- $dO \\in \\mathbb{R}^{n\\times d}$: å‡ºåŠ›ã®å‹¾é… $\\frac{\\partial{\\phi}}{\\partial{O}}$\n",
    "- $dQ \\in \\mathbb{R}^{n\\times d}$: ã‚¯ã‚¨ãƒªã®å‹¾é… $\\frac{\\partial{\\phi}}{\\partial{Q}}$\n",
    "- $dK \\in \\mathbb{R}^{n\\times d}$: ã‚­ãƒ¼ã®å‹¾é… $\\frac{\\partial{\\phi}}{\\partial{K}}$\n",
    "- $dV \\in \\mathbb{R}^{n\\times d}$: ãƒãƒªãƒ¥ãƒ¼ã®å‹¾é… $\\frac{\\partial{\\phi}}{\\partial{V}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c1d96f",
   "metadata": {},
   "source": [
    "$dV$ ã¯é€£é–å¾‹ã‚ˆã‚Šã€$dV = P^T dO$ ã§æ±‚ã‚ã‚‰ã‚Œã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed61b0d",
   "metadata": {},
   "source": [
    "$$\n",
    "dv_j = \\sum_{i} P_{ij} do_i = \\sum_{i} \\frac{e^{q_i^T k_j}}{L_i} do_i\n",
    "$$\n",
    "\n",
    "- $L_i$ ã¯ã€é †ä¼æ’­ã§è¨ˆç®—æ¸ˆã¿ã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯\n",
    "- $dv_j$ ã¯ã€ç¹°ã‚Šè¿”ã—è¶³ã—åˆã‚ã›ã‚‹ã“ã¨ã§è¿½åŠ ã®ãƒ¡ãƒ¢ãƒªç„¡ã—ã§è¨ˆç®—ãŒå¯èƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af273aa",
   "metadata": {},
   "source": [
    "$dQ$ ã¨ $dK$ ã‚’æ±‚ã‚ã‚‹ãŸã‚ã«ã€$dP$ ã¨ $dS$ ãŒå¿…è¦\n",
    "\n",
    "$dP$ ã¯ $dP = dOV^T$ ã§æ±‚ã‚ã‚‰ã‚Œã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e3cb22",
   "metadata": {},
   "source": [
    "$$\n",
    "dP_{ij} = do_i^T v_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aec5e3",
   "metadata": {},
   "source": [
    "$dS$ã¯ã€$y=\\text{softmax}(x)$ã®ãƒ¤ã‚³ãƒ“ã‚¢ãƒ³ãŒ$diag(y) - yy^T$ã¨ã„ã†äº‹å®Ÿã‚ˆã‚Šæ±‚ã‚ã‚‰ã‚Œã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d9541",
   "metadata": {},
   "source": [
    "$$\n",
    "dS_{i:} = (diag(P_{i:}) - P_{i:}^T P_{i:}) dP_{i:} = P_{i:} \\circ dP_{i:} - (P_{i:}^T dP_{i:}) P_{i:}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7713ba",
   "metadata": {},
   "source": [
    "$$\n",
    "D_i = P_{i:}^T dP_{i:} = \\sum_{j} \\frac{e^{q_i^T k_j}}{L_i} do_i^T v_j = do_i^T \\sum_{j} \\frac{e^{q_i^T k_j}}{L_i} v_j = do_i^T o_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b558cd7",
   "metadata": {},
   "source": [
    "$$\n",
    "dS_{i:} = P_{i:} \\circ dP_{i:} - D_i P_{i:}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954e1ca5",
   "metadata": {},
   "source": [
    "$dQ$ ã¯ã€$S_{ij} = q_i^T k_j$ ã‚ˆã‚Šè¨ˆç®—ã§ãã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae5274",
   "metadata": {},
   "source": [
    "$$\n",
    "dq_i = \\sum_{j} dS_{ij} k_j = \\sum_{j} P_{ij}(dP_{ij} - D_i)k_j = \\sum_{j} \\frac{e^{q_i^T k_j}}{L_i}(do_i^T v_j - D_i)k_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d08d91",
   "metadata": {},
   "source": [
    "$dK$ã‚‚åŒæ§˜ã«æ±‚ã‚ã‚‰ã‚Œã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a1a03",
   "metadata": {},
   "source": [
    "$$\n",
    "dk_j = \\sum_{i} dS_{ij} q_i = \\sum_{i} P_{ij}(dP_{ij} - D_i)q_i = \\sum_{i} \\frac{e^{q_i^T k_j}}{L_i}(do_i^T v_j - D_i)q_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dce774",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šã®è¨ˆç®—å¼ã§ã€é€†ä¼æ¬ã‚‚ $O(N)$ ã®ç·šå½¢ãƒ¡ãƒ¢ãƒªã§è¨ˆç®—ã§ãã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588a822",
   "metadata": {},
   "source": [
    "æ¨™æº–çš„ãªé€†ä¼æ’­ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :\n",
    "\n",
    "![](image/algorithm3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca12ffb",
   "metadata": {},
   "source": [
    "Flash Attentionã§ã®é€†ä¼æ’­ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa93187",
   "metadata": {},
   "source": [
    "$K$ã¨$V$ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€$Q$ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ç•ªã«å‡¦ç†ã—ã¦ã€Kã¨Vã®å‹¾é…ã‚’è¶³ã—ç¶šã‘ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7faaac2",
   "metadata": {},
   "source": [
    "1. é †ä¼æ’­ã§ä½¿ç”¨ã—ãŸä¹±æ•°ç”Ÿæˆå™¨ã®çŠ¶æ…‹ $\\mathcal{R}$ ã‚’å¾©å…ƒ\n",
    "2. $K$ãƒ»$V$ã‹ã‚‰èª­ã¿è¾¼ã‚€ãƒ–ãƒ­ãƒƒã‚¯ã®åˆ—æ•°$B_c$ã¨$Q$ã‹ã‚‰èª­ã¿è¾¼ã‚€ãƒ–ãƒ­ãƒƒã‚¯ã®è¡Œæ•°$B_r$ã‚’è¨­å®š\n",
    "3. $Q$ãƒ»$K$ãƒ»$V$ãƒ»ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’æ±ºå®š\n",
    "4. $O$ãƒ»$l$ãƒ»$m$ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’æ±ºå®š\n",
    "5. $dQ$ãƒ»$dK$ãƒ»$dV$ã‚’åˆæœŸåŒ–ã—ã€ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’æ±ºå®š\n",
    "6. $K_j$ãƒ–ãƒ­ãƒƒã‚¯ã¨$V_j$ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ç•ªã«å‡¦ç†ã™ã‚‹å¤–å´ã®ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹\n",
    "7. HBMã‹ã‚‰$K_j$ãƒ»$V_j$ã‚’SRAMã«èª­ã¿è¾¼ã‚€\n",
    "8. SRAMã§$\\tilde{dK_j}$ã¨$\\tilde{dV_j}$ã‚’0ã§åˆæœŸåŒ–\n",
    "9. $Q_i$ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ç•ªã«å‡¦ç†ã™ã‚‹å†…å´ã®ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹\n",
    "10. HBMã‹ã‚‰$Q_i$ãƒ»$O_i$ãƒ»$dO_i$ãƒ»$dQ_i$ãƒ»$l_i$ãƒ»$m_i$ã‚’èª­ã¿è¾¼ã‚€\n",
    "11. ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— $S_ij = \\tau Q_i K_j^T$ï¼ˆ$\\tau$ ã¯ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®ã‚¹ã‚±ãƒ¼ãƒ«å®šæ•°ï¼‰\n",
    "12. å› æœãƒã‚¹ã‚¯ã‚’é©ç”¨ $S_{ij}^{\\text{masked}} = \\text{MASK}(S_{ij})$\n",
    "13. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— $P_{ij} = \\text{diag}(l_i)^{-1} \\exp{(S_{ij}^{\\text{masked}} - m_i)}$\n",
    "14. ä¹±æ•°ç”Ÿæˆå™¨ã‹ã‚‰ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç”¨ã®ãƒã‚¹ã‚¯ã‚’å¾©å…ƒ\n",
    "15. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã«ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’é©ç”¨ $P_{ij}^{\\text{dropped}} = P_{ij} Z_{ij}$\n",
    "16. $\\tilde{dV}$ã«$(P_{ij}^{\\text{dropped}})^T dO_i$ã‚’åŠ ç®—\n",
    "17. $dP_{ij}^{\\text{dropped}} = dO_iV_j^T$ã‚’è¨ˆç®—\n",
    "18. $dP_{ij} = dP_{ij}^{\\text{dropped}} Z_{ij}$ã‚’è¨ˆç®—\n",
    "19. $D_i = \\text{rowsum}(dO_i O_i)$ã‚’è¨ˆç®—\n",
    "20. $dS_{ij} = P_{ij} (dP_{ij} - D_i)$ã‚’è¨ˆç®—\n",
    "21. $dQ_i$ã«$\\tau dS_{ij}K_j$ã‚’åŠ ç®—ã—ã€HBMã«æ›¸ãè¾¼ã‚€\n",
    "22. $\\tilde{dK_j}$ã«$\\tau dS_{ij}^T Q_i$ã‚’åŠ ç®—\n",
    "23. å†…å´ã®ãƒ«ãƒ¼ãƒ—çµ‚äº†\n",
    "24. $\\tilde{dK_j}$ã‚’$dK$ã¨ã—ã¦ã€$\\tilde{dV_j}$ã‚’$dV_j$ã¨ã—ã¦HBMã«æ›¸ãæˆ»ã™\n",
    "25. å¤–å´ã®ãƒ«ãƒ¼ãƒ—çµ‚äº†\n",
    "26. $dQ$ãƒ»$dK$ãƒ»$dV$ã‚’è¿”ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c32330",
   "metadata": {},
   "source": [
    "![](image/algorithm4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f6473",
   "metadata": {},
   "source": [
    "## å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd644d",
   "metadata": {},
   "source": [
    "### ç’°å¢ƒæ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab906351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: triton in /opt/miniconda/envs/py312/lib/python3.12/site-packages (3.4.0)\n",
      "Requirement already satisfied: tabulate in /opt/miniconda/envs/py312/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: pytest in /opt/miniconda/envs/py312/lib/python3.12/site-packages (8.4.2)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from triton) (78.1.1)\n",
      "Requirement already satisfied: iniconfig>=1 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pytest) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pytest) (25.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pytest) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /opt/miniconda/envs/py312/lib/python3.12/site-packages (from pytest) (2.19.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸŸ© Tritonãƒãƒ¼ã‚¸ãƒ§ãƒ³: 3.4.0\n",
      "ğŸŸ© ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda:0\n"
     ]
    }
   ],
   "source": [
    "%pip install triton tabulate pytest\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import tabulate\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.tools.tensor_descriptor import TensorDescriptor\n",
    "import pytest\n",
    "\n",
    "# ãƒ­ã‚°è¨­å®š\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = 'ğŸŸ¦'\n",
    "        case logging.INFO:\n",
    "            level = 'ğŸŸ©'\n",
    "        case logging.WARNING:\n",
    "            level = 'ğŸŸ¨'\n",
    "        case logging.ERROR:\n",
    "            level = 'ğŸŸ¥'\n",
    "        case logging.CRITICAL:\n",
    "            level = 'ğŸ›‘'\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.NOTSET)\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "if os.path.exists('triton.log'):\n",
    "    os.remove('triton.log')\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "file_handler = logging.FileHandler('triton.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# 3.4.0\n",
    "logger.info(f'Tritonãƒãƒ¼ã‚¸ãƒ§ãƒ³: {triton.__version__}')\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "logger.info(f'ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dabb29",
   "metadata": {},
   "source": [
    "### ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768451d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_hip():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"hip\"\n",
    "\n",
    "is_hip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519f3352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_cuda():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n",
    "\n",
    "is_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6110252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def supports_host_descriptor():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] >= 9\n",
    "\n",
    "supports_host_descriptor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa483e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_blackwell():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] == 10\n",
    "\n",
    "is_blackwell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907d3057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_hopper():\n",
    "    return is_cuda() and torch.cuda.get_device_capability()[0] == 9\n",
    "\n",
    "is_hopper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã®è¨­å®š\n",
    "# ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã¯ã€HBMä¸Šã®å·¨å¤§ãªãƒ†ãƒ³ã‚½ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒ­ãƒƒã‚¯ã‚’èª­ã¿è¾¼ã‚€éš›ã®å½¢çŠ¶\n",
    "\n",
    "def _host_descriptor_pre_hook(nargs):\n",
    "    BLOCK_M = nargs[\"BLOCK_M\"]\n",
    "    BLOCK_N = nargs[\"BLOCK_N\"]\n",
    "    HEAD_DIM = nargs[\"HEAD_DIM\"]\n",
    "\n",
    "    if not isinstance(nargs[\"desc_q\"], TensorDescriptor):\n",
    "        return\n",
    "\n",
    "    # Qã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã®å½¢çŠ¶ã‚’ (BLOCK_M, HEAD_DIM) ã«è¨­å®š\n",
    "    # BLOCK_Må€‹ã®ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ï¼ˆHEAD_DIMï¼‰ã‚’èª­ã¿è¾¼ã‚€\n",
    "    nargs[\"desc_q\"].block_shape = [BLOCK_M, HEAD_DIM]\n",
    "\n",
    "    # FP8ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒç•°ãªã‚‹\n",
    "    if nargs[\"FP8_OUTPUT\"]:\n",
    "        nargs[\"desc_v\"].block_shape = [HEAD_DIM, BLOCK_N]\n",
    "    else:\n",
    "        # Vã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã®å½¢çŠ¶ã‚’ (BLOCK_N, HEAD_DIM) ã«è¨­å®š\n",
    "        # BLOCK_Nå€‹ã®ãƒãƒªãƒ¥ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ï¼ˆHEAD_DIMï¼‰ã‚’èª­ã¿è¾¼ã‚€\n",
    "        nargs[\"desc_v\"].block_shape = [BLOCK_N, HEAD_DIM]\n",
    "\n",
    "    # Kã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã®å½¢çŠ¶ã‚’ (BLOCK_N, HEAD_DIM) ã«è¨­å®š\n",
    "    # BLOCK_Nå€‹ã®ã‚­ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ï¼ˆHEAD_DIMï¼‰ã‚’èª­ã¿è¾¼ã‚€\n",
    "    nargs[\"desc_k\"].block_shape = [BLOCK_N, HEAD_DIM]\n",
    "\n",
    "    # Oã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã®å½¢çŠ¶ã‚’ (BLOCK_M, HEAD_DIM) ã«è¨­å®š\n",
    "    # BLOCK_Må€‹ã®å‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ï¼ˆHEAD_DIMï¼‰ã‚’èª­ã¿è¾¼ã‚€\n",
    "    nargs[\"desc_o\"].block_shape = [BLOCK_M, HEAD_DIM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87665e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep(conf):\n",
    "    BLOCK_M = conf.kwargs[\"BLOCK_M\"]\n",
    "    BLOCK_N = conf.kwargs[\"BLOCK_N\"]\n",
    "    return not (\n",
    "        is_cuda() and \\\n",
    "        torch.cuda.get_device_capability()[0] == 9 and \\\n",
    "        BLOCK_M * BLOCK_N < 128 * 128 and \\\n",
    "        conf.num_warps == 8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa859a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_invalid_configs(configs, named_args, **kwargs):\n",
    "    N_CTX = kwargs[\"N_CTX\"]\n",
    "\n",
    "    # Filter out configs where BLOCK_M > N_CTX\n",
    "    return [conf for conf in configs if conf.kwargs.get(\"BLOCK_M\", 0) <= N_CTX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "742594b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _maybe_make_tensor_desc(desc_or_ptr, shape, strides, block_shape):\n",
    "    if isinstance(desc_or_ptr, tl.tensor_descriptor):\n",
    "        return desc_or_ptr\n",
    "    else:\n",
    "        return tl.make_tensor_descriptor(desc_or_ptr, shape, strides, block_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c63c9cc",
   "metadata": {},
   "source": [
    "### è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒŠãƒ¼ã®è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218a530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚¹ãƒ†ãƒ¼ã‚¸æ•°\n",
    "# å¤šã„ã¨ä¸¦åˆ—å®Ÿè¡Œã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒæ”¹å–„ã™ã‚‹ãŒã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚‚å¢—åŠ ã™ã‚‹\n",
    "\n",
    "if is_hip():\n",
    "    NUM_STAGES_OPTIONS = [1]\n",
    "elif supports_host_descriptor():\n",
    "    NUM_STAGES_OPTIONS = [2, 3, 4]\n",
    "else:\n",
    "    NUM_STAGES_OPTIONS = [2, 3, 4]\n",
    "\n",
    "NUM_STAGES_OPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a10f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<triton.runtime.autotuner.Config at 0x759d8663c4d0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c2f0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663cc50>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c080>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c5f0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c410>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c530>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c560>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c5c0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c620>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c650>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c680>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c6b0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c6e0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c710>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c740>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c770>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c7a0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c7d0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c800>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c830>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c860>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c890>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c8c0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c8f0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c920>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c950>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c980>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c9b0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663c9e0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663ca10>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663ca40>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663ca70>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663caa0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663cad0>,\n",
       " <triton.runtime.autotuner.Config at 0x759d8663cb00>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã®ã‚¯ã‚¨ãƒªã®è¡Œæ•°BMã¯64ã¾ãŸã¯128\n",
    "# ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã®ã‚­ãƒ¼/ãƒãƒªãƒ¥ãƒ¼ã®è¡Œæ•°BNã¯32ã€64ã€128\n",
    "# ä¸€ã¤ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹ãƒ¯ãƒ¼ãƒ—æ•°ã¯4ã¾ãŸã¯8\n",
    "configs = [\n",
    "    triton.Config({'BLOCK_M': BM, 'BLOCK_N': BN}, num_stages=s, num_warps=w, pre_hook=_host_descriptor_pre_hook) \\\n",
    "    for BM in [64, 128]\\\n",
    "    for BN in [32, 64, 128]\\\n",
    "    for s in NUM_STAGES_OPTIONS \\\n",
    "    for w in [4, 8]\\\n",
    "]\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆç”¨ã®è¨­å®š\n",
    "if \"PYTEST_VERSION\" in os.environ:\n",
    "    configs = [\n",
    "        triton.Config(dict(BLOCK_M=128, BLOCK_N=64), num_stages=2, num_warps=4, pre_hook=_host_descriptor_pre_hook),\n",
    "    ]\n",
    "\n",
    "configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a97b2",
   "metadata": {},
   "source": [
    "### é †ä¼æ¬ã‚«ãƒ¼ãƒãƒ«"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd94371",
   "metadata": {},
   "source": [
    "#### _attn_fwd_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb42f74",
   "metadata": {},
   "source": [
    "Algorithm 2ã®å®Ÿè£…\n",
    "\n",
    "**1ã¤ã®ã‚¯ã‚¨ãƒªã®ãƒ–ãƒ­ãƒƒã‚¯** ã«å¯¾ã—ã¦ã€ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã®å…¨ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ã«å‡¦ç†ã—ã€ã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚«ãƒ¼ãƒãƒ«\n",
    "\n",
    "ã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿`acc`ã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ $P$ ã¨ãƒãƒªãƒ¥ãƒ¼$V$ ã®è¡Œåˆ—ç©\n",
    "\n",
    "ãƒ©ãƒƒãƒ‘ãƒ¼ã‚«ãƒ¼ãƒãƒ«ï¼ˆ`_attn_fwd`ï¼‰ã§ã€ã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿ã‚’ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯$l$ã§å‰²ã‚‹ã“ã¨ã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—ã™ã‚‹\n",
    "\n",
    "å› æœãƒã‚¹ã‚¯ã‚’é©ç”¨ã™ã‚‹ãŸã‚ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®å¯¾è§’æˆåˆ†ä¸Šã®ãƒ–ãƒ­ãƒƒã‚¯ã¨ãã‚Œä»¥å¤–ã§ç•°ãªã‚‹è¨ˆç®—:\n",
    "\n",
    "- STAGE == 1: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ä»¥å¤–ã®ãƒ–ãƒ­ãƒƒã‚¯å…¨ã¦ã«å¯¾ã™ã‚‹å‡¦ç†ï¼ˆå› æœãƒã‚¹ã‚¯ã‚’è€ƒæ…®ã—ãªã„ï¼‰\n",
    "- STAGE == 2: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ã®ãƒ–ãƒ­ãƒƒã‚¯ã®å‡¦ç†ï¼ˆå› æœãƒã‚¹ã‚¯ã‚’è€ƒæ…®ã™ã‚‹ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4cbed6",
   "metadata": {},
   "source": [
    "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—ã¨å› æœãƒã‚¹ã‚¯ï¼ˆ[ref](https://livebook.manning.com/wiki/categories/llm/causal+attention)ï¼‰:\n",
    "\n",
    "![](image/causal_attention.png)\n",
    "\n",
    "â€» Tritonã§ã¯ã‚¯ã‚¨ãƒªå˜ä½ã§ã¯ãªãã€ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ï¼ˆè¤‡æ•°ã®ã‚¯ã‚¨ãƒªï¼‰ã§å‡¦ç†ã‚’è¡Œã†ãŸã‚å›³ã®è§£é‡ˆã«æ³¨æ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_fwd_inner(\n",
    "    acc, # å‡ºåŠ›ã‚’é›†ç´„ã™ã‚‹ã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿\n",
    "    l_i, # å‰ã‚¹ãƒ†ãƒƒãƒ—ã¾ã§ã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯\n",
    "    m_i, # å‰ã‚¹ãƒ†ãƒƒãƒ—ã¾ã§ã®ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®è¦ç´ ã®æœ€å¤§å€¤\n",
    "    q, # SRAMã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®ã‚¯ã‚¨ãƒªã®ãƒ–ãƒ­ãƒƒã‚¯ (BLOCK_M, HEAD_DIM)\n",
    "    desc_k, # HBMä¸Šã®ã‚­ãƒ¼ã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿\n",
    "    desc_v,  # HBMä¸Šã®ãƒãƒªãƒ¥ãƒ¼ã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿\n",
    "    offset_y, # ãƒ™ãƒ¼ã‚¹ã‚ªãƒ•ã‚»ãƒƒãƒˆï¼ˆç‰¹å®šã®ãƒãƒƒãƒã®ç‰¹å®šã®ãƒ˜ãƒƒãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ã®é–‹å§‹ä½ç½®ï¼‰\n",
    "    dtype: tl.constexpr, # è¨ˆç®—ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å‹\n",
    "    start_m, # Qãƒ–ãƒ­ãƒƒã‚¯ã®é–‹å§‹è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    qk_scale, # QKã«æ›ã‘ã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°\n",
    "    BLOCK_M: tl.constexpr, # Qãƒ–ãƒ­ãƒƒã‚¯ã®è¡Œæ•°\n",
    "    HEAD_DIM: tl.constexpr, # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒæ•°\n",
    "    BLOCK_N: tl.constexpr, # Kãƒ–ãƒ­ãƒƒã‚¯ã‚‚ã—ãã¯Vãƒ–ãƒ­ãƒƒã‚¯ã®è¡Œæ•°\n",
    "    STAGE: tl.constexpr, # å› æœãƒã‚¹ã‚­ãƒ³ã‚°ã®ã‚¹ãƒ†ãƒ¼ã‚¸æŒ‡å®š\n",
    "    offs_m: tl.constexpr, # Qãƒ–ãƒ­ãƒƒã‚¯ã®è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é…åˆ— (BLOCK_M,)\n",
    "    offs_n: tl.constexpr, # Kãƒ–ãƒ­ãƒƒã‚¯ã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é…åˆ— (BLOCK_N,)\n",
    "    N_CTX: tl.constexpr, # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼‰ N\n",
    "    warp_specialize: tl.constexpr, # ãƒ¯ãƒ¼ãƒ—å˜ä½ã§ã®æœ€é©åŒ–ã®ãƒ•ãƒ©ã‚°\n",
    "    IS_HOPPER: tl.constexpr # NVIDIA Hopperã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒ•ãƒ©ã‚°\n",
    "):\n",
    "\n",
    "    #################\n",
    "    # ãƒã‚¤ãƒ³ã‚¿ã®åˆæœŸåŒ– #\n",
    "    #################\n",
    "\n",
    "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ã‚ˆã‚Šå‰ã®éƒ¨åˆ†ã‚’å‡¦ç†ã™ã‚‹å ´åˆï¼ˆoff-diagonalï¼‰\n",
    "    if STAGE == 1:\n",
    "        # Qã®ãƒ–ãƒ­ãƒƒã‚¯ã‚ˆã‚Šã‚‚å‰ã®ä½ç½®ã«ã‚ã‚‹Kã¨Vã®ãƒ–ãƒ­ãƒƒã‚¯ãŒè¨ˆç®—ç¯„å›²\n",
    "        lo, hi = 0, start_m * BLOCK_M\n",
    "\n",
    "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ã«å¯¾ã™ã‚‹å‡¦ç†ã™ã‚‹å ´åˆï¼ˆon-diagonalï¼‰\n",
    "    elif STAGE == 2:\n",
    "        # ç¾åœ¨ã®Qãƒ–ãƒ­ãƒƒã‚¯ã¨åŒã˜ä½ç½®ã«ã‚ã‚‹Kã¨Vã®ãƒ–ãƒ­ãƒƒã‚¯ãŒè¨ˆç®—ç¯„å›²\n",
    "        lo, hi = start_m * BLOCK_M, (start_m + 1) * BLOCK_M\n",
    "        lo = tl.multiple_of(lo, BLOCK_M)\n",
    "\n",
    "    # å› æœãƒã‚¹ã‚­ãƒ³ã‚°ãŒä¸è¦ãªå ´åˆ\n",
    "    else:\n",
    "        # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã«ã‚ãŸã‚‹Kã¨Vã®ãƒ–ãƒ­ãƒƒã‚¯ãŒè¨ˆç®—ç¯„å›²\n",
    "        lo, hi = 0, N_CTX\n",
    "\n",
    "    # Kãƒ–ãƒ­ãƒƒã‚¯ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "    offsetk_y = offset_y + lo\n",
    "\n",
    "    # Vãƒ–ãƒ­ãƒƒã‚¯ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "    # æ³¨æ„: FP8ãƒ‡ãƒ¼ã‚¿å‹ã®å ´åˆã€VãŒè»¢ç½®ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã‚ªãƒ•ã‚»ãƒƒãƒˆè¨ˆç®—ãŒç•°ãªã‚‹\n",
    "    if dtype == tl.float8e5:\n",
    "        offsetv_y = offset_y * HEAD_DIM + lo\n",
    "    else:\n",
    "        offsetv_y = offset_y + lo\n",
    "\n",
    "    #######################################\n",
    "    # Kãƒ–ãƒ­ãƒƒã‚¯ã¨Vãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ç•ªã«å‡¦ç†ã™ã‚‹ãƒ«ãƒ¼ãƒ— #\n",
    "    #######################################\n",
    "\n",
    "    # ãƒã‚¤ãƒ³ã‚¿ã®é–‹å§‹ä½ç½®ã‚’BLOCK_Nãšã¤å‹§ã‚ãªãŒã‚‰ã€Kã¨Vã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ç•ªã«å‡¦ç†\n",
    "    for start_n in tl.range(lo, hi, BLOCK_N, warp_specialize=warp_specialize):\n",
    "\n",
    "        # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ã¸ã®ãƒ’ãƒ³ãƒˆï¼ˆKã¨Vã®ãƒ–ãƒ­ãƒƒã‚¯ã®é–‹å§‹è¡Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯BLOCK_Nã®å€æ•°ï¼‰\n",
    "        start_n = tl.multiple_of(start_n, BLOCK_N)\n",
    "\n",
    "        # Kã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰ã—ã€è»¢ç½®\n",
    "        # (BLOCK_N, HEAD_DIM) -> (HEAD_DIM, BLOCK_N)\n",
    "        k = desc_k.load([offsetk_y, 0]).T\n",
    "\n",
    "        # ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢QKã‚’è¨ˆç®—\n",
    "        # S_ij = Qi_K_j^T\n",
    "        # (BLOCK_M, HEAD_DIM) @ (HEAD_DIM, BLOCK_N) -> (BLOCK_M, BLOCK_N)\n",
    "        qk = tl.dot(q, k)\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—ã®å¯¾è§’ã®å ´åˆï¼ˆon-diagonalï¼‰\n",
    "        if STAGE == 2:\n",
    "\n",
    "            # ã€ŒQã®è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ >= Kã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ã‚’æº€ãŸã™2æ¬¡å…ƒå› æœãƒã‚¹ã‚¯ã‚’ä½œæˆ\n",
    "            mask = offs_m[:, None] >= (start_n + offs_n[None, :])\n",
    "\n",
    "            # ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢QKã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’èª¿æ•´ã—ã€ãƒã‚¹ã‚¯ã‚’é©ç”¨\n",
    "            # S_ij^{masked} = MASK(\\tau S_{ij})\n",
    "            # (BLOCK_M, BLOCK_N)\n",
    "            qk = qk * qk_scale + tl.where(mask, 0, -1.0e6)\n",
    "\n",
    "            # èª¿æ•´ã—ãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢QKã®è¦ç´ ã®æœ€å¤§å€¤ã‚’æ›´æ–°\n",
    "            # m_i^{new} = max(m_i, rowmax(S_ij^{masked}))\n",
    "            # (BLOCK_M,)\n",
    "            m_ij = tl.maximum(m_i, tl.max(qk, 1))\n",
    "\n",
    "            # èª¿æ•´ã—ãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‹ã‚‰æœ€å¤§å€¤ã‚’å¼•ã\n",
    "            # S_ij = S_ij^{masked} - m_i^{new}\n",
    "            # (BLOCK_M, BLOCK_N) - (BLOCK_M, 1) -> (BLOCK_M, BLOCK_N)\n",
    "            qk -= m_ij[:, None]\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ã‚ˆã‚Šå‰ã®éƒ¨åˆ†ã®å ´åˆï¼ˆoff-diagonalï¼‰\n",
    "        else:\n",
    "            # ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢QKã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’èª¿æ•´ã—ã€æœ€å¤§å€¤ã‚’æ›´æ–°\n",
    "            # m_i^{new} = max(m_i, rowmax(Ï„ S_ij) * \\tau)\n",
    "            # (BLOCK_M,)\n",
    "            m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)\n",
    "\n",
    "            # èª¿æ•´ã—ãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‹ã‚‰æœ€å¤§å€¤ã‚’å¼•ã\n",
    "            # S_ij = S_ij - m_i^{new}\n",
    "            # (BLOCK_M, BLOCK_N) - (BLOCK_M, 1) -> (BLOCK_M, BLOCK_N)\n",
    "            qk = qk * qk_scale - m_ij[:, None]\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢Pã‚’è¨ˆç®—ï¼ˆæŒ‡æ•°ã§ã¯ãªã2ã®ã¹ãä¹—ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§é«˜é€ŸåŒ–ï¼‰\n",
    "        # \\tilde{P}_{ij} = \\exp(S_{ij})\n",
    "        # (BLOCK_M, BLOCK_N)\n",
    "        p = tl.math.exp2(qk)\n",
    "\n",
    "        # æœ€å¤§å€¤ãŒæ›´æ–°ã•ã‚ŒãŸãŸã‚ã€å†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç”¨ã®ã‚¢ãƒ«ãƒ•ã‚¡ã‚’è¨ˆç®—ï¼ˆe^{m_i-m_i^{new}}\n",
    "        # alpha = exp(m_i - m_i^{new})\n",
    "        # (BLOCK_M,)\n",
    "        alpha = tl.math.exp2(m_i - m_ij)\n",
    "\n",
    "        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯ã‚’è¨ˆç®—ï¼ˆã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’åˆ—æ–¹å‘ã«æ½°ã—ã¦åˆè¨ˆï¼‰\n",
    "        # \\tilde{l}_{ij} = rowsum(\\tilde{P}_{ij})\n",
    "        # (BLOCK_M,)\n",
    "        l_ij = tl.sum(p, 1)\n",
    "\n",
    "        # å‡ºåŠ›ã®ã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿ã‚’æ›´æ–°\n",
    "        if not IS_HOPPER and warp_specialize and BLOCK_M == 128 and HEAD_DIM == 128:\n",
    "            BM: tl.constexpr = acc.shape[0]\n",
    "            BN: tl.constexpr = acc.shape[1]\n",
    "            acc0, acc1 = acc.reshape([BM, 2, BN // 2]).permute(0, 2, 1).split()\n",
    "            acc0 = acc0 * alpha[:, None]\n",
    "            acc1 = acc1 * alpha[:, None]\n",
    "            acc = tl.join(acc0, acc1).permute(0, 2, 1).reshape([BM, BN])\n",
    "        else:\n",
    "            # éå»ã«è¨ˆç®—ã—ãŸã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿ã‚’ã‚¢ãƒ«ãƒ•ã‚¡ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆæ–°ã—ã„æœ€å¤§å€¤ã®åŸºæº–ã«åˆã‚ã›ã‚‹ï¼‰\n",
    "            # O_i = O_i * alpha\n",
    "            acc = acc * alpha[:, None]\n",
    "\n",
    "        # Vãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰\n",
    "        # FP8ãƒ‡ãƒ¼ã‚¿å‹ã®å ´åˆã¯ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒç•°ãªã‚‹ï¼ˆBlockwellä¸–ä»£ä»¥å‰ï¼‰\n",
    "        if dtype == tl.float8e5:\n",
    "            # (HEAD_DIM, BLOCK_N) -> (BLOCK_N, HEAD_DIM)\n",
    "            v = desc_v.load([0, offsetv_y]).T\n",
    "        else:\n",
    "            # (BLOCK_N, HEAD_DIM)\n",
    "            v = desc_v.load([offsetv_y, 0])\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢Pã‚’ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "        p = p.to(dtype)\n",
    "\n",
    "        # Pã¨Vã®å†…ç©ã‚’è¨ˆç®—ã—ã€ã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿ã«åŠ ç®—\n",
    "        # O_i += \\tilde{P}_{ij} V_j\n",
    "        # (BLOCK_M, BLOCK_N) @ (BLOCK_N, HEAD_DIM) -> (BLOCK_M, HEAD_DIM)\n",
    "        acc = tl.dot(p, v, acc)\n",
    "\n",
    "        ###################\n",
    "        # æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã¸ã®æº–å‚™ #\n",
    "        ###################\n",
    "\n",
    "        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯ã‚’æ›´æ–°\n",
    "        # l_i^{new} = l_i * alpha + \\tilde{l}_{ij}\n",
    "        l_i = l_i * alpha + l_ij\n",
    "\n",
    "        # æœ€å¤§å€¤ã‚’æ›´æ–°\n",
    "        m_i = m_ij\n",
    "\n",
    "        # Kã®ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’æ›´æ–°\n",
    "        offsetk_y += BLOCK_N\n",
    "\n",
    "        # Vã®ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’æ›´æ–°\n",
    "        offsetv_y += BLOCK_N\n",
    "\n",
    "    return acc, l_i, m_i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec8aa06",
   "metadata": {},
   "source": [
    "#### _attn_fwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac96426d",
   "metadata": {},
   "source": [
    "**1ã¤ã®ã‚¯ã‚¨ãƒªãƒ–ãƒ­ãƒƒã‚¯ã«å¯¾ã™ã‚‹** å…¨ã¦ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—ã‚’å®Ÿè¡Œã™ã‚‹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼\n",
    "\n",
    "æ‹…å½“ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¯ã‚¨ãƒªãƒ†ãƒ³ã‚½ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€STAGEã‚’æ±ºå®šã—ã€_attn_fwd_innerã‚’å‘¼ã³å‡ºã—ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è¨ˆç®—ã‚’å®Œæˆã•ã›ã‚‹\n",
    "\n",
    "å› æœãƒã‚¹ã‚¯ã‚’é©å¿œã™ã‚‹å ´åˆã€å¯¾è§’æˆåˆ†ä»¥å¤–ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’å‡¦ç†ã—ã€å¯¾è§’æˆåˆ†ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’ç¶šã‘ã¦å‡¦ç†ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4284fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è‡ªå‹•ãƒãƒ¥ãƒ¼ãƒŠãƒ¼ã®è¨­å®š\n",
    "@triton.autotune(\n",
    "    configs=list(filter(keep, configs)),\n",
    "    key=[\"N_CTX\", \"HEAD_DIM\", \"FP8_OUTPUT\", \"warp_specialize\"],\n",
    "    prune_configs_by={'early_config_prune': prune_invalid_configs}\n",
    ")\n",
    "@triton.jit\n",
    "def _attn_fwd(\n",
    "    sm_scale, # QKã«æ›ã‘ã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°\n",
    "    M, # ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®è¦ç´ ã®æœ€å¤§å€¤mã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    Z, # ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    H, # ãƒ˜ãƒƒãƒ‰æ•°\n",
    "    desc_q, # HBMä¸Šã®Qã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿\n",
    "    desc_k, # HBMä¸Šã®Kã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿\n",
    "    desc_v, # HBMä¸Šã®Vã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿\n",
    "    desc_o, # HBMä¸Šã®Oã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿\n",
    "    N_CTX, # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼‰\n",
    "    HEAD_DIM: tl.constexpr, # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒæ•° d\n",
    "    BLOCK_M: tl.constexpr, # Qãƒ–ãƒ­ãƒƒã‚¯ã®è¡Œæ•° B_r\n",
    "    BLOCK_N: tl.constexpr, # Kãƒ–ãƒ­ãƒƒã‚¯ã‚‚ã—ãã¯Vãƒ–ãƒ­ãƒƒã‚¯ã®åˆ—æ•° B_c\n",
    "    FP8_OUTPUT: tl.constexpr, # FP8ãƒ‡ãƒ¼ã‚¿å‹ã‚’ä½¿ç”¨ã™ã‚‹ãƒ•ãƒ©ã‚°\n",
    "    STAGE: tl.constexpr, # å› æœãƒã‚¹ã‚¯ã‚’ä½¿ã†å ´åˆã¯3ã€ä½¿ã‚ãªã„å ´åˆã¯1\n",
    "    warp_specialize: tl.constexpr, # ãƒ¯ãƒ¼ãƒ—ã®å‡¦ç†ã‚’æœ€é©åŒ–ã™ã‚‹ãƒ•ãƒ©ã‚°\n",
    "    IS_HOPPER: tl.constexpr, # NVIDIA Hopperã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒ•ãƒ©ã‚°\n",
    "):\n",
    "\n",
    "    #########\n",
    "    # åˆæœŸåŒ– #\n",
    "    #########\n",
    "\n",
    "    dtype = tl.float8e5 if FP8_OUTPUT else tl.float16\n",
    "\n",
    "    tl.static_assert(BLOCK_N <= HEAD_DIM)\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ IDã®0æ¬¡å…ƒç›®ã‚’å–å¾—ã—ã€Qã®ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã™ã‚‹\n",
    "    start_m = tl.program_id(0)\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ IDã®1æ¬¡å…ƒç›®ã‚’å–å¾—ã—ã€ãƒ˜ãƒƒãƒ‰ã¨ãƒãƒƒãƒã®è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã™ã‚‹\n",
    "    off_hz = tl.program_id(1)\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒæ‹…å½“ã™ã‚‹ãƒãƒƒãƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "    off_z = off_hz // H\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒæ‹…å½“ã™ã‚‹ãƒ˜ãƒƒãƒ‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "    off_h = off_hz % H\n",
    "\n",
    "    # ãƒãƒƒãƒã¨ãƒ˜ãƒƒãƒ‰å…¨ä½“ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç·æ•°ï¼ˆå¹³å¦åŒ–ã«å¿…è¦ï¼‰\n",
    "    y_dim = Z * H * N_CTX\n",
    "\n",
    "    ####################\n",
    "    # ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ #\n",
    "    ####################\n",
    "\n",
    "    # HBMä¸Šã®Qã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
    "    desc_q = _maybe_make_tensor_desc(\n",
    "        desc_q,\n",
    "        shape=[y_dim, HEAD_DIM], # ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶\n",
    "        strides=[HEAD_DIM, 1], # ãƒ¡ãƒ¢ãƒªä¸Šã§ã®ãƒ‡ãƒ¼ã‚¿ã®ä¸¦ã³é †\n",
    "        block_shape=[BLOCK_M, HEAD_DIM] # SRAMã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã®å½¢çŠ¶ï¼ˆBLOCK_Må€‹ã®ã‚¯ã‚¨ãƒªãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ã‚’èª­ã¿è¾¼ã‚€ï¼‰\n",
    "    )\n",
    "\n",
    "    # HBMä¸Šã®Vã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
    "    # FP8ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€VãŒè»¢ç½®ã•ã‚Œã¦ã„ã‚‹\n",
    "    if FP8_OUTPUT:\n",
    "        desc_v = _maybe_make_tensor_desc(\n",
    "            desc_v,\n",
    "            shape=[HEAD_DIM, y_dim], # ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶\n",
    "            strides=[N_CTX, 1], # ãƒ¡ãƒ¢ãƒªä¸Šã§ã®ãƒ‡ãƒ¼ã‚¿ã®ä¸¦ã³é †\n",
    "            block_shape=[HEAD_DIM, BLOCK_N] # SRAMã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã®å½¢çŠ¶\n",
    "        )\n",
    "    else:\n",
    "        desc_v = _maybe_make_tensor_desc(\n",
    "            desc_v, # HBMä¸Šã®Vã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿\n",
    "            shape=[y_dim, HEAD_DIM], # ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶\n",
    "            strides=[HEAD_DIM, 1], # ãƒ¡ãƒ¢ãƒªä¸Šã§ã®ãƒ‡ãƒ¼ã‚¿ã®ä¸¦ã³é †\n",
    "            block_shape=[BLOCK_N, HEAD_DIM] # SRAMã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã®å½¢çŠ¶ï¼ˆBLOCK_Nå€‹ã®ãƒãƒªãƒ¥ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ã‚’èª­ã¿è¾¼ã‚€ï¼‰\n",
    "        )\n",
    "\n",
    "    # HBMä¸Šã®Kã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
    "    desc_k = _maybe_make_tensor_desc(\n",
    "        desc_k,\n",
    "        shape=[y_dim, HEAD_DIM], # ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶\n",
    "        strides=[HEAD_DIM, 1], # ãƒ¡ãƒ¢ãƒªä¸Šã§ã®ãƒ‡ãƒ¼ã‚¿ã®ä¸¦ã³é †\n",
    "        block_shape=[BLOCK_N, HEAD_DIM] # SRAMã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã®å½¢çŠ¶ï¼ˆBLOCK_Nå€‹ã®ã‚­ãƒ¼ãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ã‚’èª­ã¿è¾¼ã‚€ï¼‰\n",
    "    )\n",
    "\n",
    "    # HBMä¸Šã®Oã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
    "    desc_o = _maybe_make_tensor_desc(\n",
    "        desc_o,\n",
    "        shape=[y_dim, HEAD_DIM], # ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢çŠ¶\n",
    "        strides=[HEAD_DIM, 1], # ãƒ¡ãƒ¢ãƒªä¸Šã§ã®ãƒ‡ãƒ¼ã‚¿ã®ä¸¦ã³é †\n",
    "        block_shape=[BLOCK_M, HEAD_DIM] # SRAMã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã®å½¢çŠ¶ï¼ˆBLOCK_Må€‹ã®å‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ«å…¨ä½“ã‚’èª­ã¿è¾¼ã‚€ï¼‰\n",
    "    )\n",
    "\n",
    "    ######################\n",
    "    # å¤‰æ•°ã¨ãƒã‚¤ãƒ³ã‚¿ã®åˆæœŸåŒ– #\n",
    "    ######################\n",
    "\n",
    "    # æ‹…å½“ã™ã‚‹ãƒ™ãƒ¼ã‚¹ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "    # ãƒãƒƒãƒã‚ªãƒ•ã‚»ãƒƒãƒˆ * ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· * ãƒ˜ãƒƒãƒ‰æ•° + ãƒ˜ãƒƒãƒ‰ã‚ªãƒ•ã‚»ãƒƒãƒˆ * ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·\n",
    "    offset_y = off_z * (N_CTX * H) + off_h * N_CTX\n",
    "\n",
    "    # æ‹…å½“ã™ã‚‹Qãƒ–ãƒ­ãƒƒã‚¯ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "    # ãƒ™ãƒ¼ã‚¹ã‚ªãƒ•ã‚»ãƒƒãƒˆ + Qãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ * Qãƒ–ãƒ­ãƒƒã‚¯ã®è¡Œæ•°\n",
    "    qo_offset_y = offset_y + start_m * BLOCK_M\n",
    "\n",
    "    # Qãƒ–ãƒ­ãƒƒã‚¯ã®è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é…åˆ—ã‚’ä½œæˆ\n",
    "    # (BLOCK_M,)\n",
    "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "\n",
    "    # Kãƒ–ãƒ­ãƒƒã‚¯ã‚‚ã—ãã¯Vãƒ–ãƒ­ãƒƒã‚¯ã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é…åˆ—ã‚’ä½œæˆ\n",
    "    # (BLOCK_N,)\n",
    "    offs_n = tl.arange(0, BLOCK_N)\n",
    "\n",
    "    # ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®æœ€å¤§å€¤ã‚’ãƒã‚¤ãƒŠã‚¹ç„¡é™å¤§ã§åˆæœŸåŒ–\n",
    "    # (BLOCK_M,)\n",
    "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n",
    "\n",
    "    # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯ã‚’1.0ã§åˆæœŸåŒ–\n",
    "    # (BLOCK_M,)\n",
    "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n",
    "\n",
    "    # å‡ºåŠ›ã®ã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿ã‚’0ã§åˆæœŸåŒ–\n",
    "    # (BLOCK_M, HEAD_DIM)\n",
    "    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    ################################\n",
    "    # Qã®ãƒ­ãƒ¼ãƒ‰ã¨ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã®å‘¼ã³å‡ºã— #\n",
    "    ################################\n",
    "\n",
    "    # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã‚’è¨ˆç®—ï¼ˆæŒ‡æ•°ã§ã¯ãªã2ã®ã¹ãä¹—ã‚’ä½¿ç”¨ã—ã¦é«˜é€ŸåŒ–ï¼‰\n",
    "    qk_scale = sm_scale # 0.5\n",
    "    qk_scale *= 1.44269504 # 1/log(2)\n",
    "\n",
    "    # æ‹…å½“ã™ã‚‹Qãƒ–ãƒ­ãƒƒã‚¯ã‚’HBMã‹ã‚‰SRAMã«èª­ã¿è¾¼ã¿\n",
    "    # (BLOCK_M, HEAD_DIM)\n",
    "    q = desc_q.load([qo_offset_y, 0])\n",
    "\n",
    "    # STAGEãŒ1ã‚‚ã—ãã¯3ã®å ´åˆï¼ˆ0b01 & 0b01 || 0b11 & 0b01ï¼‰\n",
    "    if STAGE & 1:\n",
    "        # 1ã®å ´åˆã€å› æœãƒã‚¹ã‚¯ãªã—ã§å…¨ã¦è¨ˆç®—\n",
    "        # 3ã®å ´åˆã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ã‚ˆã‚Šä»¥å¤–ã®éƒ¨åˆ†ã«å¯¾ã™ã‚‹è¨ˆç®—ï¼ˆoff-diagonalï¼‰\n",
    "        acc, l_i, m_i = _attn_fwd_inner(\n",
    "            acc,\n",
    "            l_i,\n",
    "            m_i,\n",
    "            q,\n",
    "            desc_k,\n",
    "            desc_v,\n",
    "            offset_y,\n",
    "            dtype,\n",
    "            start_m,\n",
    "            qk_scale,\n",
    "            BLOCK_M,\n",
    "            HEAD_DIM,\n",
    "            BLOCK_N,\n",
    "            4 - STAGE, # STAGEãŒ1ãªã‚‰3ã€STAGEãŒ3ãªã‚‰1ã‚’æ¸¡ã™\n",
    "            offs_m,\n",
    "            offs_n,\n",
    "            N_CTX,\n",
    "            warp_specialize,\n",
    "            IS_HOPPER\n",
    "        )\n",
    "\n",
    "    # STAGEãŒ3ã®å ´åˆï¼ˆ0b11 & 0b10ï¼‰\n",
    "    if STAGE & 2:\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ã«å¯¾ã™ã‚‹è¨ˆç®—ï¼ˆon-diagonalï¼‰\n",
    "        acc, l_i, m_i = _attn_fwd_inner(\n",
    "            acc,\n",
    "            l_i,\n",
    "            m_i,\n",
    "            q,\n",
    "            desc_k,\n",
    "            desc_v,\n",
    "            offset_y,\n",
    "            dtype,\n",
    "            start_m,\n",
    "            qk_scale,\n",
    "            BLOCK_M,\n",
    "            HEAD_DIM,\n",
    "            BLOCK_N,\n",
    "            2, # STAGEã«2ã‚’æ¸¡ã™\n",
    "            offs_m,\n",
    "            offs_n,\n",
    "            N_CTX,\n",
    "            warp_specialize,\n",
    "            IS_HOPPER\n",
    "        )\n",
    "\n",
    "    ###################\n",
    "    # æœ€çµ‚å‡¦ç†ã¨æ›¸ãå‡ºã— #\n",
    "    ###################\n",
    "\n",
    "    # ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®è¦ç´ ã®æœ€å¤§å€¤m_iã‚’ã€æ•°å€¤çš„ãªå®‰å®šæ€§ã®ãŸã‚ã«log-sum-expã«å¤‰æ›\n",
    "    # log-sum-exp = m_i + log(l_i)\n",
    "    # (BLOCK_M,)\n",
    "    m_i += tl.math.log2(l_i)\n",
    "\n",
    "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—\n",
    "    # O_i = ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†å­ / ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®åˆ†æ¯\n",
    "    acc = acc / l_i[:, None]\n",
    "\n",
    "    # m_iã‚’HBMã«æ›¸ãå‡ºã™ãƒã‚¤ãƒ³ã‚¿ã‚’è¨ˆç®—\n",
    "    # Mã®ãƒã‚¤ãƒ³ã‚¿ + ãƒ˜ãƒƒãƒ‰ã¨ãƒãƒƒãƒã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ * ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· + Qãƒ–ãƒ­ãƒƒã‚¯ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ\n",
    "    # (BLOCK_M,)\n",
    "    m_ptrs = M + off_hz * N_CTX + offs_m\n",
    "\n",
    "    # m_iã‚’HBMã«æ›¸ãå‡ºã—\n",
    "    tl.store(m_ptrs, m_i)\n",
    "\n",
    "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’HBMã«æ›¸ãå‡ºã—\n",
    "    # (BLOCK_M, HEAD_DIM)\n",
    "    desc_o.store([qo_offset_y, 0], acc.to(dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9be3d2",
   "metadata": {},
   "source": [
    "### é€†ä¼æ’­ã‚«ãƒ¼ãƒãƒ«"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b0465",
   "metadata": {},
   "source": [
    "Algorithm 4ã‚’å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9cebd8",
   "metadata": {},
   "source": [
    "#### _attn_bwd_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e890b5",
   "metadata": {},
   "source": [
    "ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®å‹¾é…è¨ˆç®—ã«å¿…è¦ãªDeltaï¼ˆ$D_i$ï¼‰ã‚’æ±‚ã‚ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚«ãƒ¼ãƒãƒ«\n",
    "\n",
    "$$\n",
    "D_i = \\text{rowsum}(dO_i O_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd53cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_bwd_preprocess(\n",
    "    O, # é †ä¼æ’­ã®å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«\n",
    "    DO, # Oã®å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«\n",
    "    Delta, # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®å‹¾é…è¨ˆç®—ã«å¿…è¦ãªä¸­é–“ãƒ†ãƒ³ã‚½ãƒ«ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    Z, # ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    H, # ãƒ˜ãƒƒãƒ‰æ•°\n",
    "    N_CTX, # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼‰\n",
    "    BLOCK_M: tl.constexpr, # ã“ã®ã‚«ãƒ¼ãƒãƒ«ãŒå‡¦ç†ã™ã‚‹è¡Œæ•°\n",
    "    HEAD_DIM: tl.constexpr # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒæ•°\n",
    "):\n",
    "    #################\n",
    "    # ãƒã‚¤ãƒ³ã‚¿ã®åˆæœŸåŒ– #\n",
    "    #################\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ IDã®0æ¬¡å…ƒç›®ã‚’å–å¾—ã—ã€è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "    # (BLOCK_M,)\n",
    "    off_m = tl.program_id(0) * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ IDã®1æ¬¡å…ƒç›®ã‚’å–å¾—ã—ã€ãƒ˜ãƒƒãƒ‰ã¨ãƒãƒƒãƒã®è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã™ã‚‹\n",
    "    off_hz = tl.program_id(1)\n",
    "\n",
    "    # åˆ—æ–¹å‘ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ\n",
    "    # (HEAD_DIM,)\n",
    "    off_n = tl.arange(0, HEAD_DIM)\n",
    "\n",
    "    ################ \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ #\n",
    "    ################ \n",
    "\n",
    "    # é †ä¼æ’­ã®çµæœã®ä¸€éƒ¨ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "    # ãƒ™ãƒ¼ã‚¹ã‚ªãƒ•ã‚»ãƒƒãƒˆ + ãƒãƒƒãƒã¨ãƒ˜ãƒƒãƒ‰ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ + åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆ\n",
    "    # (BLOCK_M, HEAD_DIM)\n",
    "    o = tl.load(\n",
    "        O + \\\n",
    "        off_hz * HEAD_DIM * N_CTX + \\\n",
    "        off_m[:, None] * HEAD_DIM + \\\n",
    "        off_n[None, :]\n",
    "    )\n",
    "\n",
    "    # é †ä¼æ’­ã®çµæœã®å‹¾é…ã®ä¸€éƒ¨ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "    # ãƒ™ãƒ¼ã‚¹ã‚ªãƒ•ã‚»ãƒƒãƒˆ + ãƒãƒƒãƒã¨ãƒ˜ãƒƒãƒ‰ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ + åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆ\n",
    "    # (BLOCK_M, HEAD_DIM)\n",
    "    do = tl.load(\n",
    "        DO + \\\n",
    "        off_hz * HEAD_DIM * N_CTX + \\\n",
    "        off_m[:, None] * HEAD_DIM + \\\n",
    "        off_n[None, :] \\\n",
    "    ).to(tl.float32)\n",
    "\n",
    "    ##############\n",
    "    # Deltaã®è¨ˆç®— #\n",
    "    ##############\n",
    "\n",
    "    # oã¨doã‚’è¦ç´ ã”ã¨ã«ä¹—ç®—ã—ã€åˆ—æ–¹å‘ã‚’æ½°ã—ã¦åˆè¨ˆã‚’è¨ˆç®—\n",
    "    # (BLOCK_M, HEAD_DIM) * (BLOCK_M, HEAD_DIM) -> (BLOCK_M,)\n",
    "    delta = tl.sum(o * do, axis=1)\n",
    "\n",
    "    # HBMä¸Šã®Deltaã«æ›¸ãå‡ºã—\n",
    "    # ãƒ™ãƒ¼ã‚¹ã‚ªãƒ•ã‚»ãƒƒãƒˆ + ãƒãƒƒãƒã¨ãƒ˜ãƒƒãƒ‰ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ\n",
    "    tl.store(Delta + off_hz * N_CTX + off_m, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71580a45",
   "metadata": {},
   "source": [
    "#### _attn_bwd_dkdv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d836a",
   "metadata": {},
   "source": [
    "Qãƒ–ãƒ­ãƒƒã‚¯ã«å¯¾ã™ã‚‹ã‚­ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã®å‹¾é… $dK$ ãƒ»ãƒãƒªãƒ¥ãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ã®å‹¾é… $dV$ ã‚’è¨ˆç®—ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°\n",
    "\n",
    "**SRAMã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®$K$ãƒ–ãƒ­ãƒƒã‚¯ã¨$V$ãƒ–ãƒ­ãƒƒã‚¯ã‚’å›ºå®šã—**ã€$Q$ãƒ–ãƒ­ãƒƒã‚¯ã¨$dO$ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ã«å‡¦ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788effb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_bwd_dkdv(\n",
    "    dk, # Kã®å‹¾é…ã‚’ç´¯ç©ã™ã‚‹å¤‰æ•°\n",
    "    dv, # Vã®å‹¾é…ã‚’ç´¯ç©ã™ã‚‹å¤‰æ•°\n",
    "    Q, # é †ä¼æ’­ã§ä½¿ç”¨ã—ãŸã‚¯ã‚¨ãƒªãƒ†ãƒ³ã‚½ãƒ«ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    k, # SRAMã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®Kãƒ–ãƒ­ãƒƒã‚¯\n",
    "    v, # SRAMã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®Vãƒ–ãƒ­ãƒƒã‚¯\n",
    "    sm_scale, # é †ä¼æ’­ã§ä½¿ç”¨ã—ãŸã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼ˆ1/log(2)ï¼‰\n",
    "    DO, # å‡ºåŠ›Oã®å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    M, # é †ä¼æ’­ã®æœ€å¾Œã«ä¿å­˜ã—ãŸMãƒ†ãƒ³ã‚½ãƒ«ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    D, # äº‹å‰è¨ˆç®—ã•ã‚ŒãŸDeltaãƒ†ãƒ³ã‚½ãƒ«ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    stride_tok, # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ¬¡ã®è¡Œï¼‰ã«é€²ã‚€ãŸã‚ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ï¼ˆQãƒ»Kãƒ»Vãƒ»DOå…±é€šï¼‰\n",
    "    stride_d, # æ¬¡ã®æ¬¡å…ƒï¼ˆæ¬¡ã®åˆ—ï¼‰ã«é€²ã‚€ãŸã‚ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ï¼ˆQãƒ»Kãƒ»Vãƒ»DOå…±é€šï¼‰\n",
    "    H, # ãƒ˜ãƒƒãƒ‰æ•°\n",
    "    N_CTX, # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼‰\n",
    "    BLOCK_M1: tl.constexpr, # ã“ã®ã‚«ãƒ¼ãƒãƒ«ã§ä½¿ç”¨ã™ã‚‹Qãƒ–ãƒ­ãƒƒã‚¯ã®è¡Œæ•°\n",
    "    BLOCK_N1: tl.constexpr, # ã“ã®ã‚«ãƒ¼ãƒãƒ«ã§ä½¿ç”¨ã™ã‚‹Kãƒ–ãƒ­ãƒƒã‚¯ã‚‚ã—ãã¯Vãƒ–ãƒ­ãƒƒã‚¯ã®åˆ—æ•°\n",
    "    HEAD_DIM: tl.constexpr, # ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒæ•°\n",
    "    start_n, # æ‹…å½“ã™ã‚‹Kãƒ–ãƒ­ãƒƒã‚¯ã®é–‹å§‹åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    start_m, # æ‹…å½“ã™ã‚‹Qãƒ–ãƒ­ãƒƒã‚¯ã®é–‹å§‹è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    num_steps, # forãƒ«ãƒ¼ãƒ—ã®åå¾©å›æ•°\n",
    "    MASK: tl.constexpr # å› æœãƒã‚¹ã‚­ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°\n",
    "):\n",
    "    #################\n",
    "    # ãƒã‚¤ãƒ³ã‚¿ã‚’åˆæœŸåŒ– #\n",
    "    #################\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹Qãƒ–ãƒ­ãƒƒã‚¯ã®è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ã‚’è¨ˆç®—\n",
    "    # (BLOCK_M1,)\n",
    "    offs_m = start_m + tl.arange(0, BLOCK_M1)\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹Kãƒ–ãƒ­ãƒƒã‚¯ã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ã‚’è¨ˆç®—\n",
    "    # (BLOCK_N1,)\n",
    "    offs_n = start_n + tl.arange(0, BLOCK_N1)\n",
    "\n",
    "    # ãƒ˜ãƒƒãƒ€ã®æ¬¡å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é…åˆ—ã‚’ä½œæˆ\n",
    "    # (HEAD_DIM,)\n",
    "    offs_k = tl.arange(0, HEAD_DIM)\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹Qãƒ–ãƒ­ãƒƒã‚¯ã®ãƒã‚¤ãƒ³ã‚¿ã‚’è¨ˆç®—\n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒã‚¤ãƒ³ã‚¿ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ * ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ + åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆ * æ¬¡å…ƒã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "    # (BLOCK_M1, HEAD_DIM)\n",
    "    qT_ptrs = Q + \\\n",
    "        offs_m[None, :] * stride_tok + \\\n",
    "        offs_k[:, None] * stride_d\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹DOãƒ–ãƒ­ãƒƒã‚¯ã®ãƒã‚¤ãƒ³ã‚¿ã‚’è¨ˆç®—\n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒã‚¤ãƒ³ã‚¿ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ * ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ + åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆ * æ¬¡å…ƒã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "    # (BLOCK_M1, HEAD_DIM)\n",
    "    do_ptrs = DO + \\\n",
    "        offs_m[:, None] * stride_tok + \\\n",
    "        offs_k[None, :] * stride_d\n",
    "\n",
    "    # BLOCK_N1ã¯BLOCK_M1ã®å€æ•°ã§ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨¼\n",
    "    tl.static_assert(BLOCK_N1 % BLOCK_M1 == 0)\n",
    "\n",
    "    #########################\n",
    "    # Qã¨DOã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ã«å‡¦ç† #\n",
    "    #########################\n",
    "\n",
    "    curr_m = start_m\n",
    "    step_m = BLOCK_M1\n",
    "    for blk_idx in range(num_steps):\n",
    "\n",
    "        ##########################\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®å†è¨ˆç®— #\n",
    "        ##########################\n",
    "\n",
    "        # Qãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰\n",
    "        # (BLOCK_M1, HEAD_DIM)\n",
    "        qT = tl.load(qT_ptrs)\n",
    "\n",
    "        # ç”Ÿã®ã‚¢ãƒ†ãƒ³ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®è¦ç´ ã®æœ€å¤§å€¤m_iã¸ã®ãƒã‚¤ãƒ³ã‚¿ã‚’è¨ˆç®—\n",
    "        # (BLOCK_M1,)\n",
    "        offs_m = curr_m + tl.arange(0, BLOCK_M1)\n",
    "\n",
    "        # m_iã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰\n",
    "        # (BLOCK_M1,)\n",
    "        m = tl.load(M + offs_m)\n",
    "\n",
    "        # ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’å†è¨ˆç®— S_ij = Q_i K_j^T\n",
    "        # (BLOCK_M1, BLOCK_N1) = (BLOCK_M1, HEAD_DIM) @ (HEAD_DIM, BLOCK_N1)\n",
    "        qkT = tl.dot(k, qT)\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢Pã‚’å†è¨ˆç®— P_ij = exp(S_ij - m_i)\n",
    "        # (BLOCK_M1, BLOCK_N1)\n",
    "        pT = tl.math.exp2(qkT - m[None, :])\n",
    "\n",
    "        ############\n",
    "        # å‹¾é…ã®è¨ˆç®— #\n",
    "        ############\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã«ãƒã‚¹ã‚¯ã‚’é©ç”¨\n",
    "        if MASK:\n",
    "            mask = (offs_m[None, :] >= offs_n[:, None])\n",
    "            pT = tl.where(mask, pT, 0.0)\n",
    "\n",
    "        # DOãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰\n",
    "        # (BLOCK_M1, HEAD_DIM)\n",
    "        do = tl.load(do_ptrs)\n",
    "\n",
    "        # pTã‚’ã‚³ãƒ”ãƒ¼ã—ã¦fp16ã«ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "        # (BLOCK_M1, BLOCK_N1)\n",
    "        ppT = pT\n",
    "        ppT = ppT.to(tl.float16)\n",
    "\n",
    "        # dVã‚’ç´¯ç©\n",
    "        # \\tilde{dV_j} = \\tilde{dV_j} + P_{ij}^T dO_i\n",
    "        # (BLOCK_N1, BLOCK_M1) @ (BLOCK_M1, HEAD_DIM) = (BLOCK_N1, HEAD_DIM)\n",
    "        dv += tl.dot(ppT, do)\n",
    "\n",
    "        # ãƒ‡ãƒ«ã‚¿ãƒ†ãƒ³ã‚½ãƒ« D_i ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰\n",
    "        # (BLOCK_N1,)\n",
    "        Di = tl.load(D + offs_m)\n",
    "\n",
    "        # dPã‚’è¨ˆç®—\n",
    "        # dP_{ij} = dO @ V^T\n",
    "        dpT = tl.dot(v, tl.trans(do)).to(tl.float32)\n",
    "\n",
    "        # dSã‚’è¨ˆç®—\n",
    "        # dS_{ij} = P_{ij} (dP_{ij} - D_i)\n",
    "        dsT = pT * (dpT - Di[None, :])\n",
    "        dsT = dsT.to(tl.float16)\n",
    "\n",
    "        # dKã‚’è¨ˆç®—ã—ã€ã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿ã«åŠ ç®—\n",
    "        # \\tilde{dK_j} = \\tilde{dK_j} + dS_{ij}^T Q_i\n",
    "        # (HEAD_DIM, BLOCK_N1) @ (BLOCK_N1, BLOCK_M1) = (HEAD_DIM, BLOCK_M1)\n",
    "        dk += tl.dot(dsT, tl.trans(qT))\n",
    "\n",
    "        ################\n",
    "        # ãƒã‚¤ãƒ³ã‚¿ã®æ›´æ–° #\n",
    "        ################\n",
    "\n",
    "        curr_m += step_m\n",
    "        qT_ptrs += step_m * stride_tok\n",
    "        do_ptrs += step_m * stride_tok\n",
    "\n",
    "    return dk, dv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7715f690",
   "metadata": {},
   "source": [
    "#### _attn_bwd_dq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9da5aef",
   "metadata": {},
   "source": [
    "`_attn_bwd_dq`ã¯ã€$dQ$ï¼ˆã‚¯ã‚¨ãƒªã®å‹¾é…ï¼‰ã‚’è¨ˆç®—ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°\n",
    "\n",
    "**SRAMã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®ä¸€ã¤ã®Qãƒ–ãƒ­ãƒƒã‚¯ã‚’å›ºå®šã—**ã€Kãƒ–ãƒ­ãƒƒã‚¯ã¨Vãƒ–ãƒ­ãƒƒã‚¯ã‚’ãƒ«ãƒ¼ãƒ—ã—ã¦è¨ˆç®—ã—ãŸå‹¾é…ã‚’ã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿ã«åŠ ç®—ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e646f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_bwd_dq(\n",
    "    dq, # Qã®å‹¾é…ã®ã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿\n",
    "    q, # SRAMã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®Qãƒ–ãƒ­ãƒƒã‚¯\n",
    "    K, # Kãƒ†ãƒ³ã‚½ãƒ«å…¨ä½“ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    V, # Vãƒ†ãƒ³ã‚½ãƒ«å…¨ä½“ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    do, # qã«å¯¾å¿œã™ã‚‹å‡ºåŠ›Oã®å‹¾é…ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    m, # qã«å¯¾å¿œã™ã‚‹ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®è¦ç´ ã®æœ€å¤§å€¤mã®ãƒ–ãƒ­ãƒƒã‚¯\n",
    "    D, # äº‹å‰è¨ˆç®—æ¸ˆã¿ã®Deltaãƒ–ãƒ­ãƒƒã‚¯ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    stride_tok, # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ¬¡ã®è¡Œï¼‰ã«é€²ã‚€ãŸã‚ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ï¼ˆQãƒ»Kãƒ»Vãƒ»DOå…±é€šï¼‰\n",
    "    stride_d, # æ¬¡ã®æ¬¡å…ƒï¼ˆæ¬¡ã®åˆ—ï¼‰ã«é€²ã‚€ãŸã‚ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ï¼ˆQãƒ»Kãƒ»Vãƒ»DOå…±é€šï¼‰\n",
    "    H, # ãƒ˜ãƒƒãƒ‰æ•°\n",
    "    N_CTX, # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼‰\n",
    "    BLOCK_M2: tl.constexpr, # ã“ã®ã‚«ãƒ¼ãƒãƒ«ã§ä½¿ç”¨ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º\n",
    "    BLOCK_N2: tl.constexpr, # ã“ã®ã‚«ãƒ¼ãƒãƒ«ã§ä½¿ç”¨ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º\n",
    "    HEAD_DIM: tl.constexpr, # ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒæ•°\n",
    "    start_m, # æ‹…å½“ã™ã‚‹Qãƒ–ãƒ­ãƒƒã‚¯ã®é–‹å§‹è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    start_n, # æ‹…å½“ã™ã‚‹Kãƒ–ãƒ­ãƒƒã‚¯ã®é–‹å§‹åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    num_steps, # forãƒ«ãƒ¼ãƒ—ã®åå¾©å›æ•°\n",
    "    MASK: tl.constexpr # å› æœãƒã‚¹ã‚­ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°\n",
    "):\n",
    "    #################\n",
    "    # ãƒã‚¤ãƒ³ã‚¿ã®åˆæœŸåŒ–\n",
    "    #################\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹Qãƒ–ãƒ­ãƒƒã‚¯ã®è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ã‚’è¨ˆç®—\n",
    "    # (BLOCK_M2,)\n",
    "    offs_m = start_m + tl.arange(0, BLOCK_M2)\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹Kãƒ–ãƒ­ãƒƒã‚¯ã®åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ã‚’è¨ˆç®—\n",
    "    # (BLOCK_N2,)\n",
    "    offs_n = start_n + tl.arange(0, BLOCK_N2)\n",
    "\n",
    "    # ãƒ˜ãƒƒãƒ€ã®æ¬¡å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é…åˆ—ã‚’ä½œæˆ\n",
    "    # (HEAD_DIM,)\n",
    "    offs_k = tl.arange(0, HEAD_DIM)\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹Kãƒ–ãƒ­ãƒƒã‚¯ã®ãƒã‚¤ãƒ³ã‚¿ã‚’è¨ˆç®—\n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒã‚¤ãƒ³ã‚¿ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ * ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ + åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆ * æ¬¡å…ƒã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "    # (HEAD_DIM, BLOCK_N2)\n",
    "    kT_ptrs = K + \\\n",
    "        offs_n[None, :] * stride_tok + \\\n",
    "        offs_k[:, None] * stride_d\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹Vãƒ–ãƒ­ãƒƒã‚¯ã®ãƒã‚¤ãƒ³ã‚¿ã‚’è¨ˆç®—\n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒã‚¤ãƒ³ã‚¿ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ * ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ + åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆ * æ¬¡å…ƒã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "    # (HEAD_DIM, BLOCK_N2)\n",
    "    vT_ptrs = V + \\\n",
    "        offs_n[None, :] * stride_tok + \\\n",
    "        offs_k[:, None] * stride_d\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹Dãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰\n",
    "    # æ³¨æ„: Dï¼ˆDeltaï¼‰ã¯äº‹å‰ã«ds_scaleã§å‰²ã‚‰ã‚Œã¦ã„ã‚‹\n",
    "    # (BLOCK_M2,)\n",
    "    Di = tl.load(D + offs_m)\n",
    "\n",
    "    # BLOCK_M2ã¯BLOCK_N2ã®å€æ•°ã§ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨¼\n",
    "    tl.static_assert(BLOCK_M2 % BLOCK_N2 == 0)\n",
    "\n",
    "    ########################\n",
    "    # Kã¨Vã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’é †ã«å‡¦ç† #\n",
    "    ########################\n",
    "\n",
    "    curr_n = start_n\n",
    "    step_n = BLOCK_N2\n",
    "    for blk_idx in range(num_steps):\n",
    "\n",
    "        # Kãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰\n",
    "        # (HEAD_DIM, BLOCK_N2)\n",
    "        kT = tl.load(kT_ptrs)\n",
    "\n",
    "        # Vãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰\n",
    "        # (HEAD_DIM, BLOCK_N2)\n",
    "        vT = tl.load(vT_ptrs)\n",
    "\n",
    "        # ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’å†è¨ˆç®— S_ij = Q_i K_j^T\n",
    "        # (BLOCK_M2, HEAD_DIM) @ (HEAD_DIM, BLOCK_N2) -> (BLOCK_M2, BLOCK_N2)\n",
    "        qk = tl.dot(q, kT)\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢Pã‚’å†è¨ˆç®— P_ij = exp(S_ij - m_i)\n",
    "        # (BLOCK_M2, BLOCK_N2)\n",
    "        p = tl.math.exp2(qk - m)\n",
    "\n",
    "        # å› æœãƒã‚¹ã‚¯ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "        if MASK:\n",
    "            # ãƒã‚¹ã‚¯ã‚’ä½œæˆ\n",
    "            offs_n = curr_n + tl.arange(0, BLOCK_N2)\n",
    "            mask = (offs_m[:, None] >= offs_n[None, :])\n",
    "\n",
    "            # ãƒã‚¹ã‚¯ã‚’é©ç”¨\n",
    "            p = tl.where(mask, p, 0.0)\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®å‹¾é…dPã‚’è¨ˆç®—\n",
    "        # dP_{ij} = dO_{i} V_j^T\n",
    "        dp = tl.dot(do, vT).to(tl.float32)\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®å‹¾é…dSã‚’è¨ˆç®—\n",
    "        # dS = P * (dP - D_i)\n",
    "        ds = p * (dp - Di[:, None])\n",
    "        ds = ds.to(tl.float16)\n",
    "\n",
    "        # Qã®å‹¾é…dQã‚’è¨ˆç®—ã—ã€ã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿ã«åŠ ç®—\n",
    "        # dQ_i = dQ_i + dS_{ij} K_j\n",
    "        # æ³¨æ„: kTã¯äº‹å‰ã«qk_scaleã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ãŸã‚æˆ»ã™å¿…è¦ãŒã‚ã‚‹\n",
    "        dq += tl.dot(ds, tl.trans(kT))\n",
    "\n",
    "        ################\n",
    "        # ãƒã‚¤ãƒ³ã‚¿ã®æ›´æ–° #\n",
    "        ################\n",
    "\n",
    "        # Kãƒ–ãƒ­ãƒƒã‚¯ã®åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’æ›´æ–°\n",
    "        curr_n += step_n\n",
    "\n",
    "        # Kãƒ–ãƒ­ãƒƒã‚¯ã®ãƒã‚¤ãƒ³ã‚¿ã‚’æ›´æ–°\n",
    "        kT_ptrs += step_n * stride_tok\n",
    "\n",
    "        # Vãƒ–ãƒ­ãƒƒã‚¯ã®ãƒã‚¤ãƒ³ã‚¿ã‚’æ›´æ–°\n",
    "        vT_ptrs += step_n * stride_tok\n",
    "\n",
    "    return dq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417236c5",
   "metadata": {},
   "source": [
    "#### _attn_bwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81ea1ce",
   "metadata": {},
   "source": [
    "é€†ä¼æ¬å…¨ä½“ã‚’åˆ¶å¾¡ã™ã‚‹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼\n",
    "\n",
    "æ‹…å½“ç¯„å›²ã‚’æ±ºå®šã—ã€å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã€ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã€$dQ$ ãƒ» $dK$ ãƒ» $dV$ ã‚’HBMã«æ›¸ãæˆ»ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dec1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _attn_bwd(\n",
    "    Q, # é †ä¼æ’­ã§ä½¿ç”¨ã—ãŸã‚¯ã‚¨ãƒªãƒ†ãƒ³ã‚½ãƒ«ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    K, # é †ä¼æ’­ã§ä½¿ç”¨ã—ãŸã‚­ãƒ¼ãƒ†ãƒ³ã‚½ãƒ«ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    V, # é †ä¼æ’­ã§ä½¿ç”¨ã—ãŸãƒãƒªãƒ¥ãƒ¼ãƒ†ãƒ³ã‚½ãƒ«ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    sm_scale, # é †ä¼æ’­ã§ä½¿ç”¨ã—ãŸã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ï¼ˆ1/log(2)ï¼‰\n",
    "    DO, # å‡ºåŠ›Oã®å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«å…¨ä½“ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    DQ, # è¨ˆç®—çµæœã‚’æ›¸ãè¾¼ã‚€Qã®å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«å…¨ä½“ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    DK, # è¨ˆç®—çµæœã‚’æ›¸ãè¾¼ã‚€Kã®å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«å…¨ä½“ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    DV, # è¨ˆç®—çµæœã‚’æ›¸ãè¾¼ã‚€Vã®å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«å…¨ä½“ã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    M, # é †ä¼æ’­ã§ä¿å­˜ã—ãŸç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®è¦ç´ ã®æœ€å¤§å€¤mã¸ã®ãƒã‚¤ãƒ³ã‚¿\n",
    "    D, # äº‹å‰è¨ˆç®—ã•ã‚ŒãŸDeltaãƒ†ãƒ³ã‚½ãƒ«\n",
    "    stride_z, # ãƒãƒƒãƒæ–¹å‘ã«é€²ã‚€ãŸã‚ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ï¼ˆQãƒ»Kãƒ»Vãƒ»DOå…±é€šï¼‰\n",
    "    stride_h, # ãƒ˜ãƒƒãƒ‰æ–¹å‘ã«é€²ã‚€ãŸã‚ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ï¼ˆQãƒ»Kãƒ»Vãƒ»DOå…±é€šï¼‰\n",
    "    stride_tok, # æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ¬¡ã®è¡Œï¼‰ã«é€²ã‚€ãŸã‚ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ï¼ˆQãƒ»Kãƒ»Vãƒ»DOå…±é€šï¼‰\n",
    "    stride_d, # æ¬¡ã®æ¬¡å…ƒï¼ˆæ¬¡ã®åˆ—ï¼‰ã«é€²ã‚€ãŸã‚ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ï¼ˆQãƒ»Kãƒ»Vãƒ»DOå…±é€šï¼‰\n",
    "    H, # ãƒ˜ãƒƒãƒ‰æ•°\n",
    "    N_CTX, # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼‰\n",
    "    BLOCK_M1: tl.constexpr, # dKã¨dVã®è¨ˆç®—ã«ä½¿ç”¨ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º\n",
    "    BLOCK_N1: tl.constexpr, # dKã¨dVã®è¨ˆç®—ã«ä½¿ç”¨ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º\n",
    "    BLOCK_M2: tl.constexpr,  # dQã®è¨ˆç®—ã«ä½¿ç”¨ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º\n",
    "    BLOCK_N2: tl.constexpr,  # dQã®è¨ˆç®—ã«ä½¿ç”¨ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º\n",
    "    BLK_SLICE_FACTOR: tl.constexpr, # ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ›´ã«ç´°ã‹ãã‚¹ãƒ©ã‚¤ã‚¹ã™ã‚‹éš›ã®åˆ†å‰²æ•°\n",
    "    HEAD_DIM: tl.constexpr # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒæ•°\n",
    "):\n",
    "\n",
    "    #########\n",
    "    # åˆæœŸåŒ– #\n",
    "    #########\n",
    "\n",
    "    # ln(2)ã‚’å®šæ•°ã¨ã—ã¦å®šç¾©\n",
    "    LN2: tl.constexpr = 0.6931471824645996\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹IDã®2æ¬¡å…ƒç›®ã‚’å–å¾—ã—ã€ãƒãƒƒãƒã¨ãƒ˜ãƒƒãƒ‰ã®è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã™ã‚‹\n",
    "    bhid = tl.program_id(2)\n",
    "\n",
    "    # ãƒãƒƒãƒã¨ãƒ˜ãƒƒãƒ‰ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "    off_chz = (bhid * N_CTX).to(tl.int64)\n",
    "\n",
    "    # ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨ˆç®—ã—ã€INT64ã«ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "    # ãƒ˜ãƒƒãƒ€ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ + ãƒãƒƒãƒã®ã‚ªãƒ•ã‚»ãƒƒãƒˆ\n",
    "    adj = (stride_h * (bhid % H) + stride_z * (bhid // H)).to(tl.int64)\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹IDã®0æ¬¡å…ƒç›®ã‚’å–å¾—ã—ã€è¡Œã‚‚ã—ãã¯åˆ—ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã™ã‚‹\n",
    "    pid = tl.program_id(0)\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒæ‹…å½“ã™ã‚‹ãƒãƒƒãƒã¨ãƒ˜ãƒƒãƒ‰ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’ãƒã‚¤ãƒ³ã‚¿ã«åæ˜ \n",
    "    Q += adj\n",
    "    K += adj\n",
    "    V += adj\n",
    "    DO += adj\n",
    "    DQ += adj\n",
    "    DK += adj\n",
    "    DV += adj\n",
    "    M += off_chz\n",
    "    D += off_chz\n",
    "\n",
    "    # ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®é…åˆ—ã‚’ä½œæˆ\n",
    "    offs_k = tl.arange(0, HEAD_DIM)\n",
    "\n",
    "    ##############\n",
    "    # dKã¨dVã‚’è¨ˆç®— #\n",
    "    ##############\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒæ‹…å½“ã™ã‚‹Kãƒ–ãƒ­ãƒƒã‚¯ãƒ»Vãƒ–ãƒ­ãƒƒã‚¯ã®é–‹å§‹åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "    start_n = pid * BLOCK_N1\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒæ‹…å½“ã™ã‚‹Qãƒ–ãƒ­ãƒƒã‚¯ã®é–‹å§‹è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "    start_m = start_n\n",
    "\n",
    "    # ä¸€åº¦ã«å‡¦ç†ã™ã‚‹Kãƒ»Vãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’æ›´ã«ç´°ã‹ãã™ã‚‹ï¼ˆ1/2ï¼‰\n",
    "    MASK_BLOCK_M1: tl.constexpr = BLOCK_M1 // BLK_SLICE_FACTOR\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹Kãƒ–ãƒ­ãƒƒã‚¯ã®åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "    # (BLOCK_N1,)\n",
    "    offs_n = start_n + tl.arange(0, BLOCK_N1)\n",
    "\n",
    "    # dVã‚’ã‚¼ãƒ­ã§åˆæœŸåŒ–\n",
    "    # (BLOCK_N1, HEAD_DIM)\n",
    "    dv = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    # dKã‚’ã‚¼ãƒ­ã§åˆæœŸåŒ–\n",
    "    # (BLOCK_N1, HEAD_DIM)\n",
    "    dk = tl.zeros([BLOCK_N1, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹Kãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰\n",
    "    # Kãƒã‚¤ãƒ³ã‚¿ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ * ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ + åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆ * æ¬¡å…ƒã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "    # (BLOCK_N1, HEAD_DIM)\n",
    "    k = tl.load(K + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹Vãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰\n",
    "    # Vãƒã‚¤ãƒ³ã‚¿ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ * ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ + åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆ * æ¬¡å…ƒã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "    # (BLOCK_N1, HEAD_DIM)\n",
    "    v = tl.load(V + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "\n",
    "    # å¿…è¦ãªåå¾©å›æ•°ã‚’è¨ˆç®—\n",
    "    num_steps = BLOCK_N1 // MASK_BLOCK_M1\n",
    "\n",
    "    # å› æœãƒã‚¹ã‚­ãƒ³ã‚°ãŒå¿…è¦ãªå¯¾è§’ãƒ–ãƒ­ãƒƒã‚¯ã«å¯¾ã—ã¦dKã¨dVã‚’è¨ˆç®—ï¼ˆon-diagonalï¼‰\n",
    "    dk, dv = _attn_bwd_dkdv(\n",
    "        dk,\n",
    "        dv,\n",
    "        Q,\n",
    "        k,\n",
    "        v,\n",
    "        sm_scale,\n",
    "        DO,\n",
    "        M,\n",
    "        D,\n",
    "        stride_tok,\n",
    "        stride_d,\n",
    "        H,\n",
    "        N_CTX,\n",
    "        MASK_BLOCK_M1,\n",
    "        BLOCK_N1,\n",
    "        HEAD_DIM,\n",
    "        start_n,\n",
    "        start_m,\n",
    "        num_steps,\n",
    "        MASK=True # å› æœãƒã‚¹ã‚­ãƒ³ã‚°ã‚’é©ç”¨\n",
    "    )\n",
    "\n",
    "    # ã²ã¨ã¤ã®ãƒ–ãƒ­ãƒƒã‚¯åˆ†é€²ã‚ã‚‹ï¼ˆé€†ä¼æ’­ã®ãŸã‚ã€ã‚ªãƒ³ãƒ€ã‚¤ã‚¢ã‚´ãƒŠãƒ«ã‹ã‚‰ã‚ªãƒ•ãƒ€ã‚¤ã‚¢ã‚´ãƒŠãƒ«ã«é€²ã‚ã‚‹ï¼‰\n",
    "    start_m += num_steps * MASK_BLOCK_M1\n",
    "    num_steps = (N_CTX - start_m) // BLOCK_M1\n",
    "\n",
    "    # å› æœãƒã‚¹ã‚­ãƒ³ã‚°ãŒä¸è¦ãªãƒ–ãƒ­ãƒƒã‚¯ã«å¯¾ã—ã¦dKã¨dVã‚’è¨ˆç®—ï¼ˆoff-diagonalï¼‰\n",
    "    dk, dv = _attn_bwd_dkdv(\n",
    "        dk,\n",
    "        dv,\n",
    "        Q,\n",
    "        k,\n",
    "        v,\n",
    "        sm_scale,\n",
    "        DO,\n",
    "        M,\n",
    "        D,\n",
    "        stride_tok,\n",
    "        stride_d,\n",
    "        H,\n",
    "        N_CTX,\n",
    "        BLOCK_M1,\n",
    "        BLOCK_N1,\n",
    "        HEAD_DIM,\n",
    "        start_n,\n",
    "        start_m,\n",
    "        num_steps,\n",
    "        MASK=False # å› æœãƒã‚¹ã‚­ãƒ³ã‚°ã‚’é©ç”¨ã—ãªã„\n",
    "    )\n",
    "\n",
    "    # dVã®å‡ºåŠ›å…ˆã®ãƒã‚¤ãƒ³ã‚¿ã‚’è¨ˆç®—\n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒã‚¤ãƒ³ã‚¿ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ * ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ + åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆ * æ¬¡å…ƒã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "    # (BLOCK_N1, HEAD_DIM)\n",
    "    dv_ptrs = DV + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n",
    "\n",
    "    # dVã‚’æ›¸ãæˆ»ã™\n",
    "    tl.store(dv_ptrs, dv)\n",
    "\n",
    "    # dKã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’æˆ»ã™\n",
    "    dk *= sm_scale\n",
    "\n",
    "    # dKã®å‡ºåŠ›å…ˆã®ãƒã‚¤ãƒ³ã‚¿ã‚’è¨ˆç®—\n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒã‚¤ãƒ³ã‚¿ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ * ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ + åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆ * æ¬¡å…ƒã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "    # (BLOCK_N1, HEAD_DIM)\n",
    "    dk_ptrs = DK + offs_n[:, None] * stride_tok + offs_k[None, :] * stride_d\n",
    "\n",
    "    # dKã‚’æ›¸ãæˆ»ã™\n",
    "    tl.store(dk_ptrs, dk)\n",
    "\n",
    "    ###########\n",
    "    # dQã‚’è¨ˆç®— #\n",
    "    ###########\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹Qãƒ–ãƒ­ãƒƒã‚¯ã®é–‹å§‹è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "    start_m = pid * BLOCK_M2\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒå‡¦ç†ã™ã‚‹Qãƒ–ãƒ­ãƒƒã‚¯ã®çµ‚äº†è¡Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®— \n",
    "    end_n = start_m + BLOCK_M2\n",
    "\n",
    "    # ä¸€åº¦ã«å‡¦ç†ã™ã‚‹Kãƒ»Vãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’æ›´ã«ç´°ã‹ãã™ã‚‹ï¼ˆ1/2ï¼‰\n",
    "    MASK_BLOCK_N2: tl.constexpr = BLOCK_N2 // BLK_SLICE_FACTOR\n",
    "\n",
    "    # Qãƒ–ãƒ­ãƒƒã‚¯ã®è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "    # (BLOCK_M2,)\n",
    "    offs_m = start_m + tl.arange(0, BLOCK_M2)\n",
    "\n",
    "    # Qãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰\n",
    "    # Qãƒã‚¤ãƒ³ã‚¿ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ * ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ + åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆ * æ¬¡å…ƒã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "    # (BLOCK_M2, HEAD_DIM)\n",
    "    q = tl.load(Q + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "\n",
    "    # dQã‚’ã‚¼ãƒ­ã§åˆæœŸåŒ–\n",
    "    # (BLOCK_M2, HEAD_DIM)\n",
    "    dq = tl.zeros([BLOCK_M2, HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    # dOãƒ–ãƒ­ãƒƒã‚¯ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰\n",
    "    # DOãƒã‚¤ãƒ³ã‚¿ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ * ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ + åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆ * æ¬¡å…ƒã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "    # (BLOCK_M2, HEAD_DIM)\n",
    "    do = tl.load(DO + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d)\n",
    "\n",
    "    # log-sum-expãƒ†ãƒ³ã‚½ãƒ«ã‚’SRAMã«ãƒ­ãƒ¼ãƒ‰\n",
    "    # (BLOCK_M2,) -> (BLOCK_M2, 1)\n",
    "    m = tl.load(M + offs_m)\n",
    "    m = m[:, None]\n",
    "\n",
    "    # å¿…è¦ãªåå¾©å›æ•°ã‚’è¨ˆç®—\n",
    "    num_steps = BLOCK_M2 // MASK_BLOCK_N2\n",
    "\n",
    "    # å› æœãƒã‚¹ã‚­ãƒ³ã‚°ãŒå¿…è¦ãªå¯¾è§’ãƒ–ãƒ­ãƒƒã‚¯ã«å¯¾ã—ã¦dQã‚’è¨ˆç®—ï¼ˆon-diagonalï¼‰\n",
    "    dq = _attn_bwd_dq(\n",
    "        dq,\n",
    "        q,\n",
    "        K,\n",
    "        V,\n",
    "        do,\n",
    "        m,\n",
    "        D,\n",
    "        stride_tok,\n",
    "        stride_d,\n",
    "        H,\n",
    "        N_CTX,\n",
    "        BLOCK_M2,\n",
    "        MASK_BLOCK_N2,\n",
    "        HEAD_DIM,\n",
    "        start_m,\n",
    "        end_n - num_steps * MASK_BLOCK_N2,\n",
    "        num_steps,\n",
    "        MASK=True # å› æœãƒã‚¹ã‚­ãƒ³ã‚°ã‚’é©ç”¨\n",
    "    )\n",
    "\n",
    "    # ã²ã¨ã¤ã®ãƒ–ãƒ­ãƒƒã‚¯åˆ†é€²ã‚ã‚‹ï¼ˆé€†æ–¹å‘ã«é€²ã‚ã‚‹ã®ã¯ã‚³ãƒ¼ãƒ‰ã®å†åˆ©ç”¨æ€§ã¨ã‚·ãƒ³ãƒ—ãƒ«ã•ã®ãŸã‚ï¼‰\n",
    "    end_n -= num_steps * MASK_BLOCK_N2\n",
    "    num_steps = end_n // BLOCK_N2\n",
    "\n",
    "    # å› æœãƒã‚¹ã‚­ãƒ³ã‚°ãŒä¸è¦ãªãƒ–ãƒ­ãƒƒã‚¯ã«å¯¾ã—ã¦dQã‚’è¨ˆç®—ï¼ˆoff-diagonalï¼‰\n",
    "    dq = _attn_bwd_dq(\n",
    "        dq,\n",
    "        q,\n",
    "        K,\n",
    "        V,\n",
    "        do,\n",
    "        m,\n",
    "        D,\n",
    "        stride_tok,\n",
    "        stride_d,\n",
    "        H,\n",
    "        N_CTX,\n",
    "        BLOCK_M2,\n",
    "        BLOCK_N2,\n",
    "        HEAD_DIM,\n",
    "        start_m,\n",
    "        end_n - num_steps * BLOCK_N2,\n",
    "        num_steps,\n",
    "        MASK=False # å› æœãƒã‚¹ã‚­ãƒ³ã‚°ã‚’é©ç”¨ã—ãªã„\n",
    "    )\n",
    "\n",
    "    ###############\n",
    "    # dQã®æ›¸ãæˆ»ã— #\n",
    "    ###############\n",
    "\n",
    "    # dQã®å‡ºåŠ›å…ˆã®ãƒã‚¤ãƒ³ã‚¿ã‚’è¨ˆç®—\n",
    "    # ãƒ™ãƒ¼ã‚¹ãƒã‚¤ãƒ³ã‚¿ + è¡Œã‚ªãƒ•ã‚»ãƒƒãƒˆ * ãƒˆãƒ¼ã‚¯ãƒ³ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ + åˆ—ã‚ªãƒ•ã‚»ãƒƒãƒˆ * æ¬¡å…ƒã‚¹ãƒˆãƒ©ã‚¤ãƒ‰\n",
    "    # (BLOCK_M2, HEAD_DIM)\n",
    "    dq_ptrs = DQ + offs_m[:, None] * stride_tok + offs_k[None, :] * stride_d\n",
    "\n",
    "    # dQã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆé †ä¼æ’­ã§exp2ã‚’ä½¿ç”¨ã—ãŸãŸã‚ï¼‰\n",
    "    dq *= LN2\n",
    "\n",
    "    # dQã‚’æ›¸ãæˆ»ã™\n",
    "    tl.store(dq_ptrs, dq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be74a9",
   "metadata": {},
   "source": [
    "### Attentionå±¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763243ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _attention(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "        ctx, # PyTorchã§è‡ªå‹•çš„ã«æ¸¡ã•ã‚Œã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        q, # ã‚¯ã‚¨ãƒªãƒ†ãƒ³ã‚½ãƒ« (Z, H, N_CTX, HEAD_DIM)\n",
    "        k, # ã‚­ãƒ¼ãƒ†ãƒ³ã‚½ãƒ« (Z, H, N_CTX, HEAD_DIM)\n",
    "        v, # ãƒãƒªãƒ¥ãƒ¼ãƒ†ãƒ³ã‚½ãƒ« (Z, H, N_CTX, HEAD_DIM)\n",
    "        causal, # å› æœãƒã‚¹ã‚­ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°\n",
    "        sm_scale, # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°\n",
    "        warp_specialize=True # Tritonã®æœ€é©åŒ–ãƒ•ãƒ©ã‚°\n",
    "    ):\n",
    "        logger.info(f\"é †ä¼æ¬é–‹å§‹ {q.shape=}{q.dtype=}, {k.shape=}, {k.dtype=}, {v.shape=}, {v.dtype=}, {causal=}, {sm_scale=}, {warp_specialize=}\")\n",
    "\n",
    "        #########\n",
    "        # åˆæœŸåŒ– #\n",
    "        #########\n",
    "\n",
    "        # Qã®ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒã‚’å–å¾— 64\n",
    "        HEAD_DIM_Q = q.shape[-1]\n",
    "        logger.debug(f\"{HEAD_DIM_Q=}\")\n",
    "\n",
    "        # Kã®ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒã‚’å–å¾— 64\n",
    "        HEAD_DIM_K = k.shape[-1]\n",
    "        logger.debug(f\"{HEAD_DIM_K=}\")\n",
    "\n",
    "        # Vã®ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒã‚’å–å¾— 64\n",
    "        HEAD_DIM_V = v.shape[-1]\n",
    "        logger.debug(f\"{HEAD_DIM_V=}\")\n",
    "\n",
    "        # Qãƒ»Kãƒ»Vã®ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒãŒåŒã˜ã§ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨¼\n",
    "        assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n",
    "\n",
    "        # Kã®ãƒ˜ãƒƒãƒ‰æ¬¡å…ƒãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å€¤ã§ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨¼\n",
    "        assert HEAD_DIM_K in {16, 32, 64, 128, 256}\n",
    "\n",
    "        # å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«Oã‚’åˆæœŸåŒ– (1, 2, 128, 64)\n",
    "        # (Z, H, N_CTX, HEAD_DIM)\n",
    "        o = torch.empty_like(q)\n",
    "        logger.debug(f\"{o.shape=}, {o.dtype=}\")\n",
    "\n",
    "        # ã‚¹ãƒ†ãƒ¼ã‚¸ã®è¨­å®š 3\n",
    "        # å› æœãƒã‚¹ã‚¯ãŒæœ‰åŠ¹ãªå ´åˆã¯ã‚¹ãƒ†ãƒ¼ã‚¸3ã€ãã†ã§ãªã‘ã‚Œã°ã‚¹ãƒ†ãƒ¼ã‚¸1\n",
    "        stage = 3 if causal else 1\n",
    "        logger.debug(f\"{stage=}\")\n",
    "\n",
    "        extra_kern_args = {}\n",
    "\n",
    "        # AMDã®å ´åˆ False\n",
    "        if is_hip():\n",
    "            waves_per_eu = 3 if HEAD_DIM_K <= 64 else 2\n",
    "            extra_kern_args = {\"waves_per_eu\": waves_per_eu, \"allow_flush_denorm\": True}\n",
    "        logger.debug(f\"{extra_kern_args=}\")\n",
    "\n",
    "        # ç”Ÿã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã®è¦ç´ ã®æœ€å¤§å€¤mã‚’æ ¼ç´ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«Mã‚’åˆæœŸåŒ– (1, 2, 128)\n",
    "        # (Z, H, N_CTX)\n",
    "        M = torch.empty(\n",
    "            (q.shape[0], q.shape[1], q.shape[2]),\n",
    "            device=q.device,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        logger.debug(f\"{M.shape=}\")\n",
    "\n",
    "        ####################\n",
    "        # ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã®æº–å‚™ #\n",
    "        ####################\n",
    "\n",
    "        # æ–°ã—ã„GPUã®å ´åˆ True\n",
    "        if supports_host_descriptor() and not (is_hopper() and warp_specialize):\n",
    "            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã€ãƒ˜ãƒƒãƒ‰æ•°ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æ›ã‘åˆã‚ã›ã¦ãƒ•ãƒ©ãƒƒãƒˆåŒ–\n",
    "            # 1 * 2 * 128 = 256\n",
    "            y_dim = q.shape[0] * q.shape[1] * q.shape[2]\n",
    "            logger.debug(f\"{y_dim=}\")\n",
    "\n",
    "            dummy_block = [1, 1]\n",
    "\n",
    "            # Qã®ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
    "            desc_q = TensorDescriptor(\n",
    "                q,\n",
    "                shape=[y_dim, HEAD_DIM_K], # (Z * H * N_CTX, HEAD_DIM)\n",
    "                strides=[HEAD_DIM_K, 1], # (HEAD_DIM, 1)\n",
    "                block_shape=dummy_block\n",
    "            )\n",
    "            logger.debug(f\"Qã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\")\n",
    "\n",
    "            # Vã®ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
    "            if q.dtype == torch.float8_e5m2:\n",
    "                # Qã®ãƒ‡ãƒ¼ã‚¿å‹ãŒFP8ã®å ´åˆã¯ã€ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆãŒç•°ãªã‚‹ã®ã§æ³¨æ„\n",
    "                desc_v = TensorDescriptor(\n",
    "                    v,\n",
    "                    shape=[HEAD_DIM_K, y_dim], # (HEAD_DIM, Z * H * N_CTX)\n",
    "                    strides=[q.shape[2], 1], # (N_CTX, 1)\n",
    "                    block_shape=dummy_block\n",
    "                )\n",
    "                logger.debug(f\"FP8ç”¨ã®Vã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\")\n",
    "            else:\n",
    "                desc_v = TensorDescriptor(\n",
    "                    v,\n",
    "                    shape=[y_dim, HEAD_DIM_K], # (Z * H * N_CTX, HEAD_DIM)\n",
    "                    strides=[HEAD_DIM_K, 1], # (HEAD_DIM, 1)\n",
    "                    block_shape=dummy_block\n",
    "                )\n",
    "                logger.debug(f\"Vã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\")\n",
    "\n",
    "            # Kã®ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
    "            desc_k = TensorDescriptor(\n",
    "                k,\n",
    "                shape=[y_dim, HEAD_DIM_K], # (Z * H * N_CTX, HEAD_DIM)\n",
    "                strides=[HEAD_DIM_K, 1], # (HEAD_DIM, 1)\n",
    "                block_shape=dummy_block\n",
    "            )\n",
    "            logger.debug(f\"Kã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\")\n",
    "\n",
    "            # Oã®ãƒ†ãƒ³ã‚½ãƒ«ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
    "            desc_o = TensorDescriptor(\n",
    "                o,\n",
    "                shape=[y_dim, HEAD_DIM_K], # (Z * H * N_CTX, HEAD_DIM)\n",
    "                strides=[HEAD_DIM_K, 1], # (HEAD_DIM, 1)\n",
    "                block_shape=dummy_block\n",
    "            )\n",
    "            logger.debug(f\"Oã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\")\n",
    "        else:\n",
    "            desc_q = q\n",
    "            desc_v = v\n",
    "            desc_k = k\n",
    "            desc_o = o\n",
    "            logger.debug(f\"ãƒã‚¤ãƒ³ã‚¿ã‚’æº–å‚™ {desc_q.shape=}, {desc_k.shape=}, {desc_v.shape=}, {desc_o.shape=}\")\n",
    "\n",
    "        ################\n",
    "        # ã‚«ãƒ¼ãƒãƒ«ã®èµ·å‹• #\n",
    "        ################\n",
    "\n",
    "        def alloc_fn(size: int, align: int, _):\n",
    "            return torch.empty(size, dtype=torch.int8, device=\"cuda\")\n",
    "\n",
    "        triton.set_allocator(alloc_fn)\n",
    "\n",
    "        # èµ·å‹•ã‚°ãƒªãƒƒãƒ‰ã‚’å®šç¾©\n",
    "        # Qãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ˜ãƒƒãƒ‰ã”ã¨ã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å‰²ã‚Šå½“ã¦ã‚‹\n",
    "        def grid(META):\n",
    "            return (\n",
    "                triton.cdiv(q.shape[2], META[\"BLOCK_M\"]), # N_CTX / BLOCK_M = Qãƒ–ãƒ­ãƒƒã‚¯ã®ç·æ•°\n",
    "                q.shape[0] * q.shape[1], # ãƒãƒƒãƒã‚µã‚¤ã‚º * ãƒ˜ãƒƒãƒ‰æ•° = ãƒ˜ãƒƒãƒ‰ã®ç·æ•°\n",
    "                1 \n",
    "            )\n",
    "        logger.debug(f\"èµ·å‹•ã‚°ãƒªãƒƒãƒ‰ã‚’æº–å‚™\")\n",
    "\n",
    "        ctx.grid = grid\n",
    "\n",
    "        if is_blackwell() and warp_specialize:\n",
    "            if HEAD_DIM_K == 128 and q.dtype == torch.float16:\n",
    "                extra_kern_args[\"maxnreg\"] = 168\n",
    "            else:\n",
    "                extra_kern_args[\"maxnreg\"] = 80\n",
    "\n",
    "        logger.debug(f\"_attn_fwdã‚«ãƒ¼ãƒãƒ«ã®å®Ÿè¡Œ\")\n",
    "        _attn_fwd[grid](\n",
    "            sm_scale,\n",
    "            M,\n",
    "            q.shape[0],\n",
    "            q.shape[1],\n",
    "            desc_q,\n",
    "            desc_k,\n",
    "            desc_v,\n",
    "            desc_o,\n",
    "            N_CTX=q.shape[2],\n",
    "            HEAD_DIM=HEAD_DIM_K,\n",
    "            FP8_OUTPUT=q.dtype==torch.float8_e5m2,\n",
    "            STAGE=stage,\n",
    "            warp_specialize=warp_specialize,\n",
    "            IS_HOPPER=is_hopper(),\n",
    "            **extra_kern_args\n",
    "        )\n",
    "\n",
    "        #########\n",
    "        # å¾Œå‡¦ç† #\n",
    "        #########\n",
    "\n",
    "        # é€†ä¼æ¬ã§ä½¿ç”¨ã™ã‚‹å€¤ã‚’ä¿å­˜\n",
    "        ctx.save_for_backward(q, k, v, o, M)\n",
    "        ctx.sm_scale = sm_scale\n",
    "        ctx.HEAD_DIM = HEAD_DIM_K\n",
    "        ctx.causal = causal\n",
    "\n",
    "        logger.info(f\"é †ä¼æ¬çµ‚äº† {o.shape=}, {o.dtype=}\")\n",
    "        return o \n",
    "\n",
    "    @staticmethod\n",
    "    def backward(\n",
    "        ctx, # forwardã§ä¿å­˜ã—ãŸå€¤ã‚’ä¿æŒã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        do # å‡ºåŠ›Oã®å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«\n",
    "    ):\n",
    "        logger.info(f\"é€†ä¼æ¬é–‹å§‹ {do.shape=}, {do.dtype=}\")\n",
    "\n",
    "        #########\n",
    "        # åˆæœŸåŒ– #\n",
    "        #########\n",
    "\n",
    "        # ä¿å­˜ã—ãŸãƒ†ãƒ³ã‚½ãƒ«ã‚’å–å¾—\n",
    "        q, k, v, o, M = ctx.saved_tensors\n",
    "\n",
    "        logger.debug(f\"{q.shape=}, {k.shape=}, {v.shape=}, {o.shape=}, {M.shape=}\")\n",
    "\n",
    "        # dOã®ãƒ¡ãƒ¢ãƒªãŒé€£ç¶šã—ã¦ã„ã‚‹ã“ã¨ã‚’æ¤œè¨¼\n",
    "        assert do.is_contiguous()\n",
    "\n",
    "        # å…¨ã¦ã®ãƒ†ãƒ³ã‚½ãƒ«ã®ã‚¹ãƒˆãƒ©ã‚¤ãƒ‰ãŒåŒã˜ã§ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨¼\n",
    "        assert q.stride() == k.stride() == v.stride() == o.stride() == do.stride()\n",
    "\n",
    "        # å‡ºåŠ›å…ˆã®å‹¾é…ãƒ†ãƒ³ã‚½ãƒ«ã‚’åˆæœŸåŒ–\n",
    "        dq = torch.empty_like(q)\n",
    "        dk = torch.empty_like(k)\n",
    "        dv = torch.empty_like(v)\n",
    "\n",
    "        # 1, 2, 128\n",
    "        BATCH, N_HEAD, N_CTX = q.shape[:3]\n",
    "        logger.debug(f\"{BATCH=}, {N_HEAD=}, {N_CTX=}\")\n",
    "\n",
    "        PRE_BLOCK = 128\n",
    "        logger.debug(f\"{PRE_BLOCK=}\")\n",
    "\n",
    "        NUM_WARPS, NUM_STAGES = 4, 5\n",
    "        logger.debug(f\"{NUM_WARPS=}, {NUM_STAGES=}\")\n",
    "\n",
    "        BLOCK_M1, BLOCK_N1, BLOCK_M2, BLOCK_N2 = 32, 128, 128, 32\n",
    "        logger.debug(f\"{BLOCK_M1=}, {BLOCK_N1=}, {BLOCK_M2=}, {BLOCK_N2=}\")\n",
    "\n",
    "        BLK_SLICE_FACTOR = 2\n",
    "        logger.debug(f\"{BLK_SLICE_FACTOR=}\")\n",
    "\n",
    "        RCP_LN2 = 1.4426950408889634  # = 1.0 / ln(2)\n",
    "        logger.debug(f\"{RCP_LN2=}\")\n",
    "\n",
    "        # Kã‚’äº‹å‰ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã®å†è¨ˆç®—ã§exp2ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚\n",
    "        arg_k = k\n",
    "        arg_k = arg_k * (ctx.sm_scale * RCP_LN2)\n",
    "        logger.debug(f\"{arg_k.shape=}\")\n",
    "\n",
    "        ###################################\n",
    "        # _attn_bwd_preprocessã‚«ãƒ¼ãƒãƒ«ã®å®Ÿè¡Œ #\n",
    "        ###################################\n",
    "\n",
    "        PRE_BLOCK = 128\n",
    "        logger.debug(f\"{PRE_BLOCK=}\")\n",
    "        \n",
    "        # N_CTXãŒPRE_BLOCKã®å€æ•°ã§ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨¼\n",
    "        assert N_CTX % PRE_BLOCK == 0\n",
    "\n",
    "        # 2æ¬¡å…ƒã®èµ·å‹•ã‚°ãƒªãƒƒãƒ‰ã‚’å®šç¾© (1, 2)\n",
    "        pre_grid = (\n",
    "            N_CTX // PRE_BLOCK, # N_CTX / PRE_BLOCK = Qãƒ–ãƒ­ãƒƒã‚¯ã®ç·æ•°\n",
    "            BATCH * N_HEAD # ãƒãƒƒãƒã‚µã‚¤ã‚º * ãƒ˜ãƒƒãƒ‰æ•° = ãƒ˜ãƒƒãƒ‰ã®ç·æ•°\n",
    "        )\n",
    "        logger.debug(f\"{pre_grid=}\")\n",
    "\n",
    "        # preprocessã‚«ãƒ¼ãƒãƒ«ã®è¨ˆç®—çµæœã‚’æ ¼ç´ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ã‚’åˆæœŸåŒ– (1, 2, 128)\n",
    "        # (Z, H, N_CTX)\n",
    "        delta = torch.empty_like(M)\n",
    "        logger.debug(f\"{delta.shape=}\")\n",
    "\n",
    "        logger.debug(f\"_attn_bwd_preprocessã‚«ãƒ¼ãƒãƒ«ã®å®Ÿè¡Œï¼ˆDeltaã‚’è¨ˆç®—ï¼‰\")\n",
    "        _attn_bwd_preprocess[pre_grid](\n",
    "            o,\n",
    "            do,\n",
    "            delta,\n",
    "            BATCH,\n",
    "            N_HEAD,\n",
    "            N_CTX,\n",
    "            BLOCK_M=PRE_BLOCK,\n",
    "            HEAD_DIM=ctx.HEAD_DIM\n",
    "        )\n",
    "\n",
    "        #########################\n",
    "        # _attn_bwdã‚«ãƒ¼ãƒãƒ«ã®å®Ÿè¡Œ #\n",
    "        #########################\n",
    "\n",
    "        # 3æ¬¡å…ƒã®èµ·å‹•ã‚°ãƒªãƒƒãƒ‰ã‚’å®šç¾© (1, 1, 2)\n",
    "        grid = (\n",
    "            N_CTX // BLOCK_N1, # N_CTX / BLOCK_N1 = Kãƒ–ãƒ­ãƒƒã‚¯ã®ç·æ•°\n",
    "            1,\n",
    "            BATCH * N_HEAD # ãƒãƒƒãƒã‚µã‚¤ã‚º * ãƒ˜ãƒƒãƒ‰æ•° = ãƒ˜ãƒƒãƒ‰ã®ç·æ•°\n",
    "        )\n",
    "        logger.debug(f\"{grid=}\")\n",
    "\n",
    "        logger.debug(f\"_attn_bwdã‚«ãƒ¼ãƒãƒ«ã®å®Ÿè¡Œï¼ˆdQãƒ»dKãƒ»dVã‚’è¨ˆç®—ï¼‰\")\n",
    "        _attn_bwd[grid](\n",
    "            q,\n",
    "            arg_k,\n",
    "            v,\n",
    "            ctx.sm_scale,\n",
    "            do,\n",
    "            dq,\n",
    "            dk,\n",
    "            dv,\n",
    "            M,\n",
    "            delta,\n",
    "            q.stride(0),\n",
    "            q.stride(1),\n",
    "            q.stride(2),\n",
    "            q.stride(3),\n",
    "            N_HEAD,\n",
    "            N_CTX,\n",
    "            BLOCK_M1=BLOCK_M1,\n",
    "            BLOCK_N1=BLOCK_N1,\n",
    "            BLOCK_M2=BLOCK_M2,\n",
    "            BLOCK_N2=BLOCK_N2,\n",
    "            BLK_SLICE_FACTOR=BLK_SLICE_FACTOR,\n",
    "            HEAD_DIM=ctx.HEAD_DIM,\n",
    "            num_warps=NUM_WARPS,\n",
    "            num_stages=NUM_STAGES\n",
    "        )\n",
    "\n",
    "        logger.info(f\"é€†ä¼æ¬çµ‚äº† {dq.shape=}, {dq.dtype=}, {dk.shape=}, {dk.dtype=}, {dv.shape=}, {dv.dtype=}\")\n",
    "        return dq, dk, dv, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "742e83f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Function.apply of <class '__main__._attention'>>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = _attention.apply\n",
    "attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a5559",
   "metadata": {},
   "source": [
    "### æ¤œè¨¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8c40a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TORCH_HAS_FP8 = hasattr(torch, 'float8_e5m2')\n",
    "TORCH_HAS_FP8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f25e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.mark.parametrize(\"Z\", [1, 4])\n",
    "@pytest.mark.parametrize(\"H\", [2, 48])\n",
    "@pytest.mark.parametrize(\"N_CTX\", [128, 1024, (2 if is_hip() else 4) * 1024])\n",
    "@pytest.mark.parametrize(\"HEAD_DIM\", [64, 128])\n",
    "@pytest.mark.parametrize(\"causal\", [True])  # FIXME: Non-causal tests do not pass at the moment.\n",
    "@pytest.mark.parametrize(\"warp_specialize\", [False, True] if is_blackwell() else [False])\n",
    "@pytest.mark.parametrize(\"mode\", [\"fwd\", \"bwd\"])\n",
    "@pytest.mark.parametrize(\"provider\", [\"triton-fp16\"] + ([\"triton-fp8\"] if TORCH_HAS_FP8 else []))\n",
    "def test_op(Z, H, N_CTX, HEAD_DIM, causal, warp_specialize, mode, provider, dtype=torch.float16):\n",
    "    logger.info(f\"ãƒ†ã‚¹ãƒˆé–‹å§‹ Z={Z}, H={H}, N_CTX={N_CTX}, HEAD_DIM={HEAD_DIM}, causal={causal}, warp_specialize={warp_specialize}, mode={mode}, provider={provider}, dtype={dtype}\")\n",
    "\n",
    "    if mode == \"fwd\" and \"fp16\" in provider:\n",
    "        pytest.skip(\"Avoid running the forward computation twice.\")\n",
    "\n",
    "    if mode == \"bwd\" and \"fp8\" in provider:\n",
    "        pytest.skip(\"Backward pass with FP8 is not supported.\")\n",
    "\n",
    "    torch.manual_seed(20)\n",
    "\n",
    "    q = (torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=DEVICE).normal_(mean=0.0, std=0.5).requires_grad_())\n",
    "    logger.debug(f\"{q.shape=}\")\n",
    "\n",
    "    k = (torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=DEVICE).normal_(mean=0.0, std=0.5).requires_grad_())\n",
    "    logger.debug(f\"{k.shape=}\")\n",
    "\n",
    "    v = (torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=DEVICE).normal_(mean=0.0, std=0.5).requires_grad_())\n",
    "    logger.debug(f\"{v.shape=}\")\n",
    "\n",
    "    sm_scale = 0.5\n",
    "\n",
    "    ###########################################\n",
    "    # PyTorchã®æ¨™æº–é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®— #\n",
    "    ###########################################\n",
    "\n",
    "    ref_dtype = dtype\n",
    "\n",
    "    if mode == \"fwd\" and \"fp8\" in provider:\n",
    "        ref_dtype = torch.float32\n",
    "\n",
    "    q = q.to(ref_dtype)\n",
    "    k = k.to(ref_dtype)\n",
    "    v = v.to(ref_dtype)\n",
    "\n",
    "    # å› æœãƒã‚¹ã‚¯ã®ä½œæˆ\n",
    "    M = torch.tril(torch.ones((N_CTX, N_CTX), device=DEVICE))\n",
    "    logger.debug(f\"{M.shape=}\")\n",
    "\n",
    "    p = torch.matmul(q, k.transpose(2, 3)) * sm_scale\n",
    "    logger.debug(f\"{p.shape=}\")\n",
    "\n",
    "    if causal:\n",
    "        p[:, :, M == 0] = float(\"-inf\")\n",
    "        logger.debug(\"å› æœãƒã‚¹ã‚¯ã‚’é©ç”¨\")\n",
    "\n",
    "    p = torch.softmax(p.float(), dim=-1)\n",
    "    logger.debug(\"ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’é©ç”¨\")\n",
    "\n",
    "    p = p.to(ref_dtype)\n",
    "    # p = torch.exp(p)\n",
    "\n",
    "    ref_out = torch.matmul(p, v).half()\n",
    "    logger.debug(f\"ãƒãƒªãƒ¥ãƒ¼ã‚’é›†ç´„ {ref_out.shape=}\")\n",
    "\n",
    "    if mode == \"bwd\":\n",
    "        dout = torch.randn_like(q)\n",
    "        ref_out.backward(dout)\n",
    "        ref_dv, v.grad = v.grad.clone(), None\n",
    "        ref_dk, k.grad = k.grad.clone(), None\n",
    "        ref_dq, q.grad = q.grad.clone(), None\n",
    "\n",
    "    #############################\n",
    "    # Tritonã«ã‚ˆã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®— #\n",
    "    #############################\n",
    "\n",
    "    if mode == \"fwd\" and \"fp8\" in provider:\n",
    "        q = q.to(torch.float8_e5m2)\n",
    "        k = k.to(torch.float8_e5m2)\n",
    "        v = v.permute(0, 1, 3, 2).contiguous()\n",
    "        v = v.permute(0, 1, 3, 2)\n",
    "        v = v.to(torch.float8_e5m2)\n",
    "\n",
    "    tri_out = attention(q, k, v, causal, sm_scale, warp_specialize).half()\n",
    "    logger.debug(f\"Tritonã«ã‚ˆã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®— {tri_out.shape=}\")\n",
    "\n",
    "    if mode == \"fwd\":\n",
    "        atol = 3 if \"fp8\" in provider else 1e-2\n",
    "        torch.testing.assert_close(tri_out, ref_out, atol=atol, rtol=0)\n",
    "        return\n",
    "\n",
    "    tri_out.backward(dout)\n",
    "    logger.debug(f\"Tritionã«ã‚ˆã‚‹é€†ä¼æ’­è¨ˆç®— {tri_out.shape=}\")\n",
    "\n",
    "    tri_dv, v.grad = v.grad.clone(), None\n",
    "    tri_dk, k.grad = k.grad.clone(), None\n",
    "    tri_dq, q.grad = q.grad.clone(), None\n",
    "\n",
    "    ################\n",
    "    # è¨ˆç®—çµæœã‚’æ¯”è¼ƒ #\n",
    "    ################\n",
    "\n",
    "    torch.testing.assert_close(tri_out, ref_out, atol=1e-2, rtol=0)\n",
    "    rtol = 0.0\n",
    "\n",
    "    # Relative tolerance workaround for known hardware limitation of CDNA2 GPU.\n",
    "    # For details see https://pytorch.org/docs/stable/notes/numerical_accuracy.html#reduced-precision-fp16-and-bf16-gemms-and-convolutions-on-amd-instinct-mi200-devices\n",
    "    if torch.version.hip is not None and \\\n",
    "        triton.runtime.driver.active.get_current_target().arch == \"gfx90a\":\n",
    "        rtol = 1e-2\n",
    "\n",
    "    torch.testing.assert_close(tri_dv, ref_dv, atol=1e-2, rtol=rtol)\n",
    "    torch.testing.assert_close(tri_dk, ref_dk, atol=1e-2, rtol=rtol)\n",
    "    torch.testing.assert_close(tri_dq, ref_dq, atol=1e-2, rtol=rtol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d0d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸŸ© ãƒ†ã‚¹ãƒˆé–‹å§‹ Z=1, H=2, N_CTX=128, HEAD_DIM=64, causal=True, warp_specialize=False, mode=fwd, provider=triton-fp8, dtype=torch.float16\n",
      "ğŸŸ¦ q.shape=torch.Size([1, 2, 128, 64])\n",
      "ğŸŸ¦ k.shape=torch.Size([1, 2, 128, 64])\n",
      "ğŸŸ¦ v.shape=torch.Size([1, 2, 128, 64])\n",
      "ğŸŸ¦ M.shape=torch.Size([128, 128])\n",
      "ğŸŸ¦ p.shape=torch.Size([1, 2, 128, 128])\n",
      "ğŸŸ¦ å› æœãƒã‚¹ã‚¯ã‚’é©ç”¨\n",
      "ğŸŸ¦ ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’é©ç”¨\n",
      "ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’é›†ç´„ ref_out.shape=torch.Size([1, 2, 128, 64])\n",
      "ğŸŸ© é †ä¼æ¬é–‹å§‹ q.shape=torch.Size([1, 2, 128, 64])q.dtype=torch.float8_e5m2, k.shape=torch.Size([1, 2, 128, 64]), k.dtype=torch.float8_e5m2, v.shape=torch.Size([1, 2, 128, 64]), v.dtype=torch.float8_e5m2, causal=True, sm_scale=0.5, warp_specialize=False\n",
      "ğŸŸ¦ HEAD_DIM_Q=64\n",
      "ğŸŸ¦ HEAD_DIM_K=64\n",
      "ğŸŸ¦ HEAD_DIM_V=64\n",
      "ğŸŸ¦ o.shape=torch.Size([1, 2, 128, 64]), o.dtype=torch.float8_e5m2\n",
      "ğŸŸ¦ stage=3\n",
      "ğŸŸ¦ extra_kern_args={}\n",
      "ğŸŸ¦ M.shape=torch.Size([1, 2, 128])\n",
      "ğŸŸ¦ y_dim=256\n",
      "ğŸŸ¦ Qã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
      "ğŸŸ¦ FP8ç”¨ã®Vã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
      "ğŸŸ¦ Kã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
      "ğŸŸ¦ Oã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
      "ğŸŸ¦ èµ·å‹•ã‚°ãƒªãƒƒãƒ‰ã‚’æº–å‚™\n",
      "ğŸŸ¦ _attn_fwdã‚«ãƒ¼ãƒãƒ«ã®å®Ÿè¡Œ\n",
      "ğŸŸ© é †ä¼æ¬çµ‚äº† o.shape=torch.Size([1, 2, 128, 64]), o.dtype=torch.float8_e5m2\n",
      "ğŸŸ¦ Tritonã«ã‚ˆã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®— tri_out.shape=torch.Size([1, 2, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "test_op(\n",
    "    Z=1,\n",
    "    H=2,\n",
    "    N_CTX=128,\n",
    "    HEAD_DIM=32,\n",
    "    causal=True,\n",
    "    warp_specialize=False,\n",
    "    mode=\"fwd\",\n",
    "    provider=\"triton-fp8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸŸ© ãƒ†ã‚¹ãƒˆé–‹å§‹ Z=1, H=2, N_CTX=128, HEAD_DIM=64, causal=True, warp_specialize=False, mode=bwd, provider=triton-fp16, dtype=torch.float16\n",
      "ğŸŸ¦ q.shape=torch.Size([1, 2, 128, 64])\n",
      "ğŸŸ¦ k.shape=torch.Size([1, 2, 128, 64])\n",
      "ğŸŸ¦ v.shape=torch.Size([1, 2, 128, 64])\n",
      "ğŸŸ¦ M.shape=torch.Size([128, 128])\n",
      "ğŸŸ¦ p.shape=torch.Size([1, 2, 128, 128])\n",
      "ğŸŸ¦ å› æœãƒã‚¹ã‚¯ã‚’é©ç”¨\n",
      "ğŸŸ¦ ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’é©ç”¨\n",
      "ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’é›†ç´„ ref_out.shape=torch.Size([1, 2, 128, 64])\n",
      "/opt/miniconda/envs/py312/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:179.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "ğŸŸ© é †ä¼æ¬é–‹å§‹ q.shape=torch.Size([1, 2, 128, 64])q.dtype=torch.float16, k.shape=torch.Size([1, 2, 128, 64]), k.dtype=torch.float16, v.shape=torch.Size([1, 2, 128, 64]), v.dtype=torch.float16, causal=True, sm_scale=0.5, warp_specialize=False\n",
      "ğŸŸ¦ HEAD_DIM_Q=64\n",
      "ğŸŸ¦ HEAD_DIM_K=64\n",
      "ğŸŸ¦ HEAD_DIM_V=64\n",
      "ğŸŸ¦ o.shape=torch.Size([1, 2, 128, 64]), o.dtype=torch.float16\n",
      "ğŸŸ¦ stage=3\n",
      "ğŸŸ¦ extra_kern_args={}\n",
      "ğŸŸ¦ M.shape=torch.Size([1, 2, 128])\n",
      "ğŸŸ¦ y_dim=256\n",
      "ğŸŸ¦ Qã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
      "ğŸŸ¦ Vã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
      "ğŸŸ¦ Kã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
      "ğŸŸ¦ Oã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’ä½œæˆ\n",
      "ğŸŸ¦ èµ·å‹•ã‚°ãƒªãƒƒãƒ‰ã‚’æº–å‚™\n",
      "ğŸŸ¦ _attn_fwdã‚«ãƒ¼ãƒãƒ«ã®å®Ÿè¡Œ\n",
      "ğŸŸ© é †ä¼æ¬çµ‚äº† o.shape=torch.Size([1, 2, 128, 64]), o.dtype=torch.float16\n",
      "ğŸŸ¦ Tritonã«ã‚ˆã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®— tri_out.shape=torch.Size([1, 2, 128, 64])\n",
      "ğŸŸ© é€†ä¼æ¬é–‹å§‹ do.shape=torch.Size([1, 2, 128, 64]), do.dtype=torch.float16\n",
      "ğŸŸ¦ q.shape=torch.Size([1, 2, 128, 64]), k.shape=torch.Size([1, 2, 128, 64]), v.shape=torch.Size([1, 2, 128, 64]), o.shape=torch.Size([1, 2, 128, 64]), M.shape=torch.Size([1, 2, 128])\n",
      "ğŸŸ¦ BATCH=1, N_HEAD=2, N_CTX=128\n",
      "ğŸŸ¦ PRE_BLOCK=128\n",
      "ğŸŸ¦ NUM_WARPS=4, NUM_STAGES=5\n",
      "ğŸŸ¦ BLOCK_M1=32, BLOCK_N1=128, BLOCK_M2=128, BLOCK_N2=32\n",
      "ğŸŸ¦ BLK_SLICE_FACTOR=2\n",
      "ğŸŸ¦ RCP_LN2=1.4426950408889634\n",
      "ğŸŸ¦ arg_k.shape=torch.Size([1, 2, 128, 64])\n",
      "ğŸŸ¦ PRE_BLOCK=128\n",
      "ğŸŸ¦ pre_grid=(1, 2)\n",
      "ğŸŸ¦ delta.shape=torch.Size([1, 2, 128])\n",
      "ğŸŸ¦ _attn_bwd_preprocessã‚«ãƒ¼ãƒãƒ«ã®å®Ÿè¡Œï¼ˆDeltaã‚’è¨ˆç®—ï¼‰\n",
      "ğŸŸ¦ grid=(1, 1, 2)\n",
      "ğŸŸ¦ _attn_bwdã‚«ãƒ¼ãƒãƒ«ã®å®Ÿè¡Œï¼ˆdQãƒ»dKãƒ»dVã‚’è¨ˆç®—ï¼‰\n",
      "ğŸŸ© é€†ä¼æ¬çµ‚äº† dq.shape=torch.Size([1, 2, 128, 64]), dq.dtype=torch.float16, dk.shape=torch.Size([1, 2, 128, 64]), dk.dtype=torch.float16, dv.shape=torch.Size([1, 2, 128, 64]), dv.dtype=torch.float16\n",
      "ğŸŸ¦ Tritionã«ã‚ˆã‚‹é€†ä¼æ’­è¨ˆç®— tri_out.shape=torch.Size([1, 2, 128, 64])\n"
     ]
    }
   ],
   "source": [
    "test_op(\n",
    "    Z=1,\n",
    "    H=2,\n",
    "    N_CTX=128,\n",
    "    HEAD_DIM=32,\n",
    "    causal=True,\n",
    "    warp_specialize=False,\n",
    "    mode=\"bwd\",\n",
    "    provider=\"triton-fp16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b2ef5",
   "metadata": {},
   "source": [
    "### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0115d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from flash_attn.flash_attn_interface import \\\n",
    "        flash_attn_qkvpacked_func as flash_attn_func\n",
    "    HAS_FLASH = True\n",
    "except BaseException:\n",
    "    HAS_FLASH = False\n",
    "\n",
    "HAS_FLASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_HAS_FP8 = hasattr(torch, 'float8_e5m2')\n",
    "TORCH_HAS_FP8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7727bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH, N_HEADS = 4, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vary seq length for fixed head and batch=4\n",
    "configs = []\n",
    "for HEAD_DIM in [64, 128]:\n",
    "    for mode in [\"fwd\", \"bwd\"]:\n",
    "        for causal in [True, False]:\n",
    "            # Enable warpspec for causal fwd on Hopper\n",
    "            enable_ws = mode == \"fwd\" and (is_blackwell() or (is_hopper() and not causal))\n",
    "            for warp_specialize in [False, True] if enable_ws else [False]:\n",
    "                configs.append(\n",
    "                    triton.testing.Benchmark(\n",
    "                        x_names=[\"N_CTX\"],\n",
    "                        x_vals=[2**i for i in range(10, 15)],\n",
    "                        line_arg=\"provider\",\n",
    "                        line_vals=[\"triton-fp16\"] + ([\"triton-fp8\"] if TORCH_HAS_FP8 else []) +\n",
    "                        ([\"flash\"] if HAS_FLASH else []),\n",
    "                        line_names=[\"Triton [FP16]\"] + ([\"Triton [FP8]\"] if TORCH_HAS_FP8 else []) +\n",
    "                        ([\"Flash-2\"] if HAS_FLASH else []),\n",
    "                        styles=[(\"red\", \"-\"), (\"blue\", \"-\"), (\"green\", \"-\")],\n",
    "                        ylabel=\"TFLOPS\",\n",
    "                        plot_name=\n",
    "                        f\"fused-attention-batch{BATCH}-head{N_HEADS}-d{HEAD_DIM}-{mode}-causal={causal}-warp_specialize={warp_specialize}\",\n",
    "                        args={\n",
    "                            \"H\": N_HEADS,\n",
    "                            \"BATCH\": BATCH,\n",
    "                            \"HEAD_DIM\": HEAD_DIM,\n",
    "                            \"mode\": mode,\n",
    "                            \"causal\": causal,\n",
    "                            \"warp_specialize\": warp_specialize,\n",
    "                        },\n",
    "                    ))\n",
    "\n",
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fc1427",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(configs)\n",
    "def bench_flash_attention(BATCH, H, N_CTX, HEAD_DIM, causal, warp_specialize, mode, provider, device=DEVICE):\n",
    "    assert mode in [\"fwd\", \"bwd\"]\n",
    "    dtype = torch.float16\n",
    "    if \"triton\" in provider:\n",
    "        q = torch.randn((BATCH, H, N_CTX, HEAD_DIM), dtype=dtype, device=device, requires_grad=True)\n",
    "        k = torch.randn((BATCH, H, N_CTX, HEAD_DIM), dtype=dtype, device=device, requires_grad=True)\n",
    "        v = torch.randn((BATCH, H, N_CTX, HEAD_DIM), dtype=dtype, device=device, requires_grad=True)\n",
    "        if mode == \"fwd\" and \"fp8\" in provider:\n",
    "            q = q.to(torch.float8_e5m2)\n",
    "            k = k.to(torch.float8_e5m2)\n",
    "            v = v.permute(0, 1, 3, 2).contiguous()\n",
    "            v = v.permute(0, 1, 3, 2)\n",
    "            v = v.to(torch.float8_e5m2)\n",
    "        sm_scale = 1.3\n",
    "        fn = lambda: attention(q, k, v, causal, sm_scale, warp_specialize)\n",
    "        if mode == \"bwd\":\n",
    "            o = fn()\n",
    "            do = torch.randn_like(o)\n",
    "            fn = lambda: o.backward(do, retain_graph=True)\n",
    "        ms = triton.testing.do_bench(fn)\n",
    "\n",
    "    if provider == \"flash\":\n",
    "        qkv = torch.randn((BATCH, N_CTX, 3, H, HEAD_DIM), dtype=dtype, device=device, requires_grad=True)\n",
    "        fn = lambda: flash_attn_func(qkv, causal=causal)\n",
    "        if mode == \"bwd\":\n",
    "            o = fn()\n",
    "            do = torch.randn_like(o)\n",
    "            fn = lambda: o.backward(do, retain_graph=True)\n",
    "        ms = triton.testing.do_bench(fn)\n",
    "    flops_per_matmul = 2.0 * BATCH * H * N_CTX * N_CTX * HEAD_DIM\n",
    "    total_flops = 2 * flops_per_matmul\n",
    "    if causal:\n",
    "        total_flops *= 0.5\n",
    "    if mode == \"bwd\":\n",
    "        total_flops *= 2.5  # 2.0(bwd) + 0.5(recompute)\n",
    "    return total_flops * 1e-12 / (ms * 1e-3)\n",
    "\n",
    "# only works on post-Ampere GPUs right now\n",
    "bench_flash_attention.run(save_path=\".\", print_data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
